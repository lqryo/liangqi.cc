<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Deep is Better?下图表示，network的层数越多，Error Rate越低。这说明network越deep，performance越好吗？但是仔细思考以下，network越深它的参数就越多，它能cover的function也就越多，那它的performance更好也是很正常的。  如果正真要比较shallow和deep的network的performance，需要让Shallow和">
<meta property="og:type" content="article">
<meta property="og:title" content="Why Deep">
<meta property="og:url" content="http://yoursite.com/2019/09/26/why deep/index.html">
<meta property="og:site_name" content="Qi-Liang&#39;blog">
<meta property="og:description" content="Deep is Better?下图表示，network的层数越多，Error Rate越低。这说明network越deep，performance越好吗？但是仔细思考以下，network越深它的参数就越多，它能cover的function也就越多，那它的performance更好也是很正常的。  如果正真要比较shallow和deep的network的performance，需要让Shallow和">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/1.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/2.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/3.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/4.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/5.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/6.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/7.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/8.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/9.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/10.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/11.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/12.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/13.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/14.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/15.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/16.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/17.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/18.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/19.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/20.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/21.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/22.png">
<meta property="og:image" content="http://yoursite.com/2019/09/26/why%20deep/23.png">
<meta property="og:updated_time" content="2019-10-20T02:49:33.399Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Why Deep">
<meta name="twitter:description" content="Deep is Better?下图表示，network的层数越多，Error Rate越低。这说明network越deep，performance越好吗？但是仔细思考以下，network越深它的参数就越多，它能cover的function也就越多，那它的performance更好也是很正常的。  如果正真要比较shallow和deep的network的performance，需要让Shallow和">
<meta name="twitter:image" content="http://yoursite.com/2019/09/26/why%20deep/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/09/26/why deep/">





  <title>Why Deep | Qi-Liang'blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qi-Liang'blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">stay young,stay simple</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/26/why deep/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Why Deep</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-26T23:40:21+08:00">
                2019-09-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Deep-is-Better"><a href="#Deep-is-Better" class="headerlink" title="Deep is Better?"></a>Deep is Better?</h1><p>下图表示，network的层数越多，Error Rate越低。这说明network越deep，performance越好吗？但是仔细思考以下，network越深它的参数就越多，它能cover的function也就越多，那它的performance更好也是很正常的。</p>
<p><img src="/2019/09/26/why deep/1.png" alt="1"></p>
<p>如果正真要比较shallow和deep的network的performance，需要让Shallow和Deep model的参数是一样多的。这样的比较才是公平的。</p>
<p><img src="/2019/09/26/why deep/2.png" alt="2"></p>
<p>用上面的方式比较的结果如下</p>
<p><img src="/2019/09/26/why deep/3.png" alt="3"></p>
<p>我们发现$2\times 2K$的network比$1\times 16K$的network效果还要好，说明network的层数更多确实比仅仅增加参数要管用。那为什么会这样呢？有人认为deep learning就是一个暴力枚举的方法，弄一个很大的model，然后收集一大笔data，所以就能得到比较好的performance。但实际不是这样，因为从上表我们知道，单纯增加参数而只是让network变宽，对performance的帮助是比较小的。</p>
<h1 id="Modularization"><a href="#Modularization" class="headerlink" title="Modularization"></a>Modularization</h1><p>但我们在做deep learning的时候，其实就是在做模组化这件事（<strong>Modularization</strong>）。就比如我们写代码的时候，并不是将所有的function都写在main function里面。我们会在main function里面去调用子函数，子函数里面也会去调用别的子函数。这样做的好处是有些函数是可以公用的。</p>
<p>假设我们要用deep learning去做影像分类，要把image分成4类，如下，我们对这4类影像收集一些data，然后去train 4个classifer，就可以解决这个问题。但问题是，长头发的男生这个data可能是比较少的，这样train出来的长头发男生的classifier就比较弱。</p>
<p><img src="/2019/09/26/why deep/4.png" alt="4"></p>
<p>为了解决上面的问题，就可以用模组化的概念。假设我们先不是直接去解那个问题，而是把原来的问题切成比较小的问题。比如，我们learn一些classifier，这些classifier的工作是去detect有没有某一种的attribute出现。它不是直接去detect是长头发的男生还是长头发的女生，而是把这个问题切成比较小的问题。它只用判断image是男生还是女生，或只用判断image是长头发还是短头发。这样，虽然长头发的男生data很少，但是女生和男生都可以分别收集到足够的data，长发的人和短发的人也可以收集到足够多的data。这样我们train这些basic classifier的时候就不会太差。最后来解我们要真正处理的问题的时候，每一个classifier就去参考这些basic的attribute的输出，最后要下决定的那个classifier，它是把前面的basic的classifier当作module，去call它的output，而每一个classifier都共用同样的module。后面的classifier可以利用前面的classifier，所以只要用比较少的training data就可以把结果train好。</p>
<p><img src="/2019/09/26/why deep/5.png" alt="5"></p>
<p>每一个neuron其实就是一个classifier,第一层的neuron是最基础的classifier，而第二层是比较复杂的classifier，它用第一层classifier的output当作input，第三层的neuron又把第二层neuron当作module，依此类推。要注意的是，在做deep learning的时候，怎么做模组化这件事情是machine自动学到的。模组化的好处就是，就算我们training data没有那么多，我们也可以把task做好。这样就解释了deep learning所要的training data是比较少的。</p>
<p>这可能和我们的认识有些矛盾。有人认为AI就等于big data+deep learning，所以deep learning会work，是因为big data的关系。其实并不是这样的。假设我们有真正很大的big data，打到可以把全世界的image通通收集起来，那还何必做machine learning呢？直接table lookup就好了。所以，machine learning和big data在某种程度上其实是相反的，我们之所以不能table lookup，就是因为我们没有足够的data,所以需要machine去做举一反三的事情。</p>
<h1 id="Modularization-Speech"><a href="#Modularization-Speech" class="headerlink" title="Modularization - Speech"></a>Modularization - Speech</h1><p>deep learning在影像和语音上面的表现特别好。下面会说明为什么在语音上需要模组化的概念。</p>
<p>先简单介绍一下人类语言的架构。人说的每一句话其实是由一串<strong>phoneme（音素）</strong>所组成，它是语言学家订出来的人类发音的基本单位。比如<em>what do you think</em>这就话的phoneme组成如下。同样的phoneme可能会有不太一样的发音。因为人类发音器官的限制，phoneme的发音会受到前后phone所影响，为了表达这件事情，我们会给同样的phoneme不同的model，这个东西就叫做<strong>tri-phone</strong>，tri-phone的表达方式是，比如下图的第一个uw，加上前面的phoneme d跟后面的phoneme y，第二个uw,加上前面的y和后面的th。tri-phone不是考虑3个phone的意思，tri-phone的意思是，现在一个phone，我们用不同的model来表示它。如果一个phone，它的context不一样，我们就用不同的model来描述这样的phoneme。一个phoneme可以拆成几个state，state的个数是需要我们自己定的。比如我们通常就定成3个state。</p>
<p><img src="/2019/09/26/why deep/6.png" alt="6"></p>
<p>以上是人类语言的基本架构，那要怎么做语音辨识呢？语音辨识非常的复杂，这里只讲语音辨识的第一步。第一步要做的事情是<strong>把acoustic feature转成state</strong>。这是一个单纯的classification problem。所谓的acoustic feature，简单讲就是,input声音讯号，它是一串wave form，在这个wave form上面取一个window，这个window通常不会取太大。把一个window里面用一个feature来描述这个window里面的特性，这个东西就是一个acoustic feature。在这个声音信号上面，会每隔一小段时间，就取一个window，所以一个声音信号就会变成一串<strong>vector sequence</strong>，这个就叫做<strong>acoustic feature sequence</strong>。在做语音辨识的第一阶段，需要做的事情就是，决定每一个acoustic feature它属于哪一个state。也就是要建一个classifier，例如这个classifier告诉我们第一个acoustic feature它属于state a。但是光只有这样子是没有办法做一个语音辨识系统的，这个东西只是state而已，我们还要把state转成phoneme，然后再把phoneme转成文字。接下来，还要用language model考虑同音异字的问题。</p>
<p><img src="/2019/09/26/why deep/7.png" alt="7"></p>
<p>比较一下过去在用deep learning之前和用deep learning之后，在语音辨识上的模型有什么不同，以便更能体会为什么deep learning在语音辨识上会有非常显著的成果。在语音辨识的第一个阶段，就是要做分类，也就是决定一个acoustic feature它属于哪一个state。传统的方法叫做<strong>HMM-GMM</strong>。这个方法是假设每一个属于某一个state的acoustic feature的分布是stationary的，所以可以用一个model来描述它。比如下图第一个state(第一当作中心的tri-phone的第一个state)，它可以用一个GMM来描述，另外一个state可以用另外一个GMM来描述。这个时候给一个feature，就可以算这个acoustic feature从每一个state产生出来的几率，这个东西就叫做<strong>Gaussian Mixture Model(GMM)</strong>。</p>
<p><img src="/2019/09/26/why deep/8.png" alt="8"></p>
<p>但如果仔细想一下，上面的方法其实根本不太会work，因为tri-phone的数目太多了。一般语言，中文英文，都有30到40个phoneme。在tri-phone里面，每一个phoneme，随着它context的不同，也要用不同的model,所以至少有$30^3$个tri-phone，而每一个tri-phone又有3个state，所以有数万个state。每一个state都要用一个GMM来描述，那参数太多了，training data根本不够。所以传统上，在deep learning之前怎么处理这件事呢？有一些state，其实它们会共用同样的model distribution，这件事情叫做<strong>tied_state</strong>。不同的state共用同样的distribution，意思就是说，假如我们在些程序的时候，不同的state的名称，就好像是pointer一样，不同的pointer,它们可能会指向同样的distribution。那到底哪些state要共用，而哪些不要共用，这就需要凭着经验和一些语言学的知识来决定。但是这样是不够的，如果只分state的distribution，要共用或不共用，这样太粗了，所以有人开始提出一些想法，比如如何让它部分共用等等。在deep learning流行之前，再前一个提出来的比较有创新的方法，叫做<strong>subspace GMM</strong>,其实它里面有模组化的影子。我们原来是每一个state就有一个distribution,而在subspace GMM里面，我们先把很多的Gaussian找出来，即先找一个Gaussian pool,每个state的information就是一个key，这个key告诉我们要从这个Gaussian的Pool里面挑哪些Gaussian出来，比能有某一个state 1挑第一、第三、第五个Gaussian，某一个state 2挑第一、第四、第六个Gaussian，如果这样做的话，这些state有些时候就可以shate部分的Gaussian，而有些时候就可以完全不share Gaussian，至于要share多少的Gaussian,这个东西可以是从training data里面学习得来。</p>
<p><img src="/2019/09/26/why deep/9.png" alt="9"></p>
<p>上面是在DNN火红之前的做法，但如果仔细想想，HMM-GMM方式，所有的phone或者state，是independent model的，这件事情对model人类的声音来说是不efficient的。人类的声音，不同的phoneme，虽然我们在分类的时候把它归类为不同的class，但这些phoneme之间并不是完全无关的，它们都是由人类的发音器官所generate出来的，它们中间是有根据人类发音器官发音方式具有某些关系的。</p>
<p>举例来说，在下图中，画出了人类语言里面所有的母音。母音的发音其实只受三件事情的影响，一个是舌头的前后位置，一个是舌头上下的位置，还有一个就是嘴型。下图中有英文的5个母音：a、e、i、o、u，从图中可看出，i和u的差别是舌头放在前面和放在后面的差别。在图中同一个位置的母音，表示说舌头的位置是一样的，但是嘴型是不一样的。所以，因为不同的phoneme之间是有关系的，所以每一个phoneme都搞一个自己的model，这样做其实是没有效率的。</p>
<p><img src="/2019/09/26/why deep/10.png" alt="10"></p>
<p>如果用deep learning要怎么做呢？我们要learn一个deep neural network，这个deep neural network的input就是一个acoustic feature，它的output就是这个feature属于哪一个state的几率。这是一个很单纯的classification的问题。这里最关键的一点是，所有的state，都共用一个DNN。在整个辨识里面，就只有一个DNN而已,而没有每一个state都有一个DNN。有人可能会觉得，从GMM变到deep learning厉害的地方就是，本来GMM通常最多也就64个Gaussian mixture而已，那DNN有10层，每层1000个neuron，参数变多了，所以performance就变好了，这是一个暴力碾压的方法，其实也没什么。其实不是这样，在做HMM-GMM的时候，GMM只有64个mixture，好像觉得很简单，但是其实每一个state都有一个Gaussian mixture，所以真正合起来，它的参数是多的不得了的。如果仔细去算一下GMM用到参数和DNN用的参数，它们用的参数其实是差不多多的——DNN只用一个很大的model，GMM是用很多很小的model。但是DNN把所有的state通通用同一个model来做分类，会是比较有效率的做法。</p>
<p><img src="/2019/09/26/why deep/11.png" alt="11"></p>
<p>为什么这样是计较有效率的做法呢？举例来说，如果把一个DNN它的某一个hidden layer拿出来，比如说它有1000个neuron，没有办法去分析它，但是可以把那1000个neuron的output降维，降到2维。在下面的图中，每一个点代表了一个acoustic feature。它通过DNN以后，把output layer的output降到2维，可以发现它的分布如下。图中的颜色其实就是a、e、i、o、u这5个母音。我们会发现，这5个母音的分布，跟右上角母音的分布，其实几乎是一样的。所以，DNN做的事情，它的比较lower的layer做到事情并不是马上去侦测现在input的发音是属于哪一个phone或哪一个state，而是先观察，每当听到这个发音的时候，人是用什么样的方式在发这个声音的。它的舌头位置在哪里，位置高还是低，前还是后等等。这样，lower layer（靠近input的layer），先知道了发音的方式以后，接下来的layer，在根据这个结果去决定现在的发音是属于哪一个state或哪一个phone。所以，所有的phone会用同一组detector，也就是说这些lower的layer是一个人类发音方式的detector，而所有phone的侦测都是用同一个detector完成的，都share同一组参数。所以它这边有做到模组化这件事情。当做模组化的时候，是用比较有效率的方式来使用参数。</p>
<p><img src="/2019/09/26/why deep/12.png" alt="12"></p>
<h2 id="Universality-Theorem"><a href="#Universality-Theorem" class="headerlink" title="Universality Theorem"></a>Universality Theorem</h2><p>根据<strong>Universality Theorem</strong>，任何一个连续的function，都可以用一层足够宽neural network来完成。在90年代，这是很多人放弃做deep learning的一个原因。但是，这个理论只告诉我们可能性，但没有告诉我们要做到这件事情有多少效率。当我们只用一个hidden layer的时候，其实是没有效率的。当我们有hierarchy的structure，用这种方式来描述function的时候，它是比较有效率的。</p>
<p><img src="/2019/09/26/why deep/13.png" alt="13"></p>
<h1 id="Analogy"><a href="#Analogy" class="headerlink" title="Analogy"></a>Analogy</h1><p>这里举另一个例子。在逻辑电路里面，整个电路是由一堆逻辑门（与、或、非）所构成，在neural network里面，整个network是由一堆neuron所构成。对于逻辑电路，只要两层逻辑门，就可以表示任意的boolean function，在我们实际在设计电路的时候，根本不可能会这样做。因为当我们用hierarchy的架构的时候，这个时候拿来设计一个电路是比较有效率的。类比到neural network，其实意思是一样的。</p>
<p><img src="/2019/09/26/why deep/14.png" alt="14"></p>
<p>一个日常生活中的例子</p>
<p><img src="/2019/09/26/why deep/15.png" alt="15"></p>
<h2 id="More-Analogy"><a href="#More-Analogy" class="headerlink" title="More Analogy"></a>More Analogy</h2><p>用之前讲的例子来做比喻。假设现在input的点有4个，如下，红色的点是一类，蓝色的点是一类。如果没有hidden layer，而是一个linear的model,无论怎么做都没有办法把蓝色分在一边，把红色分在一边。但是当加了hidden layer以后，就做了一个feature的transformation，把原来的$x_1$和$x_2$投影到另一个平面变成$x_1^{\prime}$和$x_2^{\prime}$。其实这样做好像就是把原来的平面对折了一样，让两个蓝色的点重合在一起，就好像是剪窗花的时候将纸对折一样。当我们做deep learning的时候，其实是在用比较有效率的方式来使用data。</p>
<p><img src="/2019/09/26/why deep/16.png" alt="16"></p>
<p>下面是一个toy example。有一个function，它的input是$R^2$，output是0和1，如下图，它是一个地毯形状的function，红色区域输出是1，蓝色区域输出是0。现在我们用不同量的training example，在一个hidden layer和3个hidden layer的时候，会看到什么样的情形（通过调整使3个hidden layer的参数和1个hidden layer一样多）。结果如下图右边所示。</p>
<p><img src="/2019/09/26/why deep/17.png" alt="17"></p>
<p>我看可以看到当参数减少的时候，1层的network和3层的network结果都会变差，但是3层的hidden layer是比较有次序的崩坏，相比1层的hidden layer，它的结果还是比较好的。</p>
<h1 id="End-to-end-Learning"><a href="#End-to-end-Learning" class="headerlink" title="End-to-end Learning"></a>End-to-end Learning</h1><p>使用deep learning的另外一个好处是可以做<strong>End-to-End learning</strong>，End-to-End learning的意思是，比如有时我们要处理的问题非常复杂，例如语言辨识。我们解一个machine learning的问题时，要做的事情就是先找一个hypothesis的function set，也就是找一个Model。当问题很复杂的时候，这个model会变成一个生产线。这个model要表示一个很复杂的function，这个很复杂的function是由很多比较简单的function串接在一起的。而当我们做End-to-End learning的时候，意思是只给model input和output，不必去考虑中间每一个function要怎么去分工，让machine自己去学中间每一个function它应该要做什么事情。在deep learning做这件事我们只需要叠一个很深的network，每一层就是一个sample function。</p>
<p><img src="/2019/09/26/why deep/18.png" alt="18"></p>
<p>比如在语言辨识里面，在shallow learning的时代，做语音辨识的过程是：先有一段声音讯号，然后做DTF把他变成spectrogram，<br>spectrogram通过filter bank得到output，再取log，然后再做DCT得到MFCC，把MFCC输入到GMM里面，最后可以得到语音辨识的结果（GMM换成DNN也会有非常显著的improvement）。在这整个生产线里面，只有最有的GMM是由training data学出来的，前面的部分都是人手定出来的（前人研究各种人类生理的知识后定出的function）。</p>
<p><img src="/2019/09/26/why deep/19.png" alt="19"></p>
<p>但是有了deep learning以后，我们可以把GMM前面的东西用Neural network取代掉。例如把deep neural network多加几层，就可以把DCT拿掉，这件事情现在已经是typical的做法了。过去MFCC这种feature是dominate语音辨识的，但是现在已经不是了，我们可以直接从log的output开始做，甚至可以从spectrogram开始做，使用deep learning，也可以得到更好的结果。</p>
<p>有人会像能不能叠一个很深的network直接input就是time domain上的声音讯号，output直接就是文字，中间完全不要feature transform之类的。最后Google有一篇论文的结果是，它learning了一个很大的neural network，input就是声音讯号（wave phone），完全不做其他事情，最后可以做到和有做feature transform的结果打平，目前还没有做到能比feature transform更好，或许feature transform已经是讯号处理的极限了，其实deep learning出来的结果也就是feature transform。除了语音，影像也差不多，这里不再赘述。</p>
<h1 id="Complex-Task"><a href="#Complex-Task" class="headerlink" title="Complex Task"></a>Complex Task</h1><p>通常我们真正在意的task，它是非常复杂的。在这种task里面，有时候非常像的input，会有很不一样的output。而有时候很不一样的input，他们的output是一样的。如有network只有一层的话，就只能做简单的transform，没有办法把一样的东西变得很不一样，或者把不一样的东西变得很像。</p>
<p><img src="/2019/09/26/why deep/20.png" alt="20"></p>
<p>要让原来input很像的东西结果看起来很不像，需要做很多层次的转换。下图是把MFCC投影到二维平面上，不同的颜色代表不同的人说的句子，这些句子是一样的。在语音上，我们会发现说同样的句子，不同的人说，它的声音讯号看起来是非常不一样的。所以有人看到这个图会觉得语音辨识不能做。如果今天learn一个neural network，如果只看第一层的hidden layer的output，会发现不同的人讲的同一个句子还是看起来不一样。</p>
<p><img src="/2019/09/26/why deep/21.png" alt="21"></p>
<p>但是如果看第8个hidden layer的output的时候，会发现不同的人说的同一个句子，自动地被align在一起。也就是说这个DNN在很多layer转换的时候，它把本来看起来很不一样的东西，它知道它们应该是一样的，所以经过很多layer转换以后，就把它们兜在一起了。如下图。</p>
<p><img src="/2019/09/26/why deep/22.png" alt="22"></p>
<p>在比如手写数字辨识的例子，input是$28\times 28$的pixel，在下图中，会发现4和9几乎是叠在一起的，没有办法把它们分开。但是我们看第一个hidden layer的output，这时候会发现4和9还是很像的。再看第二个hidden layer的output，发现4、7、9逐渐被分开，第3个hidden layer，它们被分开的更明显。</p>
<p><img src="/2019/09/26/why deep/23.png" alt="23"></p>
<h1 id="To-learn-more"><a href="#To-learn-more" class="headerlink" title="To learn more"></a>To learn more</h1><p>下面是更多要使用deep learning的理由</p>
<ul>
<li><p>Do Deep Nets Really Need To Be Deep?(by Rich Caruana)</p>
</li>
<li><p>Deep learning:Theoretical Motivatons(Yoshua Bengio)</p>
</li>
<li><p>Connections between physics and deep learning</p>
</li>
<li><p>Why Deep Learning Works:Perspectives from Theoretical Chemistry</p>
</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/09/20/Lecture01/" rel="next" title="Tensorflow介绍与安装">
                <i class="fa fa-chevron-left"></i> Tensorflow介绍与安装
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/30/Lecture01/" rel="prev" title="认识headers、版本、重要资源">
                认识headers、版本、重要资源 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Liang Qi">
            
              <p class="site-author-name" itemprop="name">Liang Qi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-is-Better"><span class="nav-number">1.</span> <span class="nav-text">Deep is Better?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Modularization"><span class="nav-number">2.</span> <span class="nav-text">Modularization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Modularization-Speech"><span class="nav-number">3.</span> <span class="nav-text">Modularization - Speech</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Universality-Theorem"><span class="nav-number">3.1.</span> <span class="nav-text">Universality Theorem</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Analogy"><span class="nav-number">4.</span> <span class="nav-text">Analogy</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#More-Analogy"><span class="nav-number">4.1.</span> <span class="nav-text">More Analogy</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#End-to-end-Learning"><span class="nav-number">5.</span> <span class="nav-text">End-to-end Learning</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Complex-Task"><span class="nav-number">6.</span> <span class="nav-text">Complex Task</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#To-learn-more"><span class="nav-number">7.</span> <span class="nav-text">To learn more</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liang Qi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>