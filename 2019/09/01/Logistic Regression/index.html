<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="上一节我们讲到，要做Classification,我们要找的东西就是一个几率$P_{w,b}(C_1|x)$,如果$P_{w,b}(C_1|x)\geq 0.5$,就输出C1,否则就输出C2。如果用Gaussian,$P_{w,b}(C_1|x) = \sigma(z)$,其中:$$\sigma(z) = \frac{1}{1+exp(-z)}$$$$z = {\bf w}\cdot {\bf x">
<meta property="og:type" content="article">
<meta property="og:title" content="Logistic Regression">
<meta property="og:url" content="http://yoursite.com/2019/09/01/Logistic Regression/index.html">
<meta property="og:site_name" content="学习笔记">
<meta property="og:description" content="上一节我们讲到，要做Classification,我们要找的东西就是一个几率$P_{w,b}(C_1|x)$,如果$P_{w,b}(C_1|x)\geq 0.5$,就输出C1,否则就输出C2。如果用Gaussian,$P_{w,b}(C_1|x) = \sigma(z)$,其中:$$\sigma(z) = \frac{1}{1+exp(-z)}$$$$z = {\bf w}\cdot {\bf x">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/1.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/2.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/3.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/4.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/5.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/6.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/7.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/8.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/9.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/10.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/11.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/12.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/13.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/14.png">
<meta property="og:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/15.png">
<meta property="og:updated_time" content="2019-09-13T01:59:31.496Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Logistic Regression">
<meta name="twitter:description" content="上一节我们讲到，要做Classification,我们要找的东西就是一个几率$P_{w,b}(C_1|x)$,如果$P_{w,b}(C_1|x)\geq 0.5$,就输出C1,否则就输出C2。如果用Gaussian,$P_{w,b}(C_1|x) = \sigma(z)$,其中:$$\sigma(z) = \frac{1}{1+exp(-z)}$$$$z = {\bf w}\cdot {\bf x">
<meta name="twitter:image" content="http://yoursite.com/2019/09/01/Logistic%20Regression/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/09/01/Logistic Regression/">





  <title>Logistic Regression | 学习笔记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">学习笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">stay young,stay simple</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/Logistic Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学习笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Logistic Regression</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T19:58:17+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>上一节我们讲到，要做Classification,我们要找的东西就是一个几率$P_{w,b}(C_1|x)$,如果$P_{w,b}(C_1|x)\geq 0.5$,就输出C1,否则就输出C2。如果用Gaussian,$P_{w,b}(C_1|x) = \sigma(z)$,其中:<br>$$\sigma(z) = \frac{1}{1+exp(-z)}$$<br>$$z = {\bf w}\cdot {\bf x} + b = \sum_i w_ix_i+b$$</p>
<p>我们的function set就是所有不同的w和b集合起来的函数：<br>$$f_{w,b}(x) = P_{w,b}(C_1|x)$$</p>
<p>我们用图像的方法来表示如下<br><img src="/2019/09/01/Logistic Regression/1.png" alt="1"><br>以上就是<strong>Logistic Regression</strong></p>
<h1 id="Logistic-Regression-amp-Linear-Regression"><a href="#Logistic-Regression-amp-Linear-Regression" class="headerlink" title="Logistic Regression &amp; Linear Regression"></a>Logistic Regression &amp; Linear Regression</h1><p>我们来比较一下<strong>Logistic Regression</strong>和<strong>Linear Regression</strong>。我们知道Machine Learning有三个步骤</p>
<h2 id="Step1-Model-function-set"><a href="#Step1-Model-function-set" class="headerlink" title="Step1 Model(function set)"></a>Step1 Model(function set)</h2><p>两者的function set如下<br><img src="/2019/09/01/Logistic Regression/2.png" alt="2"></p>
<h2 id="Step2-Goodness-of-a-Function"><a href="#Step2-Goodness-of-a-Function" class="headerlink" title="Step2 Goodness of a Function"></a>Step2 Goodness of a Function</h2><p>我们要决定一个function的好坏。假设有N笔数据$x^1$到$x^N$以及它们对应的分类，假设这些data都是从$f_{w,b}(x)=P_{w,b}(C_1|x)$中产生，<strong>Given a set of w and b,what is its probability of generating the data</strong>?<br><img src="/2019/09/01/Logistic Regression/3.png" alt="3"></p>
<p>对某一个w和b，产生上面N笔data的几率计算如下：<br>$$L(w,b) = f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots f_{w,b}(x^N)$$</p>
<p>The most likely $w^{\star}$ and $b^{\star}$ is the one with the largest $L(w,b)$.<br>$$w^{\star},b^{\star} = \mathop{argmax}_{w,b}L(w,b)$$</p>
<p>我们将上式做一下数学式上的转换,这样可以让计算变得容易点，如下：<br>$$w^{\star},b^{\star} = \mathop{argmax}_{w,b}L(w,b)$$ </p>
<p>$$\Downarrow$$</p>
<p>$$w^{\star},b^{\star} = \mathop{argmin}_{w,b}-InL(w,b)$$</p>
<p>其中<br>$$-InL(w,b) = -Inf_{w,b}(x^1)-Inf_{w,b}(x^2)-Inf_{w,b}(1-f_{w,b}(x^3))\cdots $$</p>
<p>为了能够用求和符号表示上面式子，我们做一个符号上的转换。用1表示Class 1,用0表示Class 2<br><img src="/2019/09/01/Logistic Regression/4.png" alt="4"><br>这样我们就可以把上面的式子表示成：<br>$$<br>\begin{equation}<br>\begin{split}<br>-InL(w,b) = &amp;-\left[\hat{y}^1Inf(x^1)+(1-\hat{y}^1In(1-f(x^1)))\right] \\<br>&amp;-\left[\hat{y}^2Inf(x^2)+(1-\hat{y}^2In(1-f(x^2)))\right] \\<br>&amp;-\left[\hat{y}^3Inf(x^3)+(1-\hat{y}^3In(1-f(x^3)))\right]\cdots<br>\end{split}<br>\end{equation}<br>$$</p>
<p>所以就有<br>$$-InL(w,b) = \sum_n-\left[\hat{y}^nInf_{w,b}(x^n)+(1-\hat{y}^nIn(1-f_{w,b}(x^n)))\right]$$</p>
<p>上面求和式子中的每一项称为<strong>Cross entropy between two Bernoulli distribution</strong>,Cross entropy代表的含义是这两个distribution有多接近,如果两个distribution一模一样，那么算出来的cross entropy就是0.<br><img src="/2019/09/01/Logistic Regression/5.png" alt="5"><br>$$H(p,q) = -\sum_xp(x)In(q(x))$$</p>
<p>所以在Logistic Regression里面，我们就可以通过如下方式来定义（左边是Logistic Regression,右边是Linear Regression）<br><img src="/2019/09/01/Logistic Regression/6.png" alt="6"><br>其中Cross Entropy:<br>$$C(f(x^n),\hat{y}^n) = -\left[\hat{y}^nInf(x^n)+(1-\hat{y}^n)In(1-f(x^n))\right]$$</p>
<p>这里先留一个问题，为什么我们不选择square error而要用cross entropy呢？</p>
<h2 id="Step3-Find-the-best-function"><a href="#Step3-Find-the-best-function" class="headerlink" title="Step3 Find the best function"></a>Step3 Find the best function</h2><p>这一步很简单，用Gradient Descent找一个最好的函数即可。我们先来算$-InL(w,b)$对$w_i$的偏微分<br>$$<br>\frac{-InL(w,b)}{\partial w_i} = \sum_n-\left[\hat{y}^n\frac{Inf_{w,b}(x^n)}{\partial w_i}+(1-\hat{y}^n)\frac{In(1-f_{w,b}(x^n))}{\partial w_i}\right]<br>$$</p>
<p>$$\frac{Inf_{w,b}(x^n)}{\partial w_i} = \frac{\partial Inf_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i}$$</p>
<p>注意：<br>$$f_{w,b}(x)=\sigma(z)=1/(1+exp(-z))$$<br>$$z = w\cdot x + b =\sum_iw_ix_i+b$$</p>
<p>所以有</p>
<p>$$<br>\begin{equation}<br>\begin{split}<br>\frac{\partial Inf_{w,b}(x)}{\partial z} &amp;= \frac{\partial In\sigma (z)}{\sigma z} \\<br>&amp;= \frac{1}{\sigma (z)}\frac{\partial \sigma(z)}{\partial z} \\<br>&amp;= \frac{1}{\partial (z)}\sigma (z)(1-\sigma (z)) \\<br>&amp;= (1-\sigma(z))<br>\end{split}<br>\end{equation}<br>$$</p>
<p>同时<br>$$\frac{\partial z}{\partial w_i} = x_i$$</p>
<p>同理可得</p>
<p>$$<br>\begin{equation}<br>\begin{split}<br>\frac{In(1-f_{w,b}(x^n))}{\partial w_i} &amp;= \frac{In(1-f_{w,b}(x))}{\partial z}\frac{\partial z}{\partial w_i} \\<br>&amp;= \frac{In(1-\sigma (z))}{\sigma z}x_i \\<br>&amp;= -\frac{1}{1-\sigma(z)}\frac{\partial \sigma(z)}{\partial z}x_i \\<br>&amp;= -\frac{1}{1-\sigma(z)}\sigma(z)(1-\sigma(z))x_i \\<br>&amp;= -\sigma(z)<br>\end{split}<br>\end{equation}<br>$$</p>
<p>最终整理可得到<br>$$<br>\begin{equation}<br>\begin{split}<br>-\frac{InL(w,b)}{\partial w_i} &amp;=\sum_n-\left[\hat{y}^n(1-f_{w,b}(x^n))x_i^n - (1-\hat{y}^n)f_{w,b}(x^n)x_i^n\right] \\<br>&amp;=\sum_n-\left[\hat{y}^n-\hat{y}^nf_{w,b}(x^n) -f_{w,b}(x^n)+\hat{y}^nf_{w,b}(x^n) \right]x_i^n \\<br>&amp;=\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n<br>\end{split}<br>\end{equation}<br>$$</p>
<p>我们发现最后得到的结果是非常neat的。<br>用Gradient Descent来update<br>$$w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$</p>
<p>分析上式的含义，其中$\hat{y}^n-f_{w,b}(x^n)$代表目标与实际输出的差距，<strong>Larger difference,larger update</strong>,这个结果还是比较合理的。<br>到这里我们可以发现，Linear regression与Logistic regression它们做Gradient Descent的式子是一模一样的<br>$${\tt Logistic\ regression}: w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$  $${\tt Linear\ regression}: w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$</p>
<p>唯一不同之处在于Logistic regression中$\hat{y}^n$一定是0或1，而Linear regression中$\hat{y}^n$可以是任意的real number.</p>
<h1 id="Why-not-Square-Error"><a href="#Why-not-Square-Error" class="headerlink" title="Why not Square Error"></a>Why not Square Error</h1><p>回到前面提到的问题，Logistic Regression为什么不能用Square Error?<br>我们用Square Error看看会怎样，还是一样的3个步骤</p>
<ol>
<li><strong>step1</strong><br>$$f_{w,b}(x) = \sigma(\sum_iw_ix_i+b)$$</li>
<li><strong>step2</strong><br>Training data:$(x^n,\hat{y}^n)$,$\hat{y}^n$:1 for class 1,0 for class 2<br>$$L(f)=\frac{1}{2}\sum_n\left(f_{w,b}(x^n)-\hat{y}^n\right)^2$$</li>
<li><strong>step3</strong><br>$$<br>\begin{equation}<br>\begin{split}<br>\frac{\partial(f_{w,b}(x)-\hat{y})^2}{\partial w_i} &amp;=2(f_{w,b}(x)-\hat{y})\frac{\partial f_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i} \\<br>&amp;= 2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i \\<br>\end{split}<br>\end{equation}<br>$$</li>
</ol>
<p>这里我们可以发现一个问题：<br>当$\hat{y}^n=1$,如果$f_{w,b}(x^n)=1$,这时得到的结果是perfect的(close to target),这个时候$\frac{\partial L}{\partial w_i}=0$,这是我们所期望的。<br>而当$\hat{y}^n=1$,如果$f_{w,b}(x^n)=0$,这时得到的结果是不够好的(far from target),然而这个时候却依然有$\frac{\partial L}{\partial w_i}=0$<br>对于$\hat{y}^n=0$依然存在上面问题。<br>如果我们把参数的变化对Total Loss做图的话，选择Cross Entropy跟Square Error的图像如下，其中黑色的是Cross Entropy,红色的是Square Error<br><img src="/2019/09/01/Logistic Regression/7.png" alt="7"><br>可以发现，对于Cross Entropy，距离目标越远，微分值越大，参数update时变化越大，这个是符合我们期望的。<br>对于Square Error，无论是距离目标远或近，微分值都比较小，这样参数update的速度是非常慢的。</p>
<h1 id="Discriminative-v-s-Generative"><a href="#Discriminative-v-s-Generative" class="headerlink" title="Discriminative v.s. Generative"></a>Discriminative v.s. Generative</h1><p>上面Logistic Regression的方法我们称之为<strong>Discriminative</strong>方法，而上一章所讲的几率模型称为<strong>Generative</strong>方法。<br>在几率模型中，只要我们将covariance matrix设成是共享的，它们两者的Model其实是一模一样的：<br>$$P(C_1|x)=\sigma(wx\cdot +b)$$<br>Discriminative可以直接将w和b找出来，而用Generative方法要先找到$\mu^1$,$\mu^2$$\sigma^{-1}$,然后再计算下面式子<br>$$w^T = (\mu^1-\mu^2)^T\Sigma^{-1}$$</p>
<p>$$b=-\frac{1}{2}(\mu^1)^T(\Sigma)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T(\Sigma)^{-1}\mu^2+In\frac{N_1}{N_2}$$</p>
<p>思考一下，用Discriminative和Generative两种方法求得的w和b是相同的吗？答案是不同！<br><strong>The same model(function set),but different function is selected by the same training data.</strong><br>在Discriminative里面，我们没有做对probability distribution做任何的假设，而在Generative里面，我们有对probability distribution做出某些假设（例如Gaussain、Navie Bayes）<br>那么用哪一种方法找到的w和b是比较好的呢？Discriminative的表现常常要比Generative要好。我们用下面的例子来说明<br><img src="/2019/09/01/Logistic Regression/8.png" alt="8"><br>假设我们有一笔training data如上，每一个data有2个feature,总共有$1+3\times 4=13$个data.其中Class1的data两个feature都是1，Class2的training data有3种不同类型。<br>现在如果给我们一个testing data,它的两个feature值都是1，作为人来判断，显然它是属于Class 1的。<br>但是Naive Bayes会怎么认为吗？所谓Naive Bayes即假设所有的feature产生的几率是independent:<br>$$P(x|C_i)=P(x_1|C_i)P(x_2|C_i)$$</p>
<p>接着我们算Prior:<br>$$P(C_1)=\frac{1}{13}$$ $$P(C_2)=\frac{12}{13}$$</p>
<p>另外<br>$$P(x_1=1|C_1)=1$$ $$P(x_2=1|C_1)=1$$<br>$$P(x_1=1|C_2)=\frac{1}{3}$$ $$P(x_2=1|C_2)=\frac{1}{3}$$ </p>
<p>还是给一个两个feature值都是1的Testing data,可以算出它属于Class 1的几率是：<br>$$P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1\times 1\times \frac{1}{13}}{1\times 1\times \frac{1}{13}+\frac{1}{3}\times \frac{1}{3}\times \frac{12}{13}}&lt;0.5$$ </p>
<p>Navie Bayes认为该data居然是属于Class 2!!这和我们的直觉是相反的！<br>对于Navie Bayes来说，data的两个feature是independent产生的，在Class 2之所以没有观测到所给的test data,是因为我们sample的不够多。</p>
<p>这再一次说明，Generative跟Discriminative的差别就在于，<strong>Generative Model它有自己做了某些假设</strong>，即它有自己“脑补”某些事情。通常情况下，“脑补”可能不是一件好的事情，但是如果是在data很少的情况下，“脑补”有时候也是有用的。有些时候Generative Model也是有优势的。比如我们的train data很少，因为Discriminative Model完全没有做任何假设，它是看着data说话的，所以它的performance受data量的影响很大。Generative Model受Data量影响相对较少。</p>
<p>Benefit of generative model</p>
<ul>
<li>With the assumption of probability distribution,less training data is needed</li>
<li>With the assumption of prabability distribution,more robust to the noise</li>
<li>Priors and class-dependent probabilities can be estimated from different sources.</li>
</ul>
<h1 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h1><p>之前我们以二分类为例子，如果要分的类别大于2呢？假设有3个Class，每个Class都有自己的${\bf w}$和b，如下<br>$$C_1:w^1,b_1\quad z_1=w^1\cdot x+b_1$$ $$C_2:w^2,b_2\quad z_2=w^2\cdot x+b_2$$ $$C_3:w^3,b_3\quad z_3=w^3\cdot x+b_3$$</p>
<p>接下来我们把$z_1$、$z_2$、$z_3$丢进一个<strong>softmax</strong> function,这个function做的事情如下<br><img src="/2019/09/01/Logistic Regression/9.png" alt="9"><br><em>Probability</em></p>
<ul>
<li>$1&gt;y_i&gt;0$</li>
<li>$\sum_iy_i=1$</li>
</ul>
<p>原来的$z_1$、$z_2$、$z_3$可以是任何值，做完softmax以后，output的值一定是介于1和0之间。softmax会对最大的值做<strong>强化</strong>，使大的值和小的值之间的差距会被拉的更开。$y_i=P(C_i|x)$,即得到input x是属于$C_i$的几率。<br>为什么要用softmax function是可以推导出来的，这里不做探讨(google maximum entropy)。</p>
<p>总结上面所做的事情，如下<br><img src="/2019/09/01/Logistic Regression/10.png" alt="10"><br>$y$与$\hat{y}$都是一个probability distribution,$\hat{y}$我们可以设成如下<br>$$if\ x\in class1 \quad \quad \hat{y}=\begin{bmatrix}1&amp;0&amp;0\end{bmatrix}^T$$ $$if\ x\in class2 \quad \quad \hat{y}=\begin{bmatrix}0&amp;1&amp;0\end{bmatrix}^T$$ $$if\ x\in class3 \quad \quad \hat{y}=\begin{bmatrix}0&amp;0&amp;1\end{bmatrix}^T$$</p>
<h1 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h1><p>Logistic Regression其实是有非常强的限制，假设有下面这个case,<br><img src="/2019/09/01/Logistic Regression/11.png" alt="11"><br>我们用Logistic Regression对上面的class1和class2做分类，发现这件事情是办不到的（即让两个红色点的输出几率大于0.5，而两个蓝色点的几率小于0.5），其原因是Logistic Regression的Boundary就是一条直线,而无论我们怎么去画一条直线，都不能将红色的点划在一边，而蓝色的点划在另一边。<br><img src="/2019/09/01/Logistic Regression/12.png" alt="12"> </p>
<h2 id="Feature-Transformation"><a href="#Feature-Transformation" class="headerlink" title="Feature Transformation"></a>Feature Transformation</h2><p>为解决上面问题，还是用Logistic Regression,我们可以用<strong>Feature Transformation</strong>。我们可以对不好的feature进行一些转换，使得Logistic Regression变得好处理。<br>我们将$\begin{bmatrix}x_1&amp;x_2\end{bmatrix}^T$转化成$\begin{bmatrix}x_1^{\prime}&amp;x_2^{\prime}\end{bmatrix}^T$</p>
<p>我们定义$x_1^{\prime}$是点到$\begin{bmatrix}0&amp;0\end{bmatrix}^T$的距离，$x_2^{\prime}$是点到$\begin{bmatrix}1&amp;1\end{bmatrix}^T$的距离，经过这样的转换，我们可以对转换后的点找到一条Boundary<br><img src="/2019/09/01/Logistic Regression/13.png" alt="13"><br>然后，要找到一个Transform是比较麻烦的（Not always easy to find a good transformation）。<strong>我们期望这个Transformation是由机器自己产生的</strong>。</p>
<h2 id="Cascading-logistic-regression-models"><a href="#Cascading-logistic-regression-models" class="headerlink" title="Cascading logistic regression models"></a>Cascading logistic regression models</h2><p>为了让机器自己产生Transformation,我们将多个logistic regression串联起来，如下图<br><img src="/2019/09/01/Logistic Regression/14.png" alt="14"><br>前面两个logistic regression负责坐标转换，使得不同分类的点可以被一条直线分开，最后的logistic regression负责分类。</p>
<p>所以将某些logistic regression的输出作为其他logistic regression的输入，这样叠在一起，它就变得powerful。<br><img src="/2019/09/01/Logistic Regression/15.png" alt="15"><br>其中每一个logistic regression称为<strong>Neuron</strong>,把这些Neuron串起来，就叫做<strong>Neural Network</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8" target="_blank" rel="noopener">Demystifying Cross-Entropy</a></li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/27/Classification/" rel="next" title="Classification">
                <i class="fa fa-chevron-left"></i> Classification
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/03/Brief Introduction of Deep Learning/" rel="prev" title="Brief Introduction of Deep Learning">
                Brief Introduction of Deep Learning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Liang Qi">
            
              <p class="site-author-name" itemprop="name">Liang Qi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Logistic-Regression-amp-Linear-Regression"><span class="nav-number">1.</span> <span class="nav-text">Logistic Regression &amp; Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Step1-Model-function-set"><span class="nav-number">1.1.</span> <span class="nav-text">Step1 Model(function set)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step2-Goodness-of-a-Function"><span class="nav-number">1.2.</span> <span class="nav-text">Step2 Goodness of a Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Step3-Find-the-best-function"><span class="nav-number">1.3.</span> <span class="nav-text">Step3 Find the best function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Why-not-Square-Error"><span class="nav-number">2.</span> <span class="nav-text">Why not Square Error</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Discriminative-v-s-Generative"><span class="nav-number">3.</span> <span class="nav-text">Discriminative v.s. Generative</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multi-class-Classification"><span class="nav-number">4.</span> <span class="nav-text">Multi-class Classification</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Limitation-of-Logistic-Regression"><span class="nav-number">5.</span> <span class="nav-text">Limitation of Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Transformation"><span class="nav-number">5.1.</span> <span class="nav-text">Feature Transformation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cascading-logistic-regression-models"><span class="nav-number">5.2.</span> <span class="nav-text">Cascading logistic regression models</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">6.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liang Qi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>