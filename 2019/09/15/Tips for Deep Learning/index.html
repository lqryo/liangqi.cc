<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Recipe of Deep LearningDeep Learning的三个step:step 1: define a set of functionstep 2: goodness of functionstep 3: pick the best function做完以上事情后，会得到一个Neural Network。接下来要做的事情就是，检查这个Neural Network在Training">
<meta property="og:type" content="article">
<meta property="og:title" content="Tips for Deep Learning">
<meta property="og:url" content="http://yoursite.com/2019/09/15/Tips for Deep Learning/index.html">
<meta property="og:site_name" content="Qi-Liang&#39;blog">
<meta property="og:description" content="Recipe of Deep LearningDeep Learning的三个step:step 1: define a set of functionstep 2: goodness of functionstep 3: pick the best function做完以上事情后，会得到一个Neural Network。接下来要做的事情就是，检查这个Neural Network在Training">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/1.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/2.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/3.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/4.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/5.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/6.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/7.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/8.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/9.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/10.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/11.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/12.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/13.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/14.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/15.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/16.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/17.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/18.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/19.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/20.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/21.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/22.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/23.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/24.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/25.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/26.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/27.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/28.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/29.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/30.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/31.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/32.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/33.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/34.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/35.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/36.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/37.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/38.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/39.png">
<meta property="og:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/40.png">
<meta property="og:updated_time" content="2019-10-10T13:57:53.131Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tips for Deep Learning">
<meta name="twitter:description" content="Recipe of Deep LearningDeep Learning的三个step:step 1: define a set of functionstep 2: goodness of functionstep 3: pick the best function做完以上事情后，会得到一个Neural Network。接下来要做的事情就是，检查这个Neural Network在Training">
<meta name="twitter:image" content="http://yoursite.com/2019/09/15/Tips%20for%20Deep%20Learning/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/09/15/Tips for Deep Learning/">





  <title>Tips for Deep Learning | Qi-Liang'blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qi-Liang'blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">stay young,stay simple</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/15/Tips for Deep Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Tips for Deep Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-15T23:14:47+08:00">
                2019-09-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Recipe-of-Deep-Learning"><a href="#Recipe-of-Deep-Learning" class="headerlink" title="Recipe of Deep Learning"></a>Recipe of Deep Learning</h1><p>Deep Learning的三个step:<br><strong>step 1: define a set of function</strong><br><strong>step 2: goodness of function</strong><br><strong>step 3: pick the best function</strong><br>做完以上事情后，会得到一个Neural Network。接下来要做的事情就是，检查这个Neural Network在Training Data上有没有得到好的结果。如果没有的话，要回头看看上面3个step哪个出了问题，我们可以做什么修改。</p>
<p>先检查train data的performance，其实是deep learning一个非常特别的地方。像其他的方法，例如K nearest neighbor或decision tree,它们做完以后在train data上的正确率就是100%。所以有人说，deep learning里面model的参数这么多，感觉很容易overfitting,但其实它不容易overfitting（nearest neighbor和decision tree在train data上正确率为100%,才容易overfitting）。在deep learning里面，overfitting并不是我们第一个会遇到的问题，它可能在training set上，根本没有一个好的正确率。</p>
<p>假设我们在training set上已经得到好的performance了，接下来将network apply到testing set上，如果现在得到的结果不好的话，就是overfitting。过程如下图。<br><img src="/2019/09/15/Tips for Deep Learning/1.png" alt="1"></p>
<h2 id="Do-not-always-blame-Overfitting"><a href="#Do-not-always-blame-Overfitting" class="headerlink" title="Do not always blame Overfitting"></a>Do not always blame Overfitting</h2><p>不是所有不好的performance就是overfitting，下面是文献中的图，横坐标是model update的次数，我们发现56层的network相比20层的network，它的performance比较差。有人可能会得到结论：56层的参数太多了，这个是overfitting，但是真的是这个吗？<br><img src="/2019/09/15/Tips for Deep Learning/2.png" alt="2"></p>
<p>在我们得到overfitting的结论之前，先检查一下在training set上的结果，对某些方法来所（比如k nearest neighbor或decision tree）不用检查这些事，但是对neural network来说，是需要检查这些事情的。因为有可能在training data的结果如下图，20层的performance在training data上本来就比56层要好，为什么会这样呢？在做neural network training的时候，有太多太多的问题，可以让training的结果是不好的（比如local minimum的问题），所以可能56层的network卡在了local minimum的地方，从而得到了一个差的参数。这个并不是overfitting，而是根本就没有train好。理论上20层的network可以做到的事情，56层的network一定可以做到。所以56层的network比20层差并不是它能力不够，所以这个应该不是underfitting。<br><img src="/2019/09/15/Tips for Deep Learning/3.png" alt="3"></p>
<p>所以在deep learning的文献上，如果当你读到一个方法的时候，永远要想一下说，这个方法是要解什么样的问题。因为在deep learning上有两个问题，一个是training set上的performace不好，一个是testing set上的performance不好。当只有一个方法propose的时候，它往往就是针对这两个问题的其中一个来做处理（例如dropout，dropout是在testing的结果不好的时候才会apply）。<br><img src="/2019/09/15/Tips for Deep Learning/4.png" alt="4"></p>
<h2 id="Vanishing-Gradient-Problem"><a href="#Vanishing-Gradient-Problem" class="headerlink" title="Vanishing Gradient Problem"></a>Vanishing Gradient Problem</h2><p>如果是在training data上的结果不好，可以考虑是不是在做network架构设计的时候是否设计得当。举例来说，我们可能用的是比较不好的activation function，那么就需要换新的activation function。在1980年代的时候，比较常用的activation function是sigmoid function，如下图，当layer原来越多的时候，accuracy逐渐下降直到崩溃。<br><img src="/2019/09/15/Tips for Deep Learning/5.png" alt="5"></p>
<p>发生这种现象的一个原因是<strong>Vanishing Gradient Problem</strong>，当network被叠的很深的时候，在靠近input的几个layer，参数对最后loss function的微分会是很小，而在比较靠近output的地方，它的微分值会很大。因此当设定同样的learning rate的时候，会发现靠近input的地方，它的参数update很慢，靠近output的地方，它的参数update是很快的，所以我们会发现在靠近input的参数几乎还是random的时候，output就已经converge了，换句话说，在靠近input的参数还是random的时候，output地方就已经根据这些random的结果，找到了一个local minimum，然后它就converge了，这个时候就会发现参数loss下降的速度变得很慢。<br><img src="/2019/09/15/Tips for Deep Learning/6.png" alt="6"></p>
<p>为什么会有这个现象发生呢？如果去看backpropagation的式子，会发现用sigmoid function会导致这件事情。但我们现在从直觉上来想为什么这件事情会发生。</p>
<p>某一个参数$w$对total cost $C$的偏微分$\frac{\partial C}{\partial w}$的意思就是说当把某一个参数做小小的变化的时候，它对cost的影响会是怎样。我们把第一个layer里的某个参数加上$\Delta w$,看看对network的output和它的target之间的loss有什么样的影响。如果$\Delta w$很大，通过sigmoid function的时候，这个output是会变小的。也就是说，改变了某一个参数的weight，对某一个neuron的output的值会有影响。但是这个影响是会衰减的。因为sigmoid function会把正无穷到负无穷大之间的值都压到0到1之间。也就是说，如果input变化很大，通过sigmoid function以后，它output的变化会是很小的，并且每通过一次sigmoid function，变化就衰减一次。network越深，它衰减的次数就越多，直到最后它对output的影响是非常小的，也造成对cost的变化很小，所以在靠近input的那些weight，它对它这个Gradient的值是小的。<br><img src="/2019/09/15/Tips for Deep Learning/7.png" alt="7"></p>
<p>要怎么解决上面问题呢？改一下activation function，现在比较常用的activation function是<strong>Rectified Linear Unit(ReLU)</strong>,它的图像如下。<br><img src="/2019/09/15/Tips for Deep Learning/8.png" alt="8"></p>
<p>选用ReLU有以下几个理由：</p>
<ul>
<li><strong>Fast to compute</strong><br>跟sigmoid function比起来，它的运算是快很多的（sigmoid里面还有exponential,那个是很慢的）。</li>
<li><strong>Biological reason</strong><br>ReLU的想法其实是有一些生物上的理由的。</li>
<li><strong>Infinite sigmoid with different biases</strong><br>ReLU等同于无穷多个sigmoid function叠加的结果。</li>
<li><strong>Vanishing gradient problem</strong><br>如下图是一个activation function为ReLU的network，ReLU它作用在两个不同的region，一个是当input&gt;0时，input=output，此时activation function是linear的；另一个是input&lt;0,output=0。</li>
</ul>
<p><img src="/2019/09/15/Tips for Deep Learning/9.png" alt="9"></p>
<p>对于output是0的network来说，它其实对整个network一点影响也没有，可以把它从整个network里面拿掉。当把output是0的neuron拿掉，剩下的neuron(input=output)都是linear的，整个network，就是一个很瘦长的linear network。这个时候就不会有activation function递减的问题了。<br><img src="/2019/09/15/Tips for Deep Learning/10.png" alt="10"></p>
<p>当我们用ReLU是，network变成了一个linear network,这样不是变得很弱吗？起始可以这样解释，这个network在整体来说还是non-linear的，当每一个neuron它operation region是一样的时候，它是linear的，换句话说，只对input做小小的改变，不改变它的operation region，它是一个linear的network，但是如果对input做比较大的改变，改变了neuron的operation region的话，它就变成是non-linear的了。</p>
<p>还有另外一个问题，ReLU不能微分。在实作上，它除了在x=0处不能微分外，其余的点都是可微的。在实际训练的时候，不可能刚好有点落到不可微分的点上。</p>
<h2 id="ReLU-variant"><a href="#ReLU-variant" class="headerlink" title="ReLU-variant"></a>ReLU-variant</h2><p>ReLU还用种种的变形。</p>
<h3 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h3><p>原来的ReLU,在输入小于0的时候，输出也是0，这个时候微分是0，这样我们就没法update参数了。 所以我们想让input小于0的时候，output还是有一点点的值。一种ReLU的变形如下，它被称为Leaky ReLU<br><img src="/2019/09/15/Tips for Deep Learning/11.png" alt="11"></p>
<h3 id="Parametric-ReLU"><a href="#Parametric-ReLU" class="headerlink" title="Parametric ReLU"></a>Parametric ReLU</h3><p>Leaky ReLU在输入小于0的时候，$z$前面的系数被固定为0.01，那可以设成其他值吗？所以就有人提出了Parametric ReLU。如下图，在输入小于0时，$a=\alpha z$，$\alpha$是一个network的参数，它可以透过training data被学出来。甚至每个neuron都可以有不同的$\alpha$值。<br><img src="/2019/09/15/Tips for Deep Learning/12.png" alt="12"></p>
<h1 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h1><p><strong>ReLU is a special cases of Maxout</strong></p>
<p>在Maxout里面，我们让network去自动学它的activation function，用training data来决定activation function应该长什么样子。</p>
<p>假如现在有一个2维度输入，它们分别乘上不同的weight得到输出（5，7，-1，1）。不来这些值要通过sigmoid function或ReLU或其他的function得到另一个值，但是现在在Maxout network里面，我们要做的事情是这样：把这些输出group起来，哪些值需要被group起来这件事是事先决定的（图中5、7是一组，-1、1是一组）。在同一个组里面选一个值最大的作为output（这就跟CNN里面的max pooling是一样的，但现在我们不是在image上做max pooling，而是在一个layer上做max pooling）。我们把layer里面本来要放到neuron里面的activation function输入值group起来，只选最大的当作output，这样就不用activation function了，下图中得到的值是7和1。我们可以把group和Max一起看作一个neuron。后续的操作与之相同。注意，在实作上要把几个elements放到一个group里面是我们自己决定的，和network struct一样，这是我们需要调的一个参数。<br><img src="/2019/09/15/Tips for Deep Learning/13.png" alt="13"></p>
<p>下面说明为什么Maxout可以做到和ReLU一模一样的事情。</p>
<p>下面有一个ReLU的neuron，显然我们有$z = x\cdot w + b$,$z$和$x$的关系是linear的，$a$和$x$的关系是图中绿色的线。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/14.png" alt="14"></p>
<p>如果我们用Maxout network，如下图，假设有一组weight是w和b，另一种weight是0和0。现在$z_1$和$x$之间的关系是图中蓝色的线，$z_2$和$x$之间的关系是图中红色的线，那么$a$与$x$的关系就是图中绿色的线。由此可以看出，ReLU是Maxout可以做到的事情（只要设计出正确的参数）。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/15.png" alt="15"></p>
<p>但是Maxout还可以做出更多的不同的activation function。比如如果上面0和0这组weight改为$w^{\prime}$和$b^{\prime}$，那么$a$与$x$的关系就会发生变化。如下图。这个activaton function长什么样子是由$w^{\prime}$和$b^{\prime}$决定的，所以它是一个<strong>Learnable Activation Function</strong>。每个neuron，根据它不同的weight，可以有不同的activation function。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/16.png" alt="16"></p>
<p>Learnable activation function有以下性质：</p>
<ul>
<li><p>Activation function in maxout network can be any piecewise linear convex function.</p>
</li>
<li><p>How many pieces depending on how many elements in a group.</p>
</li>
</ul>
<p><img src="/2019/09/15/Tips for Deep Learning/17.png" alt="17"></p>
<h2 id="Maxout-Training"><a href="#Maxout-Training" class="headerlink" title="Maxout - Training"></a>Maxout - Training</h2><p>Maxout network里面有max函数，不能微分，那我们要如何训练呢？如下图，红色方框内是比较大的值，较大的值就是Max operation的output。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/18.png" alt="18"></p>
<p>将没被接到Max的elements去掉，Max operation其实就是linear的。所以，当我们在做Maxout的时候，给定一个input,其实是得到一个比较细长的Network，我们train的其实也是这个比较细长的linear network的参数。<br><img src="/2019/09/15/Tips for Deep Learning/19.png" alt="19"></p>
<p>那那些没有被train到的elements怎么办？这看起来表面上是一个问题，但实际上不是的。因为当我们给定不同的input的时候，我们得到的$z$的值是不一样的，对应的network structrue也是不一样的。而因为我们有很多笔training data,network structrue不断地变换，最后每一个weight在实际上都会被train到。（其实训练Maxout和Max pooling是一模一样的做法）</p>
<h1 id="Adaptive-learning-rate"><a href="#Adaptive-learning-rate" class="headerlink" title="Adaptive learning rate"></a>Adaptive learning rate</h1><h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p><strong>Adagrad</strong>的做法就是每一个parameter都要有不同的learning rate，这个learning rate是一个固定的值$\eta$除以过去所有tgradient的值的平方和开根号。</p>
<p>$$w^{t+1} \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i=0}^t(g^i)^2}}g^t$$</p>
<p>Adagrad的精神就是如果考虑两个参数$w_1$和$w_2$，如下图，$w_1$在蓝色箭头方向是比较平坦的（gradient比较小），我们就给他比较大的learning rate。反之，在绿色箭头方向比较陡峭，就给他比较小的learning rate。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/20.png" alt="20"></p>
<p>但是实际上，我们面对的问题可能比adagrad能处理的问题更加复杂。我们之前在做linear regression的时候，我们看到的optimation的loss function是一个convex的形状，但当我们做deep learning的时候，这个loss function可以是任何形状。比如我们的Error Surface可能是下图的形状，这样我们就遇到一个问题：就算是同一个方向上，learning rate也必须能够快速的变动（对于convex function，一个方向是很平坦就一直平坦，很陡峭就一直陡峭）。比如$w_1$改变的方向，在某个区域很平坦，但是另外的区域又突然变得很陡峭。所以真正处理deep learning的问题，用Adagrad可能是不够的，需要有更dynamic地调整learning rate的方法。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/21.png" alt="21"></p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p><strong>RMSProp</strong>是Adagrad的进阶版，如下图。可以手动调整$\alpha$的值，$\alpha$的值越小，表示我们倾向于相信新的gradient所告诉我们的关于error surface的平滑或陡峭的程度。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/22.png" alt="22"></p>
<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>除了learning rate的问题以外，我们在做deep learning的时候，可能会卡在local minimum或saddle point，</p>
<p><img src="/2019/09/15/Tips for Deep Learning/23.png" alt="23"></p>
<p>有一个从现实世界得到灵感的方法用来解决local minimum问题，如下图，在真实的世界里面，将一个球从左上角滚下来，由于惯性的关系，它到达Local minimum的地方还是可以继续向前走，最终它可以走到比Local minimum要更好的地方。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/24.png" alt="24"></p>
<p>我们想要将这个惯性的特性塞到gradient descent里面去。这件事情就叫做<strong>Momentum</strong>。我们先来简单复习以下gradient descent。</p>
<p>一般的gradient descent，先选一个初始的值$\theta^0$,然后计算它的gradient，然后对gradient的反方向乘上learning rate $\eta$，得到$\theta^1$,然后依次继续进行，一直到gradient接近0的时候停止。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/25.png" alt="25"></p>
<p>当我们加上<strong>Momentum</strong>,我们每一次移动一个方向，不再是只有考虑gradient，而是现在的gradient加上在前一个时间点移动的方向。下图中$v$用来记录在前一个时间点移动的方向。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/26.png" alt="26"></p>
<p>可以用另一个方法来理解这件事。$v^i$其实是过去所有算出来的gradient的总和。如下<br><img src="/2019/09/15/Tips for Deep Learning/27.png" alt="27"></p>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>RMSProp+Momentum就是<strong>Adam</strong></p>
<p><img src="/2019/09/15/Tips for Deep Learning/28.png" alt="28"></p>
<hr>
<p>之前我们所讲的都是在training set的结果上不好的话怎么办，接下来要探讨如果在training data上得到更好的结果，但是在testing data上的结果仍然不好，那有什么可行的方法。下面会介绍3个方法。</p>
<h1 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h1><p>随着training,如果learning rate调的对的话，Total loss通常会越来越小。但是由于Testing set和Training set它们的distribution并不完全一样，所以可能当training的loss逐渐减少的时候，Testing set的loss却反而上升了。理想上，假如我们知道Testing data上loss的变化，我们应该停在不是training set最小而是Testing set最小的地方。在Training的时候不要一直Training下去，可能要早一点停下来。但实际上我们不知道Testing set,更不知道Testing set的error是什么，所以我们其实会用Validation set来verify这件事情。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/29.png" alt="29"></p>
<p>这里需要解释一下，上面说得Testing set并不是指真正的Testing set，它指得是已有Label data的Testing set。</p>
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><p>重新定义要去minimized的loss function。下图使用L2来做regularization。在做regularization的时候，我们一般是不会考虑bias这项。因为加regularization的目的是要让我们的function更平滑，而bias与function的平滑程度通常是没有关系的。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/30.png" alt="30"></p>
<p>计算加了L2 Regularization的loss function的Gradient，对update式子整理如下：</p>
<p><img src="/2019/09/15/Tips for Deep Learning/31.png" alt="31"></p>
<p>我们可以发现，在update式子的时候，每次在update之前，都会先将参数乘上$1-\eta\lambda$,learning rate $\eta$通常是一个很小的值，$\lambda$一般也是很小的值，$1-\eta\lambda$就是一个接近1的值。这样，$(1-\eta\lambda)w^t$会让我们的参数越来越接近0。但是这样参数最后不就会通通变为0吗？由于有后面的项$-\eta\frac{\partial L}{\partial w}$，它会与前面的项取得平衡，所以参数不会统统变0。因为在使用L2 Regularization的时候，我们每次都会让weight小一点，所以这种方法也叫<strong>Weight Decay</strong>。</p>
<p>在Deep learning里面，Regularization的重要性跟其他方法比如SVM比起来并没有那么高。一个可能的原因是，Early Stopping中我们可以决定training什么时候被停下来，因为我们在做neural network的时候通常初始参数时都是从一个接近0的值来开始初始，update的时候就是让参数离0越来越远。而Regularization的目的就是希望参数不要离0太远。让参数不要离0太远加上Regularization所造成的效果跟减少update次数所造成的效果其实可能是很像的。SVM里面没有Early Stopping这件事，一步就走到结果了。</p>
<p>我们也可以做L1 Regularization。虽然绝对值不能微分，但其实不能微分的点就一个，不用管他就好了。如果真的出现在0这个点的情况，随便给个就就好了。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/32.png" alt="32"></p>
<p>比较L1和L2会发现，它们同样是让参数变小，但它们做的事情是略有不同的。L1每次减掉的值都是固定的，而L2是每次都乘上一个小于1固定的值。所以对L2来说，只要$w$出现很大的值，这个$w$下降的很快，但是L1则不同。很大的$w$下降的速度和其他很小的$w$是一样的，通过L1的training以后，训练出来的network里面还是有一些很大很大的值。很小的值对L2来说它下降的速度很慢，所以L2训练出来的结果里面会保留很多接近0的值。L1每次都下降一个固定的值，所以在L1里面不会保留很多很小的值。所以使用L1结果就是：训练出来的参数里面有很多接近0的值，但是也有很大的值。而L2训练结果是参数平均都比较小。</p>
<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>在training的时候，每一次我们要update参数之前，都对network里面的每一个Neuron做Sampling，Sampling决定每一个Neuron要不要被丢掉，每一个Neuron有$p%$的几率会被丢掉。一个Neuron被丢掉后，和它相连的weight也失去了作用。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/33.png" alt="33"></p>
<p>做完sampling以后，network的structre就得比较细长，然后再去train这个比较细长的network。注意，这个sampling是每次update参数之前都要做一次，所以每次update参数的时候，拿来training的那个network structure是不一样。每次都要重新做一次sample。当我们在training的时候使用dropout的时候，performance是会变差的。但dropout要做的事情，就是让training set上的结果变成而在testing data上的结果变好。</p>
<p>在testing的时候要注意两件事，一是testing时不加dropout，另一个是在testing的时候，假设training时dropout rate是$p%$，在testing的时候，所有weight都要乘(1-p%),</p>
<p><img src="/2019/09/15/Tips for Deep Learning/34.png" alt="34"></p>
<p>为什么Dropout有用？直观的解释如下：</p>
<p><img src="/2019/09/15/Tips for Deep Learning/35.png" alt="35"></p>
<p>为什么testing的时候要乘(1-p%)，</p>
<p><img src="/2019/09/15/Tips for Deep Learning/36.png" alt="36"></p>
<h2 id="Dropout-is-a-kindle-of-ensemble"><a href="#Dropout-is-a-kindle-of-ensemble" class="headerlink" title="Dropout is a kindle of ensemble"></a>Dropout is a kindle of ensemble</h2><p>Dropout是一种终极的ensemble的方法。ensemble的解释如下：</p>
<p>有一个很大的training set,每次从training set里面只sample一部分data出来。之前讲bias和variance trade off的时候有提到，打靶有两种情况，一种是bias很大，一种是variance很大，所以都打不准。如果有一个很复杂的model,它往往是bias准，但是variance很大。但如果这个复杂的model有很多个，虽然它variance很大，但最后平均起来，结果就很准。今天Ensemble要做的事情，其实就是要利用这个特性。我们train很多个model,把原来的training data里面sample出很多subset，然后train很多model。每个model甚至可以structure不一样，虽然每一个model可能variance很大，如果它们都是很复杂的model,平均起来这个bias就很小。所以真正在test的时候，train了一把model，然后在testing的时候丢一笔training data进来，它通过所有的model,得到一大堆的结果，再把这一大堆结果平均起来，当作我们最后的结果。如果model很复杂的话，这一招往往有用。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/37.png" alt="37"></p>
<p>为什么说dropout是一个终极的ensemble方法呢？在dropout的时候，拿一个minibatch出来，要update参数的时候，都会做一次sample。假设有M个neuron，每个neuron可以drop或不drop，可能的network的数目有$2^M$个。我们做dropout等于是在train这$2^M$个network,每次都只用一个minibatch的data去train一个network。因为最后update的次数是有限的，可能没有办法把$2^M$个network每个都train一遍，但还是会train很多的参数。做几次update参数，就train几次network，但每个network只用一个batch来train。每个network只用一个batch来train可能会让人很不安，一个batch可能才100笔data，怎么train一个network呢？但没有关系，因为这些不同的network之间的参数是shared。所以虽然一个network只用一个batch来train，但是一个weight可能用很多个batch来train（只要它没有被dropout丢掉）。所以，dropout就是train了一大把的network structure，理论上每一次update参数的时候都train了一个network出来。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/38.png" alt="38"></p>
<p>Testing的时候，按照ensemble这个方法的逻辑应该就是把那一大把的network通通拿出来，然后把testing data丢到那一把network里面，每一个network都输出一个结果，然后把所有的结果平均起来，就是最终的结果。但是在实作上没办法这么做，因为这一把network是在太多了。dropout最神奇的地方就是，当把一个完整的network不做dropout，但是把它的weight乘上(1-p%)，然后把testing data输进去，得到output，神奇的就是，左边ensemble的结果，和右边的结果是可以approximate的。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/39.png" alt="39"></p>
<p>为了对上面的结论进行说明，我们来举一个例子。我们来train一个很简单的network，如下图右上角所示，它只有一个neuron，它的activation function是linear的，这里不考虑bias。如果做ensemble的话，每一个input可以被drop或不被drop，所以总共有如下4种structrue（做dropout的时候，不会drop output的neuron，只会drop hidden layer跟input的neuron）。把这4个network的output通通都average起来，最后的结果是$\frac{1}{2}w_1x_1+\frac{1}{2}w_2x_2$。这和将原来network的input都乘上1/2得到的结果一样。</p>
<p><img src="/2019/09/15/Tips for Deep Learning/40.png" alt="40"></p>
<p>在上面这个最简单的case里面，用不同的network structure做ensemble这件事情，跟我们把weight multiply一个值而不做ensemble所得到的output其实是一样的。但是，假如activation function不是linear（比如sigmoid function），还会work吗。结论就是不会equivalent，上图左边与右边相等的前提是network要是linear的。这就是dropout一个很神奇的地方，虽然不是equivalent的，但是最后的结果还是会work。所以，根据这个结论，有人提出一个想法：既然dropout在linear的network上，ensemble才会等于前一个weight，所以如果network很接近linear的话，dropout的performance应该会比较好，比如用ReLU或Maxout。并且Dropout确实在用ReLU或Maxout network的时候，performance比较好。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/09/13/sys bios05/" rel="next" title="sys bios05">
                <i class="fa fa-chevron-left"></i> sys bios05
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/09/15/Convolutional Neural Network/" rel="prev" title="Convolutional Neural Network">
                Convolutional Neural Network <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Liang Qi">
            
              <p class="site-author-name" itemprop="name">Liang Qi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Recipe-of-Deep-Learning"><span class="nav-number">1.</span> <span class="nav-text">Recipe of Deep Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Do-not-always-blame-Overfitting"><span class="nav-number">1.1.</span> <span class="nav-text">Do not always blame Overfitting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Vanishing-Gradient-Problem"><span class="nav-number">1.2.</span> <span class="nav-text">Vanishing Gradient Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ReLU-variant"><span class="nav-number">1.3.</span> <span class="nav-text">ReLU-variant</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Leaky-ReLU"><span class="nav-number">1.3.1.</span> <span class="nav-text">Leaky ReLU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parametric-ReLU"><span class="nav-number">1.3.2.</span> <span class="nav-text">Parametric ReLU</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Maxout"><span class="nav-number">2.</span> <span class="nav-text">Maxout</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Maxout-Training"><span class="nav-number">2.1.</span> <span class="nav-text">Maxout - Training</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Adaptive-learning-rate"><span class="nav-number">3.</span> <span class="nav-text">Adaptive learning rate</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Adagrad"><span class="nav-number">3.1.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSProp"><span class="nav-number">3.2.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Momentum"><span class="nav-number">3.3.</span> <span class="nav-text">Momentum</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Adam"><span class="nav-number">4.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Early-Stopping"><span class="nav-number">5.</span> <span class="nav-text">Early Stopping</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Regularization"><span class="nav-number">6.</span> <span class="nav-text">Regularization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dropout"><span class="nav-number">7.</span> <span class="nav-text">Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout-is-a-kindle-of-ensemble"><span class="nav-number">7.1.</span> <span class="nav-text">Dropout is a kindle of ensemble</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liang Qi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>