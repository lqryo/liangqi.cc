<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="IntroductionSemi-supervised Learning里面，在labeled data上面，我们有另外一组labeled data和一组unlabeled data，这些unlabeled data只有function的input而没有output。通常做semi-supervised learning的时候，常见的情形是unlabeled的数量远大于labeled数量。Semi">
<meta property="og:type" content="article">
<meta property="og:title" content="Semi-supervised Learning">
<meta property="og:url" content="http://yoursite.com/2019/10/25/Semi-supervised Learning/index.html">
<meta property="og:site_name" content="Qi-Liang&#39;blog">
<meta property="og:description" content="IntroductionSemi-supervised Learning里面，在labeled data上面，我们有另外一组labeled data和一组unlabeled data，这些unlabeled data只有function的input而没有output。通常做semi-supervised learning的时候，常见的情形是unlabeled的数量远大于labeled数量。Semi">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/1.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/2.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/3.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/4.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/5.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/6.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/7.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/8.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/9.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/10.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/11.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/12.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/13.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/14.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/15.png">
<meta property="og:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/16.png">
<meta property="og:updated_time" content="2019-10-27T02:49:25.852Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Semi-supervised Learning">
<meta name="twitter:description" content="IntroductionSemi-supervised Learning里面，在labeled data上面，我们有另外一组labeled data和一组unlabeled data，这些unlabeled data只有function的input而没有output。通常做semi-supervised learning的时候，常见的情形是unlabeled的数量远大于labeled数量。Semi">
<meta name="twitter:image" content="http://yoursite.com/2019/10/25/Semi-supervised%20Learning/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/10/25/Semi-supervised Learning/">





  <title>Semi-supervised Learning | Qi-Liang'blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qi-Liang'blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">stay young,stay simple</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/25/Semi-supervised Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Semi-supervised Learning</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-25T19:18:33+08:00">
                2019-10-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Semi-supervised Learning里面，在labeled data上面，我们有另外一组labeled data和一组unlabeled data，这些unlabeled data只有function的input而没有output。通常做semi-supervised learning的时候，常见的情形是unlabeled的数量远大于labeled数量。Semi-supervised Learning可以分成两种，<strong>Transductive learning</strong>和<strong>Inductive learning</strong>，它们的区别如下：</p>
<ul>
<li><p>Semi-supervised learning:$\{(x^r,\hat{y}^r)\}_{r=1}^R$,$\{x^u\}_{u=R}^{R+U}$</p>
<ul>
<li>A set of unlabeled data,usually U &gt;&gt; R</li>
<li>Transductive learning:unlabeled data is the testing data</li>
<li>Inductive learning:unlabeled data is not the testing data</li>
</ul>
</li>
</ul>
<p>做Semi-supervised Learning的原因有如下两点：</p>
<ul>
<li><p>Collecting data is easy,but collecting “labeled” data is expensive。（收集data是很容易的，但是收集带label的data是比较困难的）</p>
</li>
<li><p>We do semi-supervised learning in our lives。其实对人类来说，也是一直在做Semi-supervised Learning。</p>
</li>
</ul>
<h1 id="Why-semi-supervised-learning-helps"><a href="#Why-semi-supervised-learning-helps" class="headerlink" title="Why semi-supervised learning helps"></a>Why semi-supervised learning helps</h1><p>假设现在要做一个classifier，用来区分猫和狗。同时有一大堆猫和狗的图片，但这些图片是没有label的，并不知道哪些是猫哪些是狗。假设只考虑猫和狗有label的data的话，我们画一个boundary将毛和狗的图片分开，假如unlabeled data的分布是像灰色点这样的话，这可能就会影像我们的决定。unlabel data虽然只告诉了我们function的input，但是unlabel data的分布可以告诉我们某些事。在下图中，加了unlabel data后，boundary发生了变化。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/1.png" alt="1"></p>
<p>但是semi-supervised learning使用unlabeled的方式，往往伴随着一些假设。semi-supervised learning有没有用其实就取决于这个假设符不符合实际。</p>
<h1 id="semi-supervised-learning-for-Generative-Model"><a href="#semi-supervised-learning-for-Generative-Model" class="headerlink" title="semi-supervised learning for Generative Model"></a>semi-supervised learning for Generative Model</h1><p>在<strong>Generative Model</strong>里面，如何做semi-supervised learning？我们已经知道了Supervised learning的Generative model。在Supervised learning里，我们有一堆training data，并且知道它们属于class 1还是class 2。我们回去估测class 1和class 2的prior probability，然后去估测$P(x|C_1)$和$P(x|C2)$。假设每一个class的分布都是一个Gaussian distribution的话，class 1是从mean为$\mu^1$，covariance是$\Sigma$的Gaussian估测出来的，class 2是从mean为$\mu^2$，covariance也是是$\Sigma$的Gaussian估测出来的（之前讲过share Gaussian的performance可能会是比较好的）。有了这些值后，就可以估计给定一个新的data x，它是属于C1的<br>posterior probability，</p>
<p>$$P(c_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}$$</p>
<p>但是如果又多了一些unlabeled data，它就会影响我们的决定。如下图，右侧绿色的点为unlabel data，那么左边的$\mu^1$、$\mu^2$和$\Sigma$的值显然就是不合理的。它们的范围应该是虚线框才对。prior也会受到影响，没有unlabel data是我们觉得两个class的data是一样多的，但加入unlabel data后会觉得class 2的data其实是比较多的，所以它的prior probability其实是比较大的。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/2.png" alt="2"></p>
<p>实际的操作方式如下：</p>
<ul>
<li>Initialization:$\theta = \{P(C_1),P(C_2),\mu^1,\mu^2,\Sigma\}$</li>
<li>Step 1:compute the posterior probability of unlabeled data $P_{\theta}(C_1|x^{\mu})$</li>
<li>Step 2:Update model.($N$:total number of examples;$N_1$:number of examples belonging to $C_1$)</li>
</ul>
<p>$$ P(C_1) = \frac{N_1 + \Sigma_{x^u}P(C_1|x^u)}{N} $$</p>
<p>$$\mu^1 = \frac{1}{N_1}\sum_{x^r\in C^1}x^r + \frac{1}{\sum_{x^u}P(C_1|x^u)}\sum_{x^u}P(C_1|x^u)x^u$$</p>
<p>$$\cdots\cdots$$</p>
<ul>
<li>back to Step1</li>
</ul>
<p>在理论上，上面的方法会保证收敛。但是它的初始值$\theta$会影响最后的收敛结果。事实上，Step1就是EM algorithm中的E Step,Step2就是M step。</p>
<p>下面上上述操作的理论根据。</p>
<p>原来假设我们只有labeled data的时候，我们要做的事情是去maximize一个likelihood，每一笔training data，它的likelihood，我们是可以算出来的：</p>
<p>$$logL(\theta) = \sum_{x^r}logP_{\theta}(x^r,\hat{y}^r)$$</p>
<p>$$P_{\theta}(x^r,\hat{y}^r) = P_{\theta}(x^r|\hat{y}^r)P(\hat{y}^r)$$</p>
<p>现在如果有unlabeled data，式子有什么不一样呢？</p>
<p>$$logL(\theta) = \sum_{x^r}logP_{\theta}(x^r,\hat{y}^r) + \sum_{x^u}logP_{\theta}(x^u) \tag{1}$$</p>
<p>主要是我们不知道unlabeled data来自哪个class，要怎么去估算它的几率呢？因为C1和C2都有可能，所以一笔unlabeled data出现的几率可以如下计算($x^u$ can come from either $C_1 and $C_2$) </p>
<p>$$P_{\theta}(x^u) = P_{\theta}(x^u|C_1)P(C_1) + P_{\theta}(x^u|C_2)(C_2)$$</p>
<p>接下来要做的事情就是去maximize式子(1)，然而这个式子不是convex的，所以要用EM algorithm解(要iterative地去slove它)。</p>
<h1 id="Low-density-Separation"><a href="#Low-density-Separation" class="headerlink" title="Low-density Separation"></a>Low-density Separation</h1><p>接下来要讲一个比较general的方式，它的假设是<strong>low-density separation</strong>，也就是说这个世界是非黑即白的。非黑即白的意思是，假设我们现在有一大堆的data（label data和unlabel data），在两个class之间，它们会有一个非常明显的鸿沟。（在两个class的交界处，density是low的，data量是很少的）。low-density最具代表性的方法是<strong>Self-training</strong>。它的思想是先从labeled data去train一个model $f^{\star}$(用什么方法去得到$f^{\star}$是很general的),然后根据$f^{\star}$去label unlabeled data，得到的data叫做<strong>Pseudo-label</strong>,接下来从unlabeled data set里面拿出一些data，把它加到labeled data set里面。至于哪些data要加进去是一个open question,需要自己去design一些heuristic的rule，自己想办法来解决。有了更过的label datah后，可以回头再去train你的model $f^{\star}$。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/3.png" alt="3"></p>
<blockquote>
<p>思考：上面这个process,如果用在Regression上面，回怎么样？</p>
</blockquote>
<p>做regression不能用上面这一招，因为regression是output一个real number，将$x^u$对应的real number加到training data里面再去train，并不会影响$f^{\star}$。</p>
<p>Self-training很像Generative Model里面用到的方法，它们唯一的差别是在做self-training的时候，用的是<strong>hard label</strong>，而在做Generative model的时候，用的是<strong>Soft label</strong>。</p>
<p>在做Self-training的时候，我们会强制assign一笔training data，它一定属于某一个class，但是在Generative model的时候，是根据它的posterior probability，它可能有部分属于class 1，有部分属于class 2,所以是Soft label。那到底哪一个比较好呢？</p>
<p>如果考虑的是Neural Network的话，从label data得到一组network的参数$\theta^{\star}$，现在有一笔unlabeled data $x^u$，然后根据现在的参数$\theta^{\star}$把它分成两类，如下，它有0.7的几率属于class 1,有0.3的几率属于class 1。而如果是hard label的话，它可以直接label成class 1。如下图：</p>
<p><img src="/2019/10/25/Semi-supervised Learning/4.png" alt="4"></p>
<p>显然对于Neural Network，soft label结果是没有用的。当我们用hard label时,其实就是Low-density separation的概念，也就是说，$x^u$属于class 1的几率只比较高，并不确定它一定属于class 1，但是这是一个非黑即白的世界，如果看起来像class 1,那就一定是class 1！</p>
<h2 id="Entropy-based-Regularization"><a href="#Entropy-based-Regularization" class="headerlink" title="Entropy-based Regularization"></a>Entropy-based Regularization</h2><p>上面的方法有一个进阶版，叫做<strong>Entropy-based Regularization</strong>。这个方法是说，如果用neural network的时候，output是一个distribution。不要限制output一定是class 1或class 2，但是我们的假设是，这个output的distribution一定要很集中(因为这是一个非黑即白的世界)。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/5.png" alt="5"></p>
<p>现在的问题是要怎么用数值的方法来evaluate这个distribution到底好还是不好。可以使用<strong>Entropy</strong>。算一个distribution的Entropy，它能表示一个distribution的集中与否。它的式子如下</p>
<p>$$E(y^u) = -\sum_{m = 1}^5 y_m^u In(y_m^u)$$</p>
<p>$E(y^u)$的值越大，表示distribution越不集中。这样我们可以重新设计loss function，原来的loss function是希望找到一组参数，让现在在labeled data上的model的output跟正确的model的output它的距离越近越好，可以用cross entropy来evaluate它们之间的距离。重新设计的loss function如下</p>
<p>$$L = \sum_{x^r}C(y^r,\hat{y}^r)  +  \lambda\sum_{x^u}E(y^u) \tag{2}$$</p>
<p>上式中$\sum_{x^r}C(y^r,\hat{y}^r)$是labeled data的部分，$\sum_{x^u}E(y^u)$是unlabeled data部分，$\lambda$可以决定是偏向unlabel data多一点还是少一点。式子(2)是可以算微分的。所以就用Gradient Descent来minimize这个式子，这件事情的角色就很像之前所讲的Regularization。所以它称为Entropy-based Regularization。</p>
<p>还有别的semi-supervised learning的方式，比较著名的就是Semi-supervised SVM。SVM要做的事情就是给两个class的data,找一个boundary。这个boundary，一方面它要有最大的margin，所谓最大的margin就是让这两个class分的越开越好。同时，它也要有最小的分类的错误。现在假设有一些unlabeled data，Semi-supervised SVM会怎么处理这个问题呢？它会穷举所有可能的label。下面有4笔unlabel data，每一笔都可以属于class 1或class 2，穷举所有的可能，对每一个可能的结果，都去做一个SVM，然后再去看哪一个可能性可以让margin最大同时又minimize error。但是穷举所有可能的时间复杂度很大，下面的论文提出了一个很approximate的方法，基本精神是，一开始得到一些label，每次改一笔unlabeled data的label，看能不能让objective function变大，变大的话就改一下。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/6.png" alt="6"></p>
<h1 id="semi-supervised-learning-Smoothness-Assumption"><a href="#semi-supervised-learning-Smoothness-Assumption" class="headerlink" title="semi-supervised learning Smoothness Assumption"></a>semi-supervised learning Smoothness Assumption</h1><p><strong>Smoothness Assumption</strong>的基本假设是如果两个x是像的，那它们的label y也比较像。这个假设是比较粗糙的，更精确的假设如下</p>
<ul>
<li><p>$x$ is not uniform</p>
</li>
<li><p>If $x^1$ and $x^2$ are close in a high density region,$\hat{y}^1$ and $\hat{y}^2$ are the same(connected by a high density path)</p>
</li>
</ul>
<p>$x$的分布是不平均的，它在某些地方很平均，某些地方有很分散。如果$x^1$和$x^2$在一个high density的region很close的话，$\hat{y}^1$和$\hat{y}^2$才会很像。</p>
<p>举一个例子，假设下面是data的分布，有3笔data $x^1$、$x^2$、$x^3$，如果我们只考虑上面那个粗糙的假设，那么感觉$x^2$和$x^3$的label应该比较像，但实际上它们的label并不像。在更精确的假设中， $x^1$和$x^2$才是比较像的，因为它们中间有high density的path。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/7.png" alt="7"></p>
<p>为什么会有Smoothness Assumption这样的假设，因为在真实的情况下，这个假设是很有可能成立的。考虑手写数字辨识的例子，如下图，单纯考虑pixel上的相似度的话，两个2比较不像，而右边的2和3还比较像。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/8.png" alt="8"></p>
<p>但是如果将data通通倒出来的话，会发现这两个2中间，有很多连续的形态，它们中间有很多不直接相连的相似，所以如果根据Smoothness Assumption的话，可以得出两侧的2是比较像的。而2和3之间没有过渡的形态，所以它们其实是比较不像的，不应该属于一个class。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/9.png" alt="9"></p>
<p>如果看人脸识别的话，其实也是一样的。一个人的左脸图和右脸图是差很多的。但是假设收集到够多的unlabeled data的话，会找到左脸和有脸之间有很多过渡的形态，如下图。所以这两张脸可能是同一张人脸。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/10.png" alt="10"></p>
<p>在文件分类上Smoothness Assumption也非常有用。假设现在要分天文跟旅游的文章，天文学文章有一个固定的word distribution，旅游文章也有一个固定的word distribution。如果unlabeled data和label data是有overlapped的，那就很容易处理这个问题。但是在真实的情况下，unlabeled data和labeled data，它们中间可能没有任何overlapped word，因为世界上的word很多，而一篇文章的词汇往往不会太多。但是如果collect到够多的unlabeled data的话，文章之间的相似性就可以一路propagate过去。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/11.png" alt="11"></p>
<h2 id="Cluster-and-then-Label"><a href="#Cluster-and-then-Label" class="headerlink" title="Cluster and then Label"></a>Cluster and then Label</h2><p>要如何实践Smoothness Assumption呢，最简单的方法是<strong>Cluster and then Label</strong>。例如现在data distribution如下，蓝色部分是unlabel data。接下来，把所有的data拿来做clustering，可能分成3个cluster。然后看每个cluster里面最多的label data是哪一类，就把它分成那一类。如果要让这个方法有用，必须cluster要很强。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/12.png" alt="12"></p>
<h2 id="Graph-based-Approach"><a href="#Graph-based-Approach" class="headerlink" title="Graph-based Approach"></a>Graph-based Approach</h2><p>另外一个方法是引入<strong>Graph structure</strong>，用Graph structure来表达<em>connected by a high density path</em>这件事情。也就是把所有的data points，都建成一个graph，每一笔data point x，就是这个图上的一个点，我们需要想办法算它们之间的similarity，要想办法把它们之间的edge建出来。有了这个Graph以后，就可以说所谓的high density path的意思是，如果有两个点它们在graph上面是相连的，是走得到的，它们就是同一个class。如果没有相连，就算实际的距离不算太远，也不算同一class。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/13.png" alt="13"></p>
<p>那要怎么建一个graph呢？有些时候，这个graph的representation是很自然就可以得到的。举例来说，假设现在要做的是网页分类，而我们有记录网页之间的超链接，那么超链接自然就告诉我们网页是如何连接的。或者做的是论文的分类，而论文之间有引用的关系，引用关系也是另外一种graph的edge。</p>
<p>而有时候，需要自己想办法去建这个graph，graph的好坏对最后的结果影响是非常大的。不过这个地方是非常的heuristic，就是凭着经验和直觉，觉得怎么做比较好。通常的做法如下</p>
<ul>
<li><p>Define the similarity $s(x^i,x^j)$ between $x^i$ and $x^j$</p>
</li>
<li><p>Add edge:</p>
</li>
<li><ul>
<li>K Nearest Neighbor</li>
</ul>
</li>
<li><ul>
<li>e-Neighborhood</li>
</ul>
</li>
<li><p>Edge weight is proportional to $s(x^i,x^j)$</p>
</li>
</ul>
<p>定义相似度，对于影像的话可以base on pixel来算相似度，但是performance不太好。base on auto encoder抽出来的feature算相似度performance会比较好。</p>
<p><strong>K Nearest Neighbor</strong>:有一堆data，data和data之间都可以算出一个相似度，如果设K=3，每一个point都跟它最近的、相似度最像的三个点做相连。</p>
<p><strong>e-Neighborhood</strong>:每一个点，只有和他相似度超过每一个threshold的点才会被连起来。</p>
<p>所谓的edge也不是只有相连和不相连这样binary的选择而已，可以给edge一些weight，让edge跟要被连起来的两个data point之间相似度成正比。</p>
<p>定义相似度比较好的选择使用Gaussian Radial Basis Function:</p>
<p>$$s(x^i,x^j) = exp(-\gamma||x^i - x^j||^2)$$</p>
<p>上面的公式取exponential，是为了让$x^i$和$x^j$非常靠近的时候，similarity才会大。</p>
<p>如果在graph上面有一些labeled data,比如class 1，那么跟它们有相连的那些data point，它是属于class 1的几率也就会上升。所以每一笔data它会去影响它的邻居。光会影响邻居是不够的，因为相连代表他们本来就很像，这样帮助不会太大。Graph-based approach真正会有帮助地方是它的class会传递的，</p>
<p><img src="/2019/10/25/Semi-supervised Learning/14.png" alt="14"></p>
<p>上面说得是如何定性的使用Graph,接下来要说如何定量地使用Graph。定量的使用方式是在graph的structure上面定一个label的smoothness。定义这个label有多符合Smoothness Assumption假设。在下面的例子里，都有4个data point，两个data point之间连接的数字代表edge的weight。左右两边的graph是一样的，但是data的label有所不同，那么左右两边的图谁比较smooth呢？</p>
<p><img src="/2019/10/25/Semi-supervised Learning/15.png" alt="15"></p>
<p>直观的感觉就是左边比较smooth，因为它三角形部分的label都是一样的。但我们需要用一个数字来定量地描述它有多smooth，常见的做法如下：</p>
<ul>
<li>Define the smoothness of the labels on the graph</li>
</ul>
<p>$$S = \frac{1}{2}\sum_{i,j}w_{i,j}(y^i-y^j)^2 \tag{3}$$</p>
<p>上面式子的含义是，考虑两两有相连point，对所有相连的pair计算i、j之间的weight跟i的label与j的label的差的平方。（i,j是对所有的data，不管是有label还是没有label）</p>
<p>根据定义，左边算出来的smoothness是0.5,右边算出来的smoothness是3。（值越小越smooth）</p>
<p>式子(3)可以稍微整理一下，写成一个比较简洁的式子。把$\bf{y}$串成一个vector,$\bf{y}$包括labeled data和unlabeled data。</p>
<p>$\bf{y}$:(R+U)-dim vecotr</p>
<p>$$ {\bf y} = [\cdots y^i \cdots y^j \cdots] $$</p>
<p>所以式子(3)可写成<br>$$S = \frac{1}{2}\sum_{i,j}w_{i,j}(y^i-y^j)^2 = {\bf y}^TL{\bf y} \tag{4}$$</p>
<p>其中L是一个$(R+U)\times (R+U)$的matrix，它的名字是<strong>Graph Laplacian</strong>,L的定义如下：</p>
<p>$$L = D - W $$</p>
<p>$W$表示Point两两之间的weight connection的matrix,对于上图右侧的graph，有</p>
<p>$$ W = \begin{bmatrix}<br>0&amp;2&amp;3&amp;0\\<br>2&amp;0&amp;1&amp;0\\<br>3&amp;1&amp;0&amp;1\\<br>0&amp;0&amp;1&amp;0\\<br>\end{bmatrix}$$</p>
<p>D是把W的每一个row合起来，放在diagonal的地方。</p>
<p>$$ D = \begin{bmatrix}<br>5&amp;0&amp;0&amp;0\\<br>0&amp;3&amp;0&amp;0\\<br>0&amp;0&amp;5&amp;0\\<br>0&amp;0&amp;0&amp;1\\<br>\end{bmatrix}$$</p>
<p>现在我们就可以用式子(4)来evaluate得到的label有多smooth。在这个式子里面，$\bf{y}$的值是取决于network的parameter，这一项其实是network的dependent。所以，要把graph的information考虑到neural network的training的时候，要做的事情，其实就是在原来的loss function里面加一项。原来的loss function可能是考虑cross entropy之类的，我们就再加另外一项，这一项是smoothness的值。</p>
<p>$$ L = \sum_{x^r}C(y^r,\hat{y}^r) + \lambda S$$</p>
<p>上式中的$\lambda S$就像是一个Regularization的term。现在我们不只要调参数让neural network在那些labeled data的output跟真正的label越接近越好，同时还要让output的label,不管是在Labeled data还是unlabeled data上面，它都符合smoothness Assumption的假设。同样的，接下来就是做Gradient Descent。</p>
<p>其实，算smoothness的时候不一定要算在output的地方，如果今天是一个deep neural network的话，可以把smoothness放在network的任何地方。可以假设output是smooth，也可以同时把某一个hidden layer接出来，在乘上一些别的transform,它也要smooth。也可以要求每一个hidden layer的output都要是smooth的，可以同时把这些smooth通通都加到neural network上面去。</p>
<p><img src="/2019/10/25/Semi-supervised Learning/16.png" alt="16"></p>
<h1 id="semi-supervised-Learning-——-Better-Representation"><a href="#semi-supervised-Learning-——-Better-Representation" class="headerlink" title="semi-supervised Learning —— Better Representation"></a>semi-supervised Learning —— Better Representation</h1><p>这个方法会到unsupervised learning再讲，其主要的思想就是去芜存菁，化繁为简。</p>
<ul>
<li><p>Find the latent factors behind the observation</p>
</li>
<li><p>The latent factors (usually simpler) are better representations</p>
</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/24/Lecture05/" rel="next" title="发布者Publisher的编程实现">
                <i class="fa fa-chevron-left"></i> 发布者Publisher的编程实现
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/10/27/Unsupervised Learning Linear Methods/" rel="prev" title="Unsupervised Learning —— Linear Methods">
                Unsupervised Learning —— Linear Methods <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Liang Qi">
            
              <p class="site-author-name" itemprop="name">Liang Qi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Why-semi-supervised-learning-helps"><span class="nav-number">2.</span> <span class="nav-text">Why semi-supervised learning helps</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#semi-supervised-learning-for-Generative-Model"><span class="nav-number">3.</span> <span class="nav-text">semi-supervised learning for Generative Model</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Low-density-Separation"><span class="nav-number">4.</span> <span class="nav-text">Low-density Separation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Entropy-based-Regularization"><span class="nav-number">4.1.</span> <span class="nav-text">Entropy-based Regularization</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#semi-supervised-learning-Smoothness-Assumption"><span class="nav-number">5.</span> <span class="nav-text">semi-supervised learning Smoothness Assumption</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Cluster-and-then-Label"><span class="nav-number">5.1.</span> <span class="nav-text">Cluster and then Label</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-based-Approach"><span class="nav-number">5.2.</span> <span class="nav-text">Graph-based Approach</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#semi-supervised-Learning-——-Better-Representation"><span class="nav-number">6.</span> <span class="nav-text">semi-supervised Learning —— Better Representation</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liang Qi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>