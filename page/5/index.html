<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Qi-Liang&#39;blog">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name" content="Qi-Liang&#39;blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Qi-Liang&#39;blog">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/5/">





  <title>Qi-Liang'blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qi-Liang'blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">stay young,stay simple</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/13/sys bios05/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/13/sys bios05/" itemprop="url">sys bios05</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-13T21:11:08+08:00">
                2019-09-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/08/Embedded-System-sys-bios13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/08/Embedded-System-sys-bios13/" itemprop="url">同步 信号量</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-08T15:32:09+08:00">
                2019-09-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Embedded-System/" itemprop="url" rel="index">
                    <span itemprop="name">Embedded System</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>SYS/BIOS中用于线程同步的组件主要有：</p>
<ul>
<li>信号量（Semaphore）</li>
<li>事件（Event Module）</li>
<li>门（Gate)</li>
<li>邮箱（Mailbox）</li>
<li>队列（Queue）</li>
</ul>
<h1 id="信号量"><a href="#信号量" class="headerlink" title="信号量"></a>信号量</h1><p>信号量通常用于协调一些处于竞争关系的任务之间对共享资源的访问。前面讲到，任务（Task）线程是唯一可以被挂起，处于挂起的任务可以用于等待某一事件的发生，或者等待某些资源可以被利用，这样任务才能继续执行。在SYS/BIOS里面，每一个信号量都有一个与之相关联的计数值，对于信号量有两种操作：post()和pend(),这两种操作的实质就是改变计数值。<br>而在SYS/BIOS系统中，信号量有两种计数模式，一种是以二进制方式计数，一种是以计数方式计数。它们的区别如下，其中sem为信号量。<br><img src="/2019/09/08/Embedded-System-sys-bios13/1.png" alt="1"><br>当我们在任务线程中执行Semaphore_pend()，如果检查到关联的计数值为0的话，那么就可以执行一个等待。等待可以设置一个等待超时时间，或者可以一直等待下去。处于等待状态的任务就会被挂起。只有当信号量再次被发布，计数值不为0，任务线程会处于等待就绪，或者是运行状态。二进制与计数两种计数量的区别是对于资源可利用的种类，或者资源可以被多少任务线程利用。如果只是两个线程，二进制信号量就够了。如果是多个线程，则使用计数型的。</p>
<blockquote>
<p>Semaphore objects can be declared as either counting or binary semaphores and as either simple (FIFO) or priority-aware semaphores. Semaphores can be used for task synchronization and mutual exclusion.The same APIs are used for both counting and binary semaphores.<strong>By default,semaphores are simple counting semaphores.</strong></p>
</blockquote>
<p>例如一个LED灯，由于它只有两种状态（被使用，不能被使用），所以对它的操作使用二进制型的信号量就够了。</p>
<p><img src="/2019/09/08/Embedded-System-sys-bios13/2.png" alt="2"><br>以信号量举个例子，如上图，在当前运行的应用程序里，有两个优先级的任务，一个是低优先级的任务，它当前处于运行状态，与之关联的信号量的计数值为0，还有一个更高优先级的任务处于挂起的状态（之前执行Semaphore_pend()函数将其挂起），与信号量相关联的计数值是1。现在在低优先级的任务中执行Semaphore_post(),在发布完成之后，因为当前计数变量的值为0，所以低优先级的任务会被挂起，然后马上执行高优先级的任务，当前高优先级的任务会处于运行状态。</p>
<p>需要注意的是，<strong>在SYS/BIOS系统中，不管是什么线程，调度的一个核心就是根据优先级来调度的，所以在任何一个时间，当前应用程序（或者说在SYS/BIOS系统下），当前运行的线程一定是整个系统当中准备就绪的可以执行的最高优先级的线程</strong>。即便是系统当中有多个同等优先级的任务，在执行的时候也是按照一个先入先出的顺序，即谁先准备好，谁先执行。在系统当中，在某一时刻，只能有一个线程处于运行状态。这和其他系统可能不太一样，它没有一个轮询的概念，比如在Linux下编写一个线程的话，如果优先级相同，系统会轮询来执行，如够我们在每个线程中打印一个数字，这个程序每次打印的结果可能是不一样的。但是在SYS/BIOS系统下，这个实时系统最主要的特点就是要保持实时性和稳定可靠性，所以这个程序的执行结果是可以被预料的（确定的）。</p>
<p>另外，对于信号量来说，可以使用同一个信号量对多个任务执行挂起的操作（线程数量可以无限多，只要内存空间允许）。作为一款实时操作系统，SYS/BIOS主要的目的就是保证不管系统的负载有多少，调度程序运行的响应时间都要维持在一定的可靠范围之内。我们需要当前任务在等待某一事件或某一资源准备就绪的时候，就会使用Semaphore_pend()函数来让任务处于挂起状态。在执行Semaphore_pend(sem)函数的时候，会将计数值减1（如果计数值不为0），但是任务不会马上处于挂起状态，它还会将任务下面的代码执行完毕。如果在执行Semaphore_pend()函数前计数值已经为0，那么当前任务会被立刻挂起，而由SYS/BIOS的任务调度程序转而执行其他等待就绪的高优先级任务。</p>
<blockquote>
<p>Tasks wait for simple counting and binary semaphores in FIFO order without regard to the priority of the tasks. Optionally, you can create “priority” semaphores that insert pending tasks into the waiting list before the first task that has a lower priority. As a result, tasks of equal priority pend in FIFO order, but tasks of higher priority are readied before tasks of lower priority.</p>
</blockquote>
<blockquote>
<p>Note that using priority semaphores can increase the interrupt latency in the system, since interrupts are disabled while the list of tasks waiting on the semaphore is scanned for the proper insertion point. This is typically about a dozen instructions per waiting task. For example, if you have 10 tasks of higher priority waiting, then all 10 will be checked with interrupts disabled before the new task is entered onto the list</p>
</blockquote>
<h1 id="信号量配置"><a href="#信号量配置" class="headerlink" title="信号量配置"></a>信号量配置</h1><p>信号量的配置既可以使用静态配置也可以使用动态配置</p>
<p>信号量有4种类型</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">semParams.mode = Semaphore_Mode_COUNTING;</span><br><span class="line"><span class="comment">//semParams.mode = Semaphore_Mode_BINARY;</span></span><br><span class="line"><span class="comment">//semParams.mode = Semaphore_Mode_COUNTING_PRIORITY;</span></span><br><span class="line"><span class="comment">//semParams.mode = Semaphore_Mode_BINARY_PRIORITY;</span></span><br></pre></td></tr></table></figure>

<h2 id="静态配置"><a href="#静态配置" class="headerlink" title="静态配置"></a>静态配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var Semaphore = xdc.useModule(ti.sysbios.knl.Semaphore);</span><br><span class="line">Program.global.sem = Semaphore.create(1);</span><br></pre></td></tr></table></figure>

<p>静态配置在程序的开发阶段在CCS工程的.cfg文件里进行配置（.cfg文件可以不只一个），也可以根据需要编写多个.cfg文件，如果系统比较复杂，可以给不同的模块都放在单独的.cfg文件当中。另外要注意，在静态配置中所创建的一些全局变量，如果我们要做运行时进行操作，需要在C代码中引用头文件：<strong>#include&lt;xdc/cfg/global.h&gt;</strong></p>
<p>SYS/BIOS系统是可裁剪的，只有添加到系统应用程序当中的组件最后才会被编译到.out文件当中。</p>
<h2 id="动态配置"><a href="#动态配置" class="headerlink" title="动态配置"></a>动态配置</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ti/sysbios/knl/Semaphore.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">Semaphore_Params semParams;</span><br><span class="line">Semaphore_Params_init(&amp;semParams);</span><br><span class="line">semParams.mode = Semaphore_Mode_BINARY; <span class="comment">//指定信号量模式为二进制模式</span></span><br><span class="line">sem=Semaphore_create(<span class="number">1</span>,&amp;semParams,<span class="literal">NULL</span>)</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/07/Embedded-System-sys-bios04/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/07/Embedded-System-sys-bios04/" itemprop="url">SYS/BIOS及其相关组件</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-07T19:00:23+08:00">
                2019-09-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Embedded-System/" itemprop="url" rel="index">
                    <span itemprop="name">Embedded System</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="SYS-BIOS内核"><a href="#SYS-BIOS内核" class="headerlink" title="SYS/BIOS内核"></a>SYS/BIOS内核</h1><p>SYS/BIOS并不是我们所认识的传统意义的操作系统，它只是完成一些最基础的一个系统所应包含的功能。所以一般情况下，称SYS/BIOS为一个内核更为合适。TI的一些文档里面也称SYS/BIOS为TI-RTOS Kernel.</p>
<h2 id="产品演进"><a href="#产品演进" class="headerlink" title="产品演进"></a>产品演进</h2><p>TI的产品演进如下图<br><img src="/2019/09/07/Embedded-System-sys-bios04/1.png" alt="1"><br>DSP/BIOS-&gt;SYS/BIOS-&gt;TI-RTOS</p>
<p>TI-RTOS 2.x的组件支持<br><img src="/2019/09/07/Embedded-System-sys-bios04/2.png" alt="2"></p>
<p>SYS/BIOS组件<br>RTSC中也将组件成为包<br><img src="/2019/09/07/Embedded-System-sys-bios04/3.png" alt="3"></p>
<p>SYS/BIOS的设备支持</p>
<h2 id="DSP-BIOS-SYS-BIOS-TI-RTOS区别"><a href="#DSP-BIOS-SYS-BIOS-TI-RTOS区别" class="headerlink" title="DSP/BIOS SYS/BIOS TI-RTOS区别"></a>DSP/BIOS SYS/BIOS TI-RTOS区别</h2><h2 id="SYS-BIOS提供的组件"><a href="#SYS-BIOS提供的组件" class="headerlink" title="SYS/BIOS提供的组件"></a>SYS/BIOS提供的组件</h2><h2 id="SYS-BIOS设备支持及性能。"><a href="#SYS-BIOS设备支持及性能。" class="headerlink" title="SYS/BIOS设备支持及性能。"></a>SYS/BIOS设备支持及性能。</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/07/Embedded-System-sys-bios03/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/07/Embedded-System-sys-bios03/" itemprop="url">实时操作系统</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-07T18:57:55+08:00">
                2019-09-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Embedded-System/" itemprop="url" rel="index">
                    <span itemprop="name">Embedded System</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="什么是操作系统"><a href="#什么是操作系统" class="headerlink" title="什么是操作系统"></a>什么是操作系统</h1><p>操作系统是管理计算机硬件与软件资源的计算机程序。操作系统需要处理如管理与配置内存、决定系统资源供需的优先次序、控制输入与输出设备、操作管理文件系统等。简单来说，我们依靠操作系统来给我们提供一些中间件以及系统的底层服务。</p>
<p><img src="/2019/09/07/Embedded-System-sys-bios03/1.png" alt="1"><br>如上图是windows NT的抽象架构，windows NT是微软的一个操作系统，也是一个核心架构。1993年开始面市。其中最底层是硬件层，其上有<strong>硬件抽象层</strong>，它将我们驱动的一些上层的调用和底层的硬件实际的操作区分开来，这样的话我们在更换不同的硬件设备的时候只需要对驱动进行修改就可以了，而上面所有应用层的调用接口就可以保持一致。然后在内核里面还提供了一些内核服务，包括输入输出管理、文件系统、网络系统等等。在内核的上面是用户模式，在windows下主要是win32子系统（在早期版本也有OS/2和POSIX的子系统，因为当初windows NT架构就是微软和IBM公司联合开发，后来分道扬镳，微软继续开发windows nt系统，IBM继续开发OS/2系统，实际上他们并没有什么太大的区别。）然后再往上就是win32子系统为我们提供的一些接口来运行我们的应用程序。</p>
<blockquote>
<p><strong>WRK(Windows Research Kernel)</strong>,它是微软为高校操作系统课程提供的可修改和跟踪的操作系统教学平台，它给出了Windows这个成功的商业软件操作系统内核大部分代码，可以对其进行修改、编译，并且可以用这个内核启动Windows操作系统</p>
</blockquote>
<h2 id="内核文件"><a href="#内核文件" class="headerlink" title="内核文件"></a>内核文件</h2><p>与内核相关的文件，在windows系统下，Ntoskrnl.exe是核心文件，前面提到的WRK工程，它除了提供源代码，还提供了编译器，它编译出来的文件就是Ntoskrnl.exe。<br><img src="/2019/09/07/Embedded-System-sys-bios03/2.png" alt="2"></p>
<h2 id="Windows-8的软件开发架构"><a href="#Windows-8的软件开发架构" class="headerlink" title="Windows 8的软件开发架构"></a>Windows 8的软件开发架构</h2><p>下面是Windows 8的软件开发架构图，前面介绍的windows系统比较底层的一些东西，而下面是我们在应用层要做的事。<br><img src="/2019/09/07/Embedded-System-sys-bios03/3.png" alt="3"></p>
<h1 id="什么是实时操作系统"><a href="#什么是实时操作系统" class="headerlink" title="什么是实时操作系统"></a>什么是实时操作系统</h1><p>一般来说操作系统为我们提供了这些内容：</p>
<ul>
<li><strong>底层控制（启动引导/输入输出控制）</strong></li>
<li><strong>多任务管理</strong></li>
<li><strong>存储器管理</strong>：内存和外存，外存包括硬盘、光盘等</li>
<li><strong>硬件抽象层</strong>：提高了操作系统对不同硬件平台的适用</li>
<li><strong>文件系统</strong>：windwos下一般是NTFS文件系统，嵌入式下种类比较多</li>
<li><strong>协议栈（网络/USB等等）</strong>：网络协议栈包括TCP、IP。</li>
</ul>
<p>对于实时操作系统，它提供的内容可能会更多一些。</p>
<ul>
<li><strong>系统模块化并可裁剪</strong>：尽可能减少到系统资源的消耗</li>
<li><strong>快速响应（中断/任务）</strong></li>
<li><strong>响应时间的确定性（时间抖动很低）</strong>:不管任务的负责程度如何，响应时间都应该是确定的。</li>
<li><strong>低资源消耗</strong></li>
</ul>
<p>总结来说，实时操作系统最大的特点就是<strong>实时</strong>，它最重要的要求是对要执行的任务或者说中断要有非常快的响应，即实时性。而衡量实时操作系统的一个重要指标就是它的时间抖动很低（不管任务多寡，执行切换操作所需的时间基本是一致的）。</p>
<p>实时操作系统可大概分为<strong>硬实时操作系统</strong>和<strong>软实时操作系统</strong>，我们所见到的大部分实时系统其实都是介于两者之间的。硬实时操作系统对响应时间较为苛刻，而软实时操作系统则较为宽松，它要求尽量在较短时间内完成响应。实时操作系统的分类主要有两种：<strong>事件驱动</strong>和<strong>时间触发</strong>。SYS/BIOS就是抢占型实时操作系统。<br><img src="/2019/09/07/Embedded-System-sys-bios03/4.png" alt="4"></p>
<h1 id="为什么要使用实时操作系统"><a href="#为什么要使用实时操作系统" class="headerlink" title="为什么要使用实时操作系统"></a>为什么要使用实时操作系统</h1><p>随着应用编写的复杂性越来越高，大多数工程师越来越倾向使用实时操作系统。最常见的原因是使用多线程，它既能兼顾代码的复杂性，也能使代码的开发更为简单。</p>
<p>从应用角度来看，有以下几个使用实时操作系统的理由：</p>
<ul>
<li><strong>复杂性</strong>：传统编写应用程序，是在main函数中使用一个无限循环，来将我们的代码逐条执行，这种前台后台的应用可能有些时候不能满足我们的要求（在无限循环中的所作工作是前台，程序对中断的响应是后台，中断的优先级比无限循环中的要高）。</li>
<li><strong>多任务</strong>：对多任务的管理更加方便。</li>
<li><strong>模块化</strong>：一个应用不只一个工程师来完成，所以对模块化的要求越来越高。</li>
<li><strong>减少底层或者重复性工作</strong>：在我们常用的电子计算机上，一些接口的标准化程度已经相当高了，比如插硬盘就是SATA接口，插扩展盘卡就是PCIe接口，而我们要扩展其他设备如键盘、鼠标、传真机等就可以通过USB接口来实现，所以这个接口对于外设比较统一。而对于嵌入式平台，就大大不同了，有一些所用到的总线都是工业上的标准，我们平时用到的比较少。但是如果我们每一次在使用这些的时候都要自己来编写底层的控制以及实现逻辑就太复杂了，可以使用一种操作系统，操作系统本身就给我们提供了这样的接口，就可以省去自己来写底层代码的工作量。</li>
<li><strong>资源更优化分配管理</strong>：主要是减少内存碎片，提高内存的使用率。尤其对dso来说，它的片上内存虽然小，但是效率非常高，在做算法的时候，我们倾向把数据放在片上内存来操作。</li>
<li><strong>软件协议栈（网络/USB等等）</strong></li>
</ul>
<p>另一个使用实时操作系统的动力是可移植性，如果我们编写的应用不仅仅要在dsp上使用，还要在arm上使用，在单片机上使用，那么我们就需要程序的应用层和底层尽可能分离开，这样程序的可移植性就会更高。</p>
<p>还有就是对于调试来说，实时操作系统可以帮我们输出一些实时分析的调试信息。</p>
<h1 id="实时操作系统的性能指标"><a href="#实时操作系统的性能指标" class="headerlink" title="实时操作系统的性能指标"></a>实时操作系统的性能指标</h1><p>实时操作系统主要的两个性能指标</p>
<h2 id="中断响应时间"><a href="#中断响应时间" class="headerlink" title="中断响应时间"></a>中断响应时间</h2><p>中断主要作用是大大提高了CPU应对突发情况的响应能力，所以对实时操作系统中断响应时间是一个非常重要的指标。中断响应时间粗略估算方法如下<br>中断响应时间 = 关中断的最长时间<br>            +保护CPU内部寄存器的时间<br>            +进入中断服务函数的执行时间<br>            +开始执行中断服务函数（ISR）的第一条指令的时间</p>
<h2 id="任务切换时间"><a href="#任务切换时间" class="headerlink" title="任务切换时间"></a>任务切换时间</h2><p>当多任务内核决定运行另外的任务时，它把正在运行的任务的当前状态（即CPU寄存器中的全部内容）保存到任务自己的栈区之中，然后把下一个将要运行的任务的当前状态从该任务的栈中重新装入CPU的寄存器，并开始下一个任务的运行。这个过程就称为任务切换，做任务切换所需要的时间取决于CPU有多少寄存器要入栈。CPU的寄存器越多，额外负载就越重。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/07/Embedded-System-sys-bios02/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/07/Embedded-System-sys-bios02/" itemprop="url">XDCTools</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-07T18:17:58+08:00">
                2019-09-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Embedded-System/" itemprop="url" rel="index">
                    <span itemprop="name">Embedded System</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="RTSC的几个关键词"><a href="#RTSC的几个关键词" class="headerlink" title="RTSC的几个关键词"></a>RTSC的几个关键词</h1><h2 id="Package-包"><a href="#Package-包" class="headerlink" title="Package 包"></a>Package 包</h2><p>包，也就是前面RTSC开发流程5个步骤中第3步进行packaging操作所形成的包。包既具有逻辑上的概率也具有物理上的概念。逻辑上讲它是<strong>Modules及Interfaces</strong>的容器。以数据信号处理函数库这个组件为例，在这个库当中不只一个函数，比如快速傅里叶变换、卷积相关函数等，所以这一切的集合就是包。物理上的概念是，每一个包都要以文件的形式存在我们的文件系统上面，在包当中包含所有我们的头文件、源文件、库文件、脚本以及相关文档等等。从逻辑上讲，package相当于一些语言中namespace(命名空间)的概念。</p>
<h2 id="Module-模块"><a href="#Module-模块" class="headerlink" title="Module 模块"></a>Module 模块</h2><p>相关常量、类型和函数的集合。在一个包中可能不只一个模块。相当于java或.net语言下的类的概念。</p>
<h2 id="Interface-接口"><a href="#Interface-接口" class="headerlink" title="Interface 接口"></a>Interface 接口</h2><p>Module模块的最终实现形式。</p>
<hr>
<p>我们用一个列子来说明。在TI出一套算法框架（标准）之前，算法的编写比较不统一的，这给我们算法的整合带来许多困难。不统一的意思是：在一个项目中，不同工程师所编写的算法它的调用接口是不一致的。有了TI的算法框架以后，对于算法的调用和整合可以使用统一的一种方式来实现，这样对于我们编写复杂的程序比较方便。这套算法框架叫做<strong>XDAIS算法标准（eXpressDsp Algorithm Interoperability Standard）</strong>，在这个标准之后还扩张了XDM的标准，这是为了简化XDAIS算法的开发，因为XDAIS算法涵盖了算法开发的整套流程，相对比较复杂，XDM相当于XDAIS的上层接口。</p>
<h1 id="XDCTools"><a href="#XDCTools" class="headerlink" title="XDCTools"></a>XDCTools</h1><p>前面所讲的RTSC,它只是一套标准（或者说框架），它是理论层面的东西。理论上的东西拿到实际上去用，需要有相应工具来实现，这就是XDCTools所要做的事情。这个工具要实现前面所说的RTSC5个步骤中的所有环节。在RTSC开发的一整个流程都可以使用这一个工具来实现。</p>
<p>XDC从缩写上可以是两个意思，一个是<strong>eXpress Dsp Components</strong>,另一个是<strong>eXpanDed C</strong>(扩展C语言)。这个工具是开源免费的。XDCTools包含以下几个组件：</p>
<ul>
<li>xdc</li>
<li>xdc.tools.cdoc</li>
<li>xdc.tools.repoman</li>
<li>xdc.tools.path</li>
<li>xdc.tools.configuro</li>
<li>xs<br><img src="/2019/09/07/Embedded-System-sys-bios02/1.png" alt="1"><br>XDCTools这个工具涵盖了组件也就是RTSC中所说的包package整个开发流程（生命周期），以及程序开发的整个流程（生命周期），还有程序在运行过程中分析的整个流程，它是一套比较完善的工具，例如对于package来说，它对包的创建、编译、发布等等相关工具都有提供。而对于运行时来说，一些组件比如Log也是由XDCTools来提供的。</li>
</ul>
<h2 id="语言支持"><a href="#语言支持" class="headerlink" title="语言支持"></a>语言支持</h2><p>在现在计算机上使用的语言主要是面向对象的一种编程，而我们在嵌入式平台要想实现组件技术，单单使用C语言在某些特性上无法满足，所以在RTSC上引入了两种描述性的语言，他们是为了补充C语言的功能而引入的。</p>
<p>在运行的时候，因为是嵌入式系统，它的编程需要C、C++或汇编来编写，所以在运行时使用的语言主要是C语言。而在组件的开发或者程序的开发的时候，使用的语言是<strong>XDCspec</strong>和<strong>XDCscript</strong>。XDCspec主要是对组件的定义和描述时使用的，而XDCscript是对于组件的配置使用的。这三种语言一起被成为eXpanDed C。</p>
<p><img src="/2019/09/07/Embedded-System-sys-bios02/2.png" alt="2"></p>
<p>另外，XDCTools只是实现RTSC组件技术的一个工具，并不包含有编译工具链，所以使用它时候我们必须要搭配相应编译工具链。 </p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>RTSC是一种面向嵌入式C语言的组件技术，使用组件技术有很多便利之处，当然使用与否的选择权在你。不过如果要使用SYS/BIOS系统的话，是必须要使用这种组件技术的，而XDCTools就是实现这种技术的一种工具。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://en.wikipedia.org/wiki/XDAIS_algorithms" target="_blank" rel="noopener">XDAIS algorithm</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/07/Embedded-System-sys-bios01/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/07/Embedded-System-sys-bios01/" itemprop="url">实时软件组件-RTSC</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-07T16:50:51+08:00">
                2019-09-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Embedded-System/" itemprop="url" rel="index">
                    <span itemprop="name">Embedded System</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="RTSC"><a href="#RTSC" class="headerlink" title="RTSC"></a>RTSC</h1><p>实时软件组件<strong>Real-Time Software Components</strong>(RTSC),为嵌入式C语言引入组件技术。<br>什么是组件技术呢？在我们的程序开发过程中，我们会扮演两种角色：</p>
<ul>
<li><strong>生产者</strong>：编写可供调用的函数库，并尽量使其具有可移植性</li>
<li><strong>消费者</strong>：调用各种各样已经编写好的函数库<br>在传统的开发过程中，我们往往会同时扮演这两种角色，主要原因是以往嵌入式CPU的性能不够强，所以我们在编程的时候，最优先考虑的因素是性能，使我们的代码优化，在资源比较紧张的嵌入式CPU上来运行。但是随着嵌入式的发展，嵌入式CPU的性能越来越强，所以我们在嵌入式CPU上编写的程序也越来越复杂，当然我们的需求也越来越复杂。这样我们对嵌入式程序的开发往往是由一个团队来完成，团队中的每一个成员负责不同的部分。如下<br><img src="/2019/09/07/Embedded-System-sys-bios01/1.png" alt="1"></li>
</ul>
<p>由于应用程序是不同的人来开发的，这就要求有一种标准来将不同人开发的组件集成在为我们所需要的应用程序。那么这样就对我们在嵌入式程序开发过程中对于代码的<strong>可重用性</strong>和<strong>可定制性</strong>的要求越来越高。</p>
<h2 id="可重用性"><a href="#可重用性" class="headerlink" title="可重用性"></a>可重用性</h2><p>代码可以被重复调用。例如windows中的dll文件（动态链接库）。</p>
<h2 id="可定制性"><a href="#可定制性" class="headerlink" title="可定制性"></a>可定制性</h2><p>在我们的PC上硬件标准比较统一，但是在嵌入式环境下却不是这样。常见的有基于ARM架构的CPU、DSP，以及最常见的单片机（MCU）。它们的CPU架构不一样，指令集也就不一样，所以编写的一套应用程序或一个库是不可能实现二进制兼容的。那么我们要针对不同的CPU（平台）进行一些可定制话的操作。<br>以TI提供的SYS/BIOS实时操作来说，目前SYS/BIOS已经可以在TI的全系列CPU上运行。包括DSP系列、C2000系列、C5000系列（只能运行SYS/BIOS之前的版本）、C6000系列。当然对于性能比较强的处理器如A15很少会跑这样的实时操作系统，因为它在某些应用上无法达到我们的需求，而且在一些组件的移植上会比较困难，比如说数据库。对于这么多CPU，TI肯定想找到一种比较方便的方式，来编译这些库，比如说sys/bios一次性要提供所有基于这些平台的相应的库，那么就需要一种可定制化的操作，针对不同CPU架构，不同平台，来指定一些特定的编译信息，这样的话在编译的时候可以按照配置编译出相应的库。</p>
<hr>
<p>其实组件技术在计算机上编程在很多年前就已经开始普及了，例如java和.net框架下的高级编程语言，它们很早就使用了组件管理技术。但是针对嵌入式的组件管理技术是在近几年才开始出现的。</p>
<h1 id="RTSC开发流程"><a href="#RTSC开发流程" class="headerlink" title="RTSC开发流程"></a>RTSC开发流程</h1><p>简单来说，我们编写的应用程序无非是将一些组件来组合起来（按照规划和需要）。在以往的嵌入式开发中，我们从系统的驱动到上层应用都是整体来开发的。但是对于复杂应用来说这样就不合适了。比如在DSP开发一套频谱分析系统，不需要在任何部分都由我们自己来做，比如说对于串口的输出来说，我们完全不需要再编写串口的驱动了，包括理解串口的时序和协议，只需要简单的调用库函数，对串口进行初始化，根据需要输出想要输出的数据就可以了，不用具体了解它是如何工作的。这样让我们专注于在应用的开发，从而减轻我们的开发负担。</p>
<p>RTSC开发流程图如下<br><img src="/2019/09/07/Embedded-System-sys-bios01/2.png" alt="2"></p>
<ol>
<li><p><strong>SPECIFICATION</strong>描述<br>这一步是要明确我们开发的组件要实现的功能。这里用到了SDCspec脚本语言，它是基于javasrcipt,是javasrcipt的一个超集。它的主要功能是弥补C语言在描述上的不足，可以更方便的描述组件所要实现的功能。在编译的时候会将所做的描述转成c的源文件。</p>
</li>
<li><p><strong>IMPLEMENTATION</strong> 实现<br>这一步实现具体的功能</p>
</li>
<li><p><strong>PACKAGING</strong> 封装<br>也叫打包，对我们的组件使用同一接口，这样不同的人编写的组件的库都可以被统一的调用。以上组件生产者所做的工作就完成了，接着是组件消费者，即对应用程序的开发。</p>
</li>
<li><p><strong>CONFIGURATION</strong>配置<br>定制我们的组件，在配置完成后进入编译和链接阶段。在编译的时候会根据配置文件(.cfg)生成.cmd和.c文件，当然我们也可以自己建立.cmd文件。在RTSC工作中.cmd是根据platform（平台配置）来动态生成的，这是因为我们引用的不同库可能对于内存的分配是不一样的，如果我们手工来编写.cmd的话工作量会非常大，对于嵌入式来说只有我们用到的库才会被链接到最终的程序。.cmd的主要功能除了内存分配，还有选定我们链接的组件（库）。一般我们使用动态生成的.cmd就可以了。<br>配置阶段使用的是XDCscript语言（与SDCspec类似）。编译链接后生成可执行程序.out文件，这是我们的应用程序就已经产生了。</p>
</li>
<li><p><strong>ANALYSIS</strong>分析<br>分析对于我们评估整个应用也是十分重要的。主要是通过LOG输出程序运行的状态信息。</p>
</li>
</ol>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://www.bilibili.com/video/av61638240" target="_blank" rel="noopener">sys/bios视频教程</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/04/Backpropagation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/04/Backpropagation/" itemprop="url">Backpropagation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-04T19:42:07+08:00">
                2019-09-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>当我们用Gradient Descent方法来train一个neural network的时候，具体应该怎么做？</p>
<h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><p>我们先来回顾一下Gradient Descent，假设我们的neural network有一堆参数<br>$$\theta=\{w_1,w_2,\cdots,b_1,b_2,\cdots\}$$</p>
<p>选一个初始的参数$\theta^0$,计算$\theta^0$对Loss Function的Gradient:$\Delta L(\theta^0)$,其中<br>$$<br>\Delta L(\theta) = \begin{bmatrix}<br>\partial L(\theta)/\partial w_1 \\<br>\partial L(\theta)/\partial w_2 \\<br>\vdots \\<br>\partial L(\theta)/\partial b_1 \\<br>\partial L(\theta)/\partial b_2 \\<br>\vdots<br>\end{bmatrix}<br>$$</p>
<p>计算出上面的vector以后，我们就可以去更新我们的参数：<br>$$Compute \Delta L(\theta^0)\quad\quad\theta^1 = \theta^0 - \eta \Delta L(\theta^0)$$ $$Compute \Delta L(\theta^1)\quad\quad\theta^2 = \theta^1 - \eta \Delta L(\theta^1)$$ $$\cdots\cdots$$</p>
<p>一般Neural network的参数非常多，$\Delta L(\theta^0)$可能是一个上百万维的vector</p>
<p><strong>To compute the gradients efficiently,we use backpropagation</strong></p>
<h1 id="Chain-Rule"><a href="#Chain-Rule" class="headerlink" title="Chain Rule"></a>Chain Rule</h1><p>要学习backpropagation，首先要知道<strong>Chain Rule</strong></p>
<ul>
<li><strong>Case 1</strong></li>
</ul>
<p>假设有两个函数$y=g(x)$和$z=h(y)$,如果给$x$一个很小的变化,$y$会跟着有变化，从而影响$z$的变化:<br>$$\Delta x\rightarrow \Delta y\rightarrow \Delta z$$</p>
<p>如果我们要计算$x$对$z$的微分，可以拆成两项：<br>$$\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}$$</p>
<ul>
<li><strong>Case 2</strong><br>如果有3个函数，$x=g(s)$,$y=h(s)$,$z=k(x,y)$<br><img src="/2019/09/04/Backpropagation/1.jpg" alt="1"><br>$s$对$z$的微分可以写成：<br>$$\frac{dz}{ds}=\frac{\partial z}{\partial x}\frac{dx}{ds}+\frac{\partial z}{\partial y}\frac{dy}{ds}$$</li>
</ul>
<h1 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h1><p>回到Neural network的Training，我们知道Loss function: $L(\theta)=\sum_{n=1}^NC^n(\theta)$<br><img src="/2019/09/04/Backpropagation/2.png" alt="2"><br>把$L(\theta)$对某一个参数$w$做偏微分：<br>$$\frac{\partial L(\theta)}{\partial w} = \sum_{n=1}^N\frac{\partial C^n(\theta)}{\partial w}$$</p>
<p>我们下面讨论$\frac{\partial C^n(\theta)}{\partial w}$该如何计算<br><img src="/2019/09/04/Backpropagation/3.png" alt="3"><br>如上图所示的network，我们先只考虑某一个neuron,如下<br><img src="/2019/09/04/Backpropagation/4.png" alt="4"><br>现在我们拿$w$当作例子（$b$也是一样的）<br>$$\frac{\partial C}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial C}{\partial z}$$</p>
<p>计算$\frac{\partial z}{\partial w}$是比较简单的(<strong>Forward pass</strong>)，而计算$\frac{\partial C}{\partial z}$是比较复杂的(<strong>Backward pass</strong>)。</p>
<h2 id="Forward-pass"><a href="#Forward-pass" class="headerlink" title="Forward pass"></a>Forward pass</h2><p>$$\frac{\partial z}{\partial w_1} = x_1$$ $$\frac{\partial z}{\partial w_2} = x_2$$<br>对于Forward pass的计算，就是该weight前面接的input的值（The value of the input connected by the weight.）<br><img src="/2019/09/04/Backpropagation/5.png" alt="5"></p>
<h2 id="Backward-pass"><a href="#Backward-pass" class="headerlink" title="Backward pass"></a>Backward pass</h2><p>Compute $\frac{\partial C}{\partial z}$ for all activation function input z</p>
<p>我们使用chain rule对$\frac{\partial C}{\partial z}$做拆解,以下图为例<br><img src="/2019/09/04/Backpropagation/6.png" alt="6"><br>$$\frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}$$</p>
<p>其中$\frac{\partial a}{\partial z}$就是sigmoid function的微分$\sigma^{\prime}(z)$,又有：<br>$$\frac{\partial C}{\partial a}=<br>\frac{\partial z^{\prime}}{\partial a}\frac{\partial C}{\partial z^{\prime}}+<br>\frac{\partial z^{\prime\prime}}{\partial a}\frac{\partial C}{\partial z^{\prime\prime}}$$</p>
<p>其中：<br>$$\frac{\partial z^{\prime}}{\partial a} = w_3$$ $$\frac{\partial z^{\prime\prime}}{\partial a} = w_4$$</p>
<p>然而问题是我们不知到$\frac{\partial C}{\partial z^{\prime}}$和$\frac{\partial C}{\partial z^{\prime\prime}}$的值如何计算。我们先假设已经通过某种方法算出了这两项的值，有了这两项以后，就可以算出：<br>$$\frac{\partial C}{\partial z}=\sigma^{\prime}(z)\left[w_3\frac{\partial C}{\partial z^{\prime}}+w_4\frac{\partial C}{\partial z^{\prime\prime}}\right]$$</p>
<p>我们可以从另外的角度来看待上面的式子。假设有一个特殊的neuron如下(不在原来的network里面),<br><img src="/2019/09/04/Backpropagation/7.png" alt="7"><br>它的两个输入是$\frac{\partial C}{\partial z^{\prime}}$和$\frac{\partial C}{\partial z^{\prime\prime}}$,weight是$w_3$和$w_4$,“sigmoid function”是将输入的值乘上常数$\sigma^{\prime}(z)$</p>
<p>$\sigma^{\prime}(z)$ is a constant because $z$ is already determined in the forward pass.</p>
<p>为了计算$\frac{\partial C}{\partial z^{\prime}}$和$\frac{\partial C}{\partial z^{\prime\prime}}$，我们假设下面两个case</p>
<ul>
<li><strong>case 1:Output Layer</strong><br>如下图中的两个红色neuron,他们的output就是整个network的output<br><img src="/2019/09/04/Backpropagation/8.png" alt="8"><br>$$<br>\frac{\partial C}{\partial z^{\prime}}=<br>\frac{\partial y_1}{\partial z^{\prime}}\frac{\partial C}{\partial y_1}<br>$$<br>$\frac{\partial y_1}{\partial z^{\prime}}$取决于所定义的”sigmoid function”,$\frac{\partial C}{\partial y_1}$取决于Cost function的定义。<br>同样的可以算出<br>$$<br>\frac{\partial C}{\partial z^{\prime\prime}}=<br>\frac{\partial y_2}{\partial z^{\prime\prime}}\frac{\partial C}{\partial y_2}<br>$$</li>
</ul>
<p>这样我们就可以算出蓝色neuron的$\frac{\partial C}{\partial z}$</p>
<ul>
<li><strong>case 2:Not output Layer</strong><br>红色的neuron不是整个network的output，例如下图<br><img src="/2019/09/04/Backpropagation/9.png" alt="9"><br>根据前面的推导，如果我们知道$\frac{\partial C}{\partial z_a}$和$\frac{\partial C}{\partial z_b}$,我们就能算出$\frac{\partial C}{\partial z^{\prime}}$,如下<br><img src="/2019/09/04/Backpropagation/10.png" alt="10"></li>
</ul>
<p>但是如何计算$\frac{\partial C}{\partial z_a}$和$\frac{\partial C}{\partial z_b}$呢？如果他们不是最后一层的neuron的话，我们再计算他们后面的neuron.(Compute $\frac{\partial C}{\partial z}$ recursively.)</p>
<p>为了提高计算效率，我们从后往前计算（Compute $\frac{\partial C}{\partial z}$ for all activation function inputs z.Compute $\frac{\partial C}{\partial z}$ from the output layer）,如下<br><img src="/2019/09/04/Backpropagation/11.png" alt="11"><br>我们先算$z_5$、$z_6$的偏微分，然后再计算$z_3$、$z_4$的偏微分，因为前面的微分要依赖后面的微分。计算方式如下图<br><img src="/2019/09/04/Backpropagation/12.png" alt="12"></p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>先做Forward Pass，再做Backward Pass.<br><img src="/2019/09/04/Backpropagation/13.png" alt="13"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/03/Brief Introduction of Deep Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/03/Brief Introduction of Deep Learning/" itemprop="url">Brief Introduction of Deep Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-03T13:19:26+08:00">
                2019-09-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Deep Learning的应用实在太多了，随便Google就有一大堆。</p>
<h1 id="Ups-and-downs-of-Deep-Learning"><a href="#Ups-and-downs-of-Deep-Learning" class="headerlink" title="Ups and downs of Deep Learning"></a>Ups and downs of Deep Learning</h1><ul>
<li>1958:<strong>Perceptron</strong>(linear model)</li>
<li>1969:Perceptron has limitation</li>
<li>1980s:<strong>Multi-layer perceptron</strong><ul>
<li>Do not have significant difference from DNN today</li>
</ul>
</li>
<li>1986:<strong>Backpropagation</strong></li>
<li>1989:1 hidden layer is “good enough”,why deep?</li>
<li>2006:RBM initialization(breakthrough)</li>
<li>2009:GPU</li>
<li>2011:Start to be popular in speech recognition</li>
<li>2012:win ILSVRC image competition</li>
</ul>
<h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><p>和Machine Learning一样，Deep Learning一样也是那3个步骤：<br><strong>Step 1</strong>: define a set of function<br><strong>Step 2</strong>: goodness of function<br><strong>Step 3</strong>: pick the best function<br>Network parameter $\theta$: all the weights and biases in the “neurons”</p>
<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>上面Step1中定义的function set其实就是Neural Network。我们可以用不同的方法来连接Neural Network。怎么连接是我们手动来设计的（Different connection leads to different network structures），</p>
<h3 id="Fully-Connect-Feedforward-Network"><a href="#Fully-Connect-Feedforward-Network" class="headerlink" title="Fully Connect Feedforward Network"></a>Fully Connect Feedforward Network</h3><p>Fully Connect Feedforward Network是最常见的连接方式。每一个Neuron有一组weights和一个bias,weights和bias是我们根据training data来找出来的，如下图：<br><img src="/2019/09/03/Brief Introduction of Deep Learning/1.png" alt="1"><br>一个Network确定了参数，它就是一个function。如果一个network，只给出了结构，还没有确定参数，它就是function set。（<strong>Given network structure,define a function set</strong>）<br>当我们用Neuron network来决定function set的时候，这个function set是比较大的，它包含了很多以前做linear regression和logistic regression所没法包含的function.</p>
<p>更一般的neural network如下<br><img src="/2019/09/03/Brief Introduction of Deep Learning/2.png" alt="2"><br><strong>Fully Connect</strong>表示两个Layer之间的Neuron是两两互相连接的，<strong>Feedforward</strong>表示传递方向是由后往前传<br><img src="/2019/09/03/Brief Introduction of Deep Learning/3.png" alt="3"><br>注意其中的Input Layer不是由Neuron所组成的。</p>
<p>那么deep learning中的deep是什么意思呢？Deep = Many hidden layers。</p>
<h2 id="Matrix-Operation"><a href="#Matrix-Operation" class="headerlink" title="Matrix Operation"></a>Matrix Operation</h2><p>Network的运作我们常常用Matrix Operation来表示。例如下面的Neuron network,<br><img src="/2019/09/03/Brief Introduction of Deep Learning/4.png" alt="4"><br>第一个hidden layer所做的运算如下<br>$$\sigma\left(<br>\begin{bmatrix}1&amp;-2\\-1&amp;1\end{bmatrix}\begin{bmatrix}1\\-1\end{bmatrix}<br>+\begin{bmatrix}1\\0\end{bmatrix}<br>\right) = \begin{bmatrix}0.98\\0.12\end{bmatrix}$$</p>
<p>$$\sigma\left(<br>W\vec x+\vec b<br>\right)<br>$$<br>整个Neural Network的运算其实就是一连串的Matrix Operation,如下<br><img src="/2019/09/03/Brief Introduction of Deep Learning/5.png" alt="5"><br>写成矩阵运算的式子可以让我们通过GPU去加速（Using parallel computing techniques to speed up matrix operation）</p>
<p>整个Neural Network在Output Layer之前的部分看作是一个Feature extractor。$x_1$到$x_K$可以看作是一组新的feature.Output是一个Multi-Class Classifier,它用$x_1\cdots x_K$作为feature，这是一组经过转化的比较好的feature。<br><img src="/2019/09/03/Brief Introduction of Deep Learning/6.png" alt="6"></p>
<h1 id="Handwriting-Digit-Recognition"><a href="#Handwriting-Digit-Recognition" class="headerlink" title="Handwriting Digit Recognition"></a>Handwriting Digit Recognition</h1><p>举一个实际的例子：手写数字辨识。input一张手写数字的image（解析度$16\times 16$），output对应的数字。输入对于machine来说就是一个256维的vector。在输入的image里面每一个pixel就对应一个dimension。pixel的值只取0或1。Output是一个10维的vector，$y_1$到$y_{10}$分别对应input属于1、2、……、0的概率。如下图<br><img src="/2019/09/03/Brief Introduction of Deep Learning/7.png" alt="7"><br>总结如下<br><img src="/2019/09/03/Brief Introduction of Deep Learning/8.png" alt="8"><br>其中Neural Network就是A function set containing the candidates for Handwriting Digit Recognition。接下来我们要用Gradient Descent来找一组参数用来做手写数字辨识。在这个过程中，我们需要做一些design（You need to decide the network structure to let a good function in your function set.）。我们之前在做Linear Regression或Logistic Regression是不需要对model做特别的设计。但是，对于neural network来说，现在唯一的constrain就是input要是256维，output要是10维，但是中间要有多少hidden layer,每个hidden layer有多少个neuron,这是没有限制的，我们必须自己去设计它，这些决定了我们的function set长什么样子。如果我们决定了一个差的function set，会导致最终找不到一个好的function。所以决定一个好的function set(network structure)是很关键的。</p>
<h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><ul>
<li><strong>Q:How many layers?How many neurons for each layer?</strong></li>
</ul>
<p>这个只能通过经验和直觉（<strong>Trial and Error + Intuition</strong>）。找到一个neural structure有时候是一件不太容易的事情。从非deep learning的方法到deep learning的方法，machine learning其实并没有变得简单，而是我们把一个问题转化成另一个问题。本来不是deep的model，我们要得到好的结果，往往要做Feature Engineering,也就是做Feature Transform,找一组好的feature。</p>
<p>但是我们做deep learning的时候，往往不需要找一组好的feature，比如说影像辨识，可以直接把pixel输入进去。过去做影像辨识，需要从影像中抽一些人定的feature，这件事情就是Feature Transform。然而deep learning带来的新的问题是我们需要来design structure。所以deep learning是不是真的好用取决于哪个问题比较容易（Feature Transform or Design Structure）。</p>
<p>如果是影像辨识或语言辨识，design network Structure可能比Feature Engineering容易，因为人的看和听都太过潜意识了，它离我们的意识层次太远（人本身不知道自己是如何在做语音辨识这件事情），所以，对人来说抽一组好的Feature，还不如让机器自己去找出好的feature，这件事情反而比较容易。（语音辨识和影像辨识是最早开始用deep learning的，一用下去进步量非常惊人）</p>
<p>另一方面，Deep Learning在NLP上面的performance没有那么好，这个原因可能是对于文字处理，人是比较强的，比如一片文档的负面情绪词汇和正面情绪词汇人是比较了解的。当然，长久而言，deep learning去做文字处理也能占到一些优势，只是一下子，眼下看起来进步没有那么显著。</p>
<ul>
<li><strong>Q:Can the structure be automatically determined?</strong></li>
</ul>
<p>可以的，例如，Evolutionary Artificial Neural Networks。不过这些方法目前还不是很普及，alpha go这些强大的应用不是用这些方法实现的。</p>
<ul>
<li><strong>Q:Can we design the network structure?</strong></li>
</ul>
<p>自己去设计structure,不用fully connected,是可以的。一个特殊的structure就是<strong>Convolutional Neural Network(CNN)</strong></p>
<h1 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h1><p>deep learning的第二步，我们怎么决定一组参数的好坏呢？这跟我们在做Multiclass classification的时候一模一样，计算$y$和$\hat{y}$的cross entropy，然后去调整参数，让它越小越好<br><img src="/2019/09/03/Brief Introduction of Deep Learning/9.png" alt="9"></p>
<p>当然Training data里面不会只有一笔data,我们要把所有data的cross entropy都算出来,如下<br><img src="/2019/09/03/Brief Introduction of Deep Learning/10.png" alt="10"></p>
<p>然后加起来得到Total Loss:<br>$$L = \sum_{n=1}^NC^n$$</p>
<p>最后找到使得L值最小的参数$\theta^{\star}$。找到$\theta^{\star}$所用到的方法就是Gradient Descent,其过程如下图<br><img src="/2019/09/03/Brief Introduction of Deep Learning/11.png" alt="11"><br>一个neural network可能有数百万个参数，我们不可能为每一个参数都算微分，这样太花时间了，有一个很有效的计算微分的方式就是<strong>Backpropagation</strong></p>
<h1 id="Why-deep"><a href="#Why-deep" class="headerlink" title="Why deep"></a>Why deep</h1><p>有如下理论。<br><strong>Universality Theorem</strong></p>
<p>任何连续的function都可以用一个hidden layer的neural network来表示（只要这个network的neuron越多）<br>Any continuous function f $f:R^N\rightarrow R^M$ can be realized by a network with one hidden layer<br><img src="/2019/09/03/Brief Introduction of Deep Learning/12.png" alt="12"><br>既然一层hidden layer就可以表示成任何function，那我们做deep的意义何在，难道只是一个噱头吗？Why “Deep” neural network not “Fat” neural network?这个在以后会做解释。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">Neural Networks and Deep Learning</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/Logistic Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/Logistic Regression/" itemprop="url">Logistic Regression</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T19:58:17+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上一节我们讲到，要做Classification,我们要找的东西就是一个几率$P_{w,b}(C_1|x)$,如果$P_{w,b}(C_1|x)\geq 0.5$,就输出C1,否则就输出C2。如果用Gaussian,$P_{w,b}(C_1|x) = \sigma(z)$,其中:<br>$$\sigma(z) = \frac{1}{1+exp(-z)}$$<br>$$z = {\bf w}\cdot {\bf x} + b = \sum_i w_ix_i+b$$</p>
<p>我们的function set就是所有不同的$\bf{w}$和$b$集合起来的函数：<br>$$f_{w,b}(x) = P_{w,b}(C_1|x)$$</p>
<p>我们用图像的方法来表示如下<br><img src="/2019/09/01/Logistic Regression/1.png" alt="1"><br>以上就是<strong>Logistic Regression</strong></p>
<h1 id="Logistic-Regression-amp-Linear-Regression"><a href="#Logistic-Regression-amp-Linear-Regression" class="headerlink" title="Logistic Regression &amp; Linear Regression"></a>Logistic Regression &amp; Linear Regression</h1><p>我们来比较一下<strong>Logistic Regression</strong>和<strong>Linear Regression</strong>。我们知道Machine Learning有三个步骤</p>
<h2 id="Step1-Model-function-set"><a href="#Step1-Model-function-set" class="headerlink" title="Step1 Model(function set)"></a>Step1 Model(function set)</h2><p>两者的function set如下<br><img src="/2019/09/01/Logistic Regression/2.png" alt="2"></p>
<h2 id="Step2-Goodness-of-a-Function"><a href="#Step2-Goodness-of-a-Function" class="headerlink" title="Step2 Goodness of a Function"></a>Step2 Goodness of a Function</h2><p>我们要决定一个function的好坏。假设有N笔数据$x^1$到$x^N$以及它们对应的分类，假设这些data都是从$f_{w,b}(x)=P_{w,b}(C_1|x)$中产生，<strong>Given a set of w and b,what is its probability of generating the data</strong>?<br><img src="/2019/09/01/Logistic Regression/3.png" alt="3"></p>
<p>对某一个$\bf{w}$和$b$，产生上面N笔data的几率计算如下：<br>$$L(w,b) = f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots f_{w,b}(x^N)$$</p>
<p>The most likely $w^{\star}$ and $b^{\star}$ is the one with the largest $L(w,b)$.<br>$$w^{\star},b^{\star} = \mathop{argmax}_{w,b}L(w,b)$$</p>
<p>我们将上式做一下数学式上的转换,这样可以让计算变得容易点，如下：<br>$$w^{\star},b^{\star} = \mathop{argmax}_{w,b}L(w,b)$$ </p>
<p>$$\Downarrow$$</p>
<p>$$w^{\star},b^{\star} = \mathop{argmin}_{w,b}-InL(w,b)$$</p>
<p>其中<br>$$-InL(w,b) = -Inf_{w,b}(x^1)-Inf_{w,b}(x^2)-Inf_{w,b}(1-f_{w,b}(x^3))\cdots $$</p>
<p>为了能够用求和符号表示上面式子，我们做一个符号上的转换。用1表示Class 1,用0表示Class 2<br><img src="/2019/09/01/Logistic Regression/4.png" alt="4"><br>这样我们就可以把上面的式子表示成：<br>$$<br>\begin{equation}<br>\begin{split}<br>-InL(w,b) = &amp;-\left[\hat{y}^1Inf(x^1)+(1-\hat{y}^1In(1-f(x^1)))\right] \\<br>&amp;-\left[\hat{y}^2Inf(x^2)+(1-\hat{y}^2In(1-f(x^2)))\right] \\<br>&amp;-\left[\hat{y}^3Inf(x^3)+(1-\hat{y}^3In(1-f(x^3)))\right]\cdots<br>\end{split}<br>\end{equation}<br>$$</p>
<p>所以就有<br>$$-InL(w,b) = \sum_n-\left[\hat{y}^nInf_{w,b}(x^n)+(1-\hat{y}^nIn(1-f_{w,b}(x^n)))\right]$$</p>
<p>上面求和式子中的每一项称为<strong>Cross entropy between two Bernoulli distribution</strong>,Cross entropy代表的含义是这两个distribution有多接近,如果两个distribution一模一样，那么算出来的cross entropy就是0.<br><img src="/2019/09/01/Logistic Regression/5.png" alt="5"><br>$$H(p,q) = -\sum_xp(x)In(q(x))$$</p>
<p>所以在Logistic Regression里面，我们就可以通过如下方式来定义（左边是Logistic Regression,右边是Linear Regression）<br><img src="/2019/09/01/Logistic Regression/6.png" alt="6"><br>其中Cross Entropy:<br>$$C(f(x^n),\hat{y}^n) = -\left[\hat{y}^nInf(x^n)+(1-\hat{y}^n)In(1-f(x^n))\right]$$</p>
<p>这里先留一个问题，为什么我们不选择square error而要用cross entropy呢？</p>
<h2 id="Step3-Find-the-best-function"><a href="#Step3-Find-the-best-function" class="headerlink" title="Step3 Find the best function"></a>Step3 Find the best function</h2><p>这一步很简单，用Gradient Descent找一个最好的函数即可。我们先来算$-InL(w,b)$对$w_i$的偏微分<br>$$<br>\frac{-InL(w,b)}{\partial w_i} = \sum_n-\left[\hat{y}^n\frac{Inf_{w,b}(x^n)}{\partial w_i}+(1-\hat{y}^n)\frac{In(1-f_{w,b}(x^n))}{\partial w_i}\right]<br>$$</p>
<p>$$\frac{Inf_{w,b}(x^n)}{\partial w_i} = \frac{\partial Inf_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i}$$</p>
<p>注意：<br>$$f_{w,b}(x)=\sigma(z)=1/(1+exp(-z))$$<br>$$z = w\cdot x + b =\sum_iw_ix_i+b$$</p>
<p>所以有</p>
<p>$$<br>\begin{equation}<br>\begin{split}<br>\frac{\partial Inf_{w,b}(x)}{\partial z} &amp;= \frac{\partial In\sigma (z)}{\sigma z} \\<br>&amp;= \frac{1}{\sigma (z)}\frac{\partial \sigma(z)}{\partial z} \\<br>&amp;= \frac{1}{\partial (z)}\sigma (z)(1-\sigma (z)) \\<br>&amp;= (1-\sigma(z))<br>\end{split}<br>\end{equation}<br>$$</p>
<p>同时<br>$$\frac{\partial z}{\partial w_i} = x_i$$</p>
<p>同理可得</p>
<p>$$<br>\begin{equation}<br>\begin{split}<br>\frac{In(1-f_{w,b}(x^n))}{\partial w_i} &amp;= \frac{In(1-f_{w,b}(x))}{\partial z}\frac{\partial z}{\partial w_i} \\<br>&amp;= \frac{In(1-\sigma (z))}{\sigma z}x_i \\<br>&amp;= -\frac{1}{1-\sigma(z)}\frac{\partial \sigma(z)}{\partial z}x_i \\<br>&amp;= -\frac{1}{1-\sigma(z)}\sigma(z)(1-\sigma(z))x_i \\<br>&amp;= -\sigma(z)<br>\end{split}<br>\end{equation}<br>$$</p>
<p>最终整理可得到<br>$$<br>\begin{equation}<br>\begin{split}<br>-\frac{InL(w,b)}{\partial w_i} &amp;=\sum_n-\left[\hat{y}^n(1-f_{w,b}(x^n))x_i^n - (1-\hat{y}^n)f_{w,b}(x^n)x_i^n\right] \\<br>&amp;=\sum_n-\left[\hat{y}^n-\hat{y}^nf_{w,b}(x^n) -f_{w,b}(x^n)+\hat{y}^nf_{w,b}(x^n) \right]x_i^n \\<br>&amp;=\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n<br>\end{split}<br>\end{equation}<br>$$</p>
<p>我们发现最后得到的结果是非常neat的。<br>用Gradient Descent来update<br>$$w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$</p>
<p>分析上式的含义，其中$\hat{y}^n-f_{w,b}(x^n)$代表目标与实际输出的差距，<strong>Larger difference,larger update</strong>,这个结果还是比较合理的。<br>到这里我们可以发现，Linear regression与Logistic regression它们做Gradient Descent的式子是一模一样的<br>$${\tt Logistic\ regression}: w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$  $${\tt Linear\ regression}: w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$</p>
<p>唯一不同之处在于Logistic regression中$\hat{y}^n$一定是0或1，而Linear regression中$\hat{y}^n$可以是任意的real number.</p>
<h1 id="Why-not-Square-Error"><a href="#Why-not-Square-Error" class="headerlink" title="Why not Square Error"></a>Why not Square Error</h1><p>回到前面提到的问题，Logistic Regression为什么不能用Square Error?<br>我们用Square Error看看会怎样，还是一样的3个步骤</p>
<ol>
<li><strong>step1</strong><br>$$f_{w,b}(x) = \sigma(\sum_iw_ix_i+b)$$</li>
<li><strong>step2</strong><br>Training data:$(x^n,\hat{y}^n)$,$\hat{y}^n$:1 for class 1,0 for class 2<br>$$L(f)=\frac{1}{2}\sum_n\left(f_{w,b}(x^n)-\hat{y}^n\right)^2$$</li>
<li><strong>step3</strong><br>$$<br>\begin{equation}<br>\begin{split}<br>\frac{\partial(f_{w,b}(x)-\hat{y})^2}{\partial w_i} &amp;=2(f_{w,b}(x)-\hat{y})\frac{\partial f_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i} \\<br>&amp;= 2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i \\<br>\end{split}<br>\end{equation}<br>$$</li>
</ol>
<p>这里我们可以发现一个问题：<br>当$\hat{y}^n=1$,如果$f_{w,b}(x^n)=1$,这时得到的结果是perfect的(close to target),这个时候$\frac{\partial L}{\partial w_i}=0$,这是我们所期望的。<br>而当$\hat{y}^n=1$,如果$f_{w,b}(x^n)=0$,这时得到的结果是不够好的(far from target),然而这个时候却依然有$\frac{\partial L}{\partial w_i}=0$<br>对于$\hat{y}^n=0$依然存在上面问题。<br>如果我们把参数的变化对Total Loss做图的话，选择Cross Entropy跟Square Error的图像如下，其中黑色的是Cross Entropy,红色的是Square Error<br><img src="/2019/09/01/Logistic Regression/7.png" alt="7"><br>可以发现，对于Cross Entropy，距离目标越远，微分值越大，参数update时变化越大，这个是符合我们期望的。<br>对于Square Error，无论是距离目标远或近，微分值都比较小，这样参数update的速度是非常慢的。</p>
<h1 id="Discriminative-v-s-Generative"><a href="#Discriminative-v-s-Generative" class="headerlink" title="Discriminative v.s. Generative"></a>Discriminative v.s. Generative</h1><p>上面Logistic Regression的方法我们称之为<strong>Discriminative</strong>方法，而上一章所讲的几率模型称为<strong>Generative</strong>方法。<br>在几率模型中，只要我们将covariance matrix设成是共享的，它们两者的Model其实是一模一样的：<br>$$P(C_1|x)=\sigma(w\cdot x +b)$$<br>Discriminative可以直接将$\bf{w}$和$b$找出来，而用Generative方法要先找到$\mu^1$,$\mu^2$,$\Sigma^{-1}$,然后再计算下面式子<br>$$w^T = (\mu^1-\mu^2)^T\Sigma^{-1}$$</p>
<p>$$b=-\frac{1}{2}(\mu^1)^T(\Sigma)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T(\Sigma)^{-1}\mu^2+In\frac{N_1}{N_2}$$</p>
<p>思考一下，用Discriminative和Generative两种方法求得的$\bf{w}$和$b$是相同的吗？答案是不同！<br><strong>The same model(function set),but different function is selected by the same training data.</strong><br>在Discriminative里面，我们没有做对probability distribution做任何的假设，而在Generative里面，我们有对probability distribution做出某些假设（例如Gaussain、Navie Bayes）<br>那么用哪一种方法找到的w和b是比较好的呢？Discriminative的表现常常要比Generative要好。我们用下面的例子来说明<br><img src="/2019/09/01/Logistic Regression/8.png" alt="8"><br>假设我们有一笔training data如上，每一个data有2个feature,总共有$1+3\times 4=13$个data.其中Class1的data两个feature都是1，Class2的training data有3种不同类型。<br>现在如果给我们一个testing data,它的两个feature值都是1，作为人来判断，显然它是属于Class 1的。<br>但是Naive Bayes会怎么认为吗？所谓Naive Bayes即假设所有的feature产生的几率是independent:<br>$$P(x|C_i)=P(x_1|C_i)P(x_2|C_i)$$</p>
<p>接着我们算Prior:<br>$$P(C_1)=\frac{1}{13}$$ $$P(C_2)=\frac{12}{13}$$</p>
<p>另外<br>$$P(x_1=1|C_1)=1$$ $$P(x_2=1|C_1)=1$$<br>$$P(x_1=1|C_2)=\frac{1}{3}$$ $$P(x_2=1|C_2)=\frac{1}{3}$$ </p>
<p>还是给一个两个feature值都是1的Testing data,可以算出它属于Class 1的几率是：<br>$$P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1\times 1\times \frac{1}{13}}{1\times 1\times \frac{1}{13}+\frac{1}{3}\times \frac{1}{3}\times \frac{12}{13}}&lt;0.5$$ </p>
<p>Navie Bayes认为该data居然是属于Class 2!!这和我们的直觉是相反的！<br>对于Navie Bayes来说，data的两个feature是independent产生的，在Class 2之所以没有观测到所给的test data,是因为我们sample的不够多。</p>
<p>这再一次说明，Generative跟Discriminative的差别就在于，<strong>Generative Model它有自己做了某些假设</strong>，即它有自己“脑补”某些事情。通常情况下，“脑补”可能不是一件好的事情，但是如果是在data很少的情况下，“脑补”有时候也是有用的。有些时候Generative Model也是有优势的。比如我们的train data很少，因为Discriminative Model完全没有做任何假设，它是看着data说话的，所以它的performance受data量的影响很大。Generative Model受Data量影响相对较少。</p>
<p>Benefit of generative model</p>
<ul>
<li>With the assumption of probability distribution,less training data is needed</li>
<li>With the assumption of prabability distribution,more robust to the noise</li>
<li>Priors and class-dependent probabilities can be estimated from different sources.</li>
</ul>
<h1 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h1><p>之前我们以二分类为例子，如果要分的类别大于2呢？假设有3个Class，每个Class都有自己的${\bf w}$和b，如下<br>$$C_1:w^1,b_1\quad z_1=w^1\cdot x+b_1$$ $$C_2:w^2,b_2\quad z_2=w^2\cdot x+b_2$$ $$C_3:w^3,b_3\quad z_3=w^3\cdot x+b_3$$</p>
<p>接下来我们把$z_1$、$z_2$、$z_3$丢进一个<strong>softmax</strong> function,这个function做的事情如下<br><img src="/2019/09/01/Logistic Regression/9.png" alt="9"><br><em>Probability</em></p>
<ul>
<li>$1&gt;y_i&gt;0$</li>
<li>$\sum_iy_i=1$</li>
</ul>
<p>原来的$z_1$、$z_2$、$z_3$可以是任何值，做完softmax以后，output的值一定是介于1和0之间。softmax会对最大的值做<strong>强化</strong>，使大的值和小的值之间的差距会被拉的更开。$y_i=P(C_i|x)$,即得到input $x$是属于$C_i$的几率。<br>为什么要用softmax function是可以推导出来的，这里不做探讨(google maximum entropy)。</p>
<p>总结上面所做的事情，如下<br><img src="/2019/09/01/Logistic Regression/10.png" alt="10"><br>$y$与$\hat{y}$都是一个probability distribution,$\hat{y}$我们可以设成如下<br>$$if\ x\in class1 \quad \quad \hat{y}=\begin{bmatrix}1&amp;0&amp;0\end{bmatrix}^T$$ $$if\ x\in class2 \quad \quad \hat{y}=\begin{bmatrix}0&amp;1&amp;0\end{bmatrix}^T$$ $$if\ x\in class3 \quad \quad \hat{y}=\begin{bmatrix}0&amp;0&amp;1\end{bmatrix}^T$$</p>
<h1 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h1><p>Logistic Regression其实是有非常强的限制，假设有下面这个case,<br><img src="/2019/09/01/Logistic Regression/11.png" alt="11"><br>我们用Logistic Regression对上面的class1和class2做分类，发现这件事情是办不到的（即让两个红色点的输出几率大于0.5，而两个蓝色点的几率小于0.5），其原因是Logistic Regression的Boundary就是一条直线,而无论我们怎么去画一条直线，都不能将红色的点划在一边，而蓝色的点划在另一边。<br><img src="/2019/09/01/Logistic Regression/12.png" alt="12"> </p>
<h2 id="Feature-Transformation"><a href="#Feature-Transformation" class="headerlink" title="Feature Transformation"></a>Feature Transformation</h2><p>为解决上面问题，还是用Logistic Regression,我们可以用<strong>Feature Transformation</strong>。我们可以对不好的feature进行一些转换，使得Logistic Regression变得好处理。<br>我们将$\begin{bmatrix}x_1&amp;x_2\end{bmatrix}^T$转化成$\begin{bmatrix}x_1^{\prime}&amp;x_2^{\prime}\end{bmatrix}^T$</p>
<p>我们定义$x_1^{\prime}$是点到$\begin{bmatrix}0&amp;0\end{bmatrix}^T$的距离，$x_2^{\prime}$是点到$\begin{bmatrix}1&amp;1\end{bmatrix}^T$的距离，经过这样的转换，我们可以对转换后的点找到一条Boundary<br><img src="/2019/09/01/Logistic Regression/13.png" alt="13"><br>然后，要找到一个Transform是比较麻烦的（Not always easy to find a good transformation）。<strong>我们期望这个Transformation是由机器自己产生的</strong>。</p>
<h2 id="Cascading-logistic-regression-models"><a href="#Cascading-logistic-regression-models" class="headerlink" title="Cascading logistic regression models"></a>Cascading logistic regression models</h2><p>为了让机器自己产生Transformation,我们将多个logistic regression串联起来，如下图<br><img src="/2019/09/01/Logistic Regression/14.png" alt="14"><br>前面两个logistic regression负责坐标转换，使得不同分类的点可以被一条直线分开，最后的logistic regression负责分类。</p>
<p>所以将某些logistic regression的输出作为其他logistic regression的输入，这样叠在一起，它就变得powerful。<br><img src="/2019/09/01/Logistic Regression/15.png" alt="15"><br>其中每一个logistic regression称为<strong>Neuron</strong>,把这些Neuron串起来，就叫做<strong>Neural Network</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8" target="_blank" rel="noopener">Demystifying Cross-Entropy</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/27/Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qi-Liang'blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/27/Classification/" itemprop="url">Classification</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-27T18:27:43+08:00">
                2019-08-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Classification的应用可以有银行信用评估、医疗诊断、手写辨识等。他们的输入输出如下</p>
<ul>
<li><strong>Credit Scoring</strong><ul>
<li>Input:income,savings,profession,age,past financial history…</li>
<li>Output:accept or refuse</li>
</ul>
</li>
<li><strong>Medical Diagnosis</strong><ul>
<li>Input:current symptoms,age,gender,past medical history…</li>
<li>Output:which kind of diseases</li>
</ul>
</li>
<li><strong>Handwriten charater recognition</strong></li>
<li><strong>Face recognition</strong></li>
</ul>
<h1 id="Classification-as-Regression"><a href="#Classification-as-Regression" class="headerlink" title="Classification as Regression"></a>Classification as Regression</h1><p>我们可以将Classification的问题当作Regression来解吗？</p>
<p>以Binary classification为例，在训练的时候，Class 1对应值1，Class 2对应值-1。在测试阶段，如果结果更靠近1，则分类为<br>class 1,如果结果更靠近-1，则分类为class 2.</p>
<p>这么做会遇到的问题如下，如下图，<br><img src="/2019/08/27/Classification/1.png" alt="1"></p>
<p>右边的图中绿色的线为classification的结果，紫色的线为Regression的结果，显然紫色的线并不是一个好的分类结果。太大于1的那些点反而还不好，它会去“惩罚”那些“太正确”的点。（<strong>Penalize to the examples that are “too correct”</strong>）</p>
<h1 id="Ideal-Alternatives"><a href="#Ideal-Alternatives" class="headerlink" title="Ideal Alternatives"></a>Ideal Alternatives</h1><ul>
<li>Function(Model)</li>
</ul>
<p>$$f(x) =<br>\begin{cases}<br>g(x)&gt;0,  &amp; \text{Output = class 1} \\<br>else, &amp; \text{Output = class 2}<br>\end{cases}<br>$$</p>
<ul>
<li><p>Loss function<br>$$L(f) = \sum_n\delta(f(x^n)\neq \hat{y}^n)$$<br>$f$在训练集上得到不正确结果(分类次数)的次数。</p>
</li>
<li><p>Find the best function</p>
</li>
</ul>
<p>注意$L(f)$没办法微分，我们不能用Gradient Descent来解。以后会讲它其实可用<strong>Perceptron</strong>和<strong>SVM</strong>来解。<br>我们先来讲另一个解法，该方法是通过几率的观点来看待。<br><img src="/2019/08/27/Classification/2.png" alt="2"><br>如上图有两个盒子，它们都有篮球和绿球，假设我们随机从某个盒子中抽取一个球出来，它是蓝色的，那么这个蓝色的球从Box1和Box2中抽取出来的几率分别是多少？<br>假设从Box1中抽取的几率为$P(B_1) = 2/3$,从Box2中抽取的几率为$P(B_2) = 1/3$,并且<br>$$P(Blue|B_1) = 4/5$$ $$P(Green|B_1) = 1/5$$ $$P(Blue|B_2) = 2/5$$ $$P(Green|B_2) = 3/5$$<br>有了上面的信息，那我们可以轻易计算<strong>得到一个篮球，它是从Box1中抽取出来的几率</strong>：<br>$$P(B_1|Blue) = \frac{P(Blue|B_1)P(B_1)}{P(Blue|B_1)P(B_1)+P(Blue|B_2)P(B_2)}$$</p>
<p>以上这些和分类有什么关系呢？<br>我们把Box1和Box2换成Class 1和Class 2，给我们一个$x$,比如说某一只宝可梦，它是从某一个class里面Sample的几率是多少呢？我们需要知道以下值</p>
<ul>
<li>$P(C_1)$ ：从Class1中抽一个样本出来的几率</li>
<li>$P(C_2)$ ：从Class2中抽一个样本出来的几率</li>
<li>$P(x|C_1)$ :从Class1中抽出x的几率</li>
<li>$P(x|C_2)$ :从Class2中抽出x的几率</li>
</ul>
<p>有了上面的值，我们就可以计算出$x$属于Class1的几率$P(C_1|x)$ :<br>$$P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}$$<br>有了$P(C_1|x)$，我们就可以比较$x$属于不同class的几率，几率最大的就是输出的结果。</p>
<p>所以现在的问题就是：我们要从train data里面将<strong>$P(C_1)$</strong>、<strong>$P(C_2)$</strong>、<strong>$P(x|C_1)$</strong>、<strong>$P(x|C_2)$</strong> 的值估算出来。这一整套的想法就叫做<strong>Generative Model</strong>。之所以叫这个名字，是因为我们有了这个model的话，就可以用它来Generate一个$x$,即可以计算某一个$x$出现的几率，如果我们知道每一个$x$出现的几率，知道$x$出现的distribution,那么我们就可以用这个distribution来sample出$x$。$x$的几率计算如下<br>$$P(x) = P(x|C_1)P(C_1) + P(x|C_2)P(C_2)$$</p>
<h2 id="Prior"><a href="#Prior" class="headerlink" title="Prior"></a>Prior</h2><p>我们先来算$P(C_1)$和$P(C_2)$，它们被称为Prior。<br>以宝可梦为例，假设Class1为水系宝可梦，Class2为一般系。先把宝可梦中ID&lt;400的作为train data,其余的作为testing data。其中有79只水系，61只一般系，于是：<br>$$P(C_1) = 79/(79+61) = 0.56$$ $$P(C_2) = 61/(79+61) = 0.44$$</p>
<h2 id="Probability-from-Class"><a href="#Probability-from-Class" class="headerlink" title="Probability from Class"></a>Probability from Class</h2><p>接下来如何计算$P(x|C_1)$呢？我们可以用一个向量来表示每一只宝可梦，简单起见，取<strong>Defense</strong>和<strong>SP Defense</strong>两个属性，即用一个二维向量来表示。每一个宝可梦用一个二维平面上的点表示，如下图<br><img src="/2019/08/27/Classification/3.png" alt="3"><br>现在的问题是如果给我们一个新的点，这个点没有出现在training data里面，如一个新的宝可梦海龟，那么从水系的宝可梦的抽出一个宝可梦，它是海龟的几率是多少。<br>我们可以这样考虑，水系的宝可梦都是从一个Gaussian Distribution里面Sample出来的，我们得到的79个training data也是从这个Gaussian Distribution里面Sample出来的，现在的问题是如何通过这79个点来最大限度的估测出这个Gaussian Distribution。</p>
<h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>Gaussian Distribution的向量形式如下，其中输入$x$是向量，输出是sample出$x$的概率，$\mu$是mean,$\Sigma$是covariance matrix.(<strong>The shape of the function determines by mean $\mu$ and covariance matrix $\Sigma$</strong>)<br>$$f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\left\lbrace -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \right\rbrace$$</p>
<p>在当前例子中$x$就是宝可梦对应的向量，越接近中心点$\mu$的点被sample的几率越大，现在的问题就是如何寻找$\mu$和$\Sigma$.</p>
<h3 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h3><p>上述的79个点可以从任意的Gaussian里被Sample出来，但是不同的Gaussian,它Sample出这79个点的可能性（<strong>Likelihood</strong>）是不一样的。换句话说，给我们一个Gaussian的$\mu$和$\Sigma$，我们可以算出它Sample出这79个点的几率。计算公式如下<br>$$L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots f_{\mu,\Sigma}(x^{79})$$<br>我们要找出$(\mu^{\star},\Sigma^{\star})$使得$L(\mu,\Sigma)$的值最大,即</p>
<p>$$ \mu^{\star},\Sigma^{\star} = \mathop{argmax}_{\mu,\Sigma}L(\mu,\Sigma)$$</p>
<p>解得：<br>$$\mu^{\star} = \frac{1}{79}\sum_{n=1}^{79}x^n$$ $$\Sigma^{\star} = \frac{1}{79}\sum_{n=1}^{79}(x^n - \mu^{\star})(x^n-\mu^{\star})^T$$</p>
<p>接着，只要算出$P(C_1|x)&gt;0.5$,我们便可以说$x$是属于Class 1。在Training data上得到的结果如下<br><img src="/2019/08/27/Classification/4.png" alt="4"><br>将得到的分界线apply到test data上，结果如下<br><img src="/2019/08/27/Classification/5.png" alt="5"><br>然而我们得到的正确率只有47%。可能是因为代表宝可梦向量的维数太低。（在2维空间无法分开，在7维空间中可能会被分开）。<br>在7维空间中进行分类，$\mu$是一个7维向量，$\Sigma$是一个$7\times7$的矩阵。然而，我们依旧只得到了54%的正确率。</p>
<h2 id="Modifying-Model"><a href="#Modifying-Model" class="headerlink" title="Modifying Model"></a>Modifying Model</h2><p>我们要怎么改进呢？<br>其实上面介绍的这种模型是比较少见的，我们不常看到给每一个Gaussian都有自己的mean和covariance matrix,更常见的作法是<strong>不同的Gaussian可以共享同一个covariance matrix</strong>,如下图。<br><img src="/2019/08/27/Classification/6.png" alt="6"><br>因为随着维度的增大，covariance matrix的参数个数增长是非常大的（$n^2$），如果分别给不同的Gaussian不同的$\Sigma$,那参数就更多了，参数多就会导致overfitting。现在我们要做的是找到$\mu^1$,$\mu^2$,$\Sigma$,使得$L(\mu^1,\mu^2,\Sigma)$的值最大<br>$$ L(\mu^1,\mu^2,\Sigma) = f_{\mu^1,\Sigma}(x^1)f_{\mu^1,\Sigma}(x^2)\cdots f_{\mu^1,\Sigma}(x^{79})f_{\mu^2,\Sigma}(x^{80})f_{\mu^2,\Sigma}(x^{81})\cdots f_{\mu^2,\Sigma}(x^{140}) $$ </p>
<p>$\mu^1$与$\mu^2$的算法与上面相同，将训练数据加起来取平均即可。$\Sigma$的计算略有区别:<br>$$ \Sigma = \frac{79}{140}\Sigma^1 + \frac{61}{140}\Sigma^2 $$</p>
<p>仍用2维向量表示宝可梦，这次得到的结果如下图<br><img src="/2019/08/27/Classification/7.png" alt="7"><br>此时我们发现分界线boundary从原来的曲线变为了直线，所以这样的模型我们也称为<strong>Linear Model</strong>，正确率从原来的54%变为了73%</p>
<h3 id="Probability-Distribution"><a href="#Probability-Distribution" class="headerlink" title="Probability Distribution"></a>Probability Distribution</h3><p>为什么要选择Gaussian Distribution，其实也可以选择别的分布。<br>假设样本${\bf x} = \begin{bmatrix}{x_{1}}&amp;{x_{2}}&amp;{\cdots}&amp;{x_{n}}\end{bmatrix}^T$的每一个维从几率模型产生的几率是independent的，于是$P(x|C_1)$可以写成:<br>$$ P(x|C_1) = P(x_1|C_1)P(x_2|C_1)\cdots P(x_n|C_1) $$</p>
<p>每一维的几率分布是independent,从$x_1$到$x_n$产生的几率都是一维的Gaussian,这样相当于原来高维度的Gaussian，它的covariance matrix变成Diagnoal matrix(不是对角线的地方值都是0),这个就可以减少参数，得到一个更简单的模型。尝试了这个模型，它的结果不太好，还是太简单了。不同维度之间的covariance看来还是必要的。</p>
<p>假设某个feature是binary的，那么它就不太可能是用Gaussain产生的（<strong>For binary features,you may assume they are from Bernoulli distributions</strong>）<br>如果假设所有的feature之间都是independent的，这种分类方法叫做<strong>Naive Bayes Classifier</strong></p>
<h2 id="Posterior-Probability"><a href="#Posterior-Probability" class="headerlink" title="Posterior Probability"></a>Posterior Probability</h2><p>接下来我们对Posterior Probability进行分析，<br>$$<br>\begin{equation}<br>\begin{split}<br>P(C_1|x) &amp;= \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}\\<br>&amp;= \frac{1}{1 + \frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}\\<br>&amp;= \frac{1}{1+exp(-z)}<br>&amp;= \sigma(z)<br>\end{split}<br>\nonumber<br>\end{equation}<br>$$</p>
<p>其中<br>$$z = In\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}$$</p>
<p>$\sigma(z)$称为<strong>Sigmoid function</strong>，函数图像如下<br><img src="/2019/08/27/Classification/8.png" alt="8"><br>我们来看一下$z$长什么样子<br>$$z = In\frac{P(x|C_1)}{P(x|C_2)}+In\frac{P(C_1)}{P(C_2)} $$</p>
<p>其中<br>$$In\frac{P(C_1)}{P(C_2)} = \frac{\frac{N_1}{N_1+N_2}}{\frac{N_2}{N_1+N_2}} = \frac{N_1}{N_2}$$<br>$$P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma^1|^{1/2}}exp\left\lbrace -\frac{1}{2}(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1) \right\rbrace$$<br>$$P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma^2|^{1/2}}exp\left\lbrace -\frac{1}{2}(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2) \right\rbrace$$</p>
<p>整理可得<br>$$In\frac{P(x|C_1)}{P(x|C_2)} = In\frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} - \frac{1}{2}[(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)-(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)]$$</p>
<p>又有<br>$$(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1) = x^T(\Sigma^1)^{-1}x - 2(\mu^1)^T(\Sigma^1)^{-1}x + (\mu^1)^T(\Sigma^1)^{-1}\mu^1$$<br>$$(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2) = x^T(\Sigma^2)^{-1}x - 2(\mu^2)^T(\Sigma^2)^{-1}x + (\mu^2)^T(\Sigma^2)^{-1}\mu^2$$</p>
<p>所以<br>$$z = In\frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} - \frac{1}{2}x^T(\Sigma^1)^{-1}x + (\mu^1)^T(\Sigma^1)^{-1}x - \frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1 + \frac{1}{2}x^T(\Sigma^2)^{-1}x - (\mu^2)^T(\Sigma^2)^{-1}x + \frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2 + In\frac{N_1}{N_2}$$</p>
<p>考虑到$\Sigma_1 = \Sigma_2 = \Sigma$,上式可以简化成<br>$$z = (\mu^1-\mu^2)^T\Sigma^{-1}x - \frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1 + \frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2 + In\frac{N_1}{N_2}$$</p>
<p>我们令${\bf w^T} = (\mu^1-\mu^2)^T\Sigma^{-1}$,$b = \frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1 + \frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2 + In\frac{N_1}{N_2}$.$\bf{w}$是向量，b是一个scalar</p>
<p>所以就有<br>$$P(C_1|\bf{x}) = \sigma({\bf w}\cdot {\bf x}+b)$$</p>
<p>从上面的式子我们可以看出来为什么我们将$\Sigma^1$与$\Sigma^2$合并，得到的boundary会是一条直线。<br>在generative model里面，我们做的事情是用某些方法来找到$N_1$,$N_2$,$\mu^1$,$\mu^2$,$\Sigma$的值，找出这些后我们就能算出${\bf w}$和$b$,从而再算出几率。</p>
<p>但是为什么要这么麻烦呢？既然我们只需要找到${\bf w}$和$b$,何必要舍近求远来求$N_1$,$N_2$,$\mu^1$,$\mu^2$,$\Sigma$的值，可不可以直接来求${\bf w}$和$b$？这个就是下一节<strong>Logistic Regression</strong>要涉及的内容</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://oysz2016.github.io/post/8611e6fb.html" target="_blank" rel="noopener">数学公式语法——MathJax教程</a></li>
<li><a href="https://www.zybuluo.com/knight/note/96093" target="_blank" rel="noopener">MathJax使用LaTeX语法编写数学公式教程</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33857596" target="_blank" rel="noopener">Hexo博客使用MathJax公式并解决Markdown渲染冲突问题</a></li>
<li><a href="http://masikkk.com/article/hexo-13-MathJax/" target="_blank" rel="noopener">公式对齐</a></li>
<li><a href="https://www.cnblogs.com/xuejianbest/p/10285243.html" target="_blank" rel="noopener">微积分常用符号</a></li>
<li><a href="https://www.githang.com/2018/12/22/hexo-new-post-path/" target="_blank" rel="noopener">如何在Hexo中对文章md文件分类</a></li>
<li><a href="https://blog.csdn.net/csdnSR/article/details/78300820" target="_blank" rel="noopener">修改hexo的主题nexT中的Pisces主题宽度</a></li>
<li><a href="https://nipgeihou.com/Hexo/hexo_max_width/" target="_blank" rel="noopener">设置图片大小</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">GitHub+Hexo搭建个人网站详细教程</a></li>
<li><a href="https://blog.csdn.net/dajian790626/article/details/78595684" target="_blank" rel="noopener">hexo搭建Github博客上传后，网页显示404问题解决方案</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Liang Qi">
            
              <p class="site-author-name" itemprop="name">Liang Qi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liang Qi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>