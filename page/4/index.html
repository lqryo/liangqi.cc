<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="学习笔记">
<meta property="og:url" content="http://yoursite.com/page/4/index.html">
<meta property="og:site_name" content="学习笔记">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="学习笔记">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/4/">





  <title>学习笔记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">学习笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">stay young,stay simple</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/01/Logistic Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学习笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/01/Logistic Regression/" itemprop="url">Logistic Regression</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-01T19:58:17+08:00">
                2019-09-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上一节我们讲到，要做Classification,我们要找的东西就是一个几率$P_{w,b}(C_1|x)$,如果$P_{w,b}(C_1|x)\geq 0.5$,就输出C1,否则就输出C2。如果用Gaussian,$P_{w,b}(C_1|x) = \sigma(z)$,其中:<br>$$\sigma(z) = \frac{1}{1+exp(-z)}$$<br>$$z = {\bf w}\cdot {\bf x} + b = \sum_i w_ix_i+b$$</p>
<p>我们的function set就是所有不同的w和b集合起来的函数：<br>$$f_{w,b}(x) = P_{w,b}(C_1|x)$$</p>
<p>我们用图像的方法来表示如下<br><img src="/2019/09/01/Logistic Regression/1.png" alt="1"><br>以上就是<strong>Logistic Regression</strong></p>
<h1 id="Logistic-Regression-amp-Linear-Regression"><a href="#Logistic-Regression-amp-Linear-Regression" class="headerlink" title="Logistic Regression &amp; Linear Regression"></a>Logistic Regression &amp; Linear Regression</h1><p>我们来比较一下<strong>Logistic Regression</strong>和<strong>Linear Regression</strong>。我们知道Machine Learning有三个步骤</p>
<h2 id="Step1-Model-function-set"><a href="#Step1-Model-function-set" class="headerlink" title="Step1 Model(function set)"></a>Step1 Model(function set)</h2><p>两者的function set如下<br><img src="/2019/09/01/Logistic Regression/2.png" alt="2"></p>
<h2 id="Step2-Goodness-of-a-Function"><a href="#Step2-Goodness-of-a-Function" class="headerlink" title="Step2 Goodness of a Function"></a>Step2 Goodness of a Function</h2><p>我们要决定一个function的好坏。假设有N笔数据$x^1$到$x^N$以及它们对应的分类，假设这些data都是从$f_{w,b}(x)=P_{w,b}(C_1|x)$中产生，<strong>Given a set of w and b,what is its probability of generating the data</strong>?<br><img src="/2019/09/01/Logistic Regression/3.png" alt="3"></p>
<p>对某一个w和b，产生上面N笔data的几率计算如下：<br>$$L(w,b) = f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3))\cdots f_{w,b}(x^N)$$</p>
<p>The most likely $w^{\star}$ and $b^{\star}$ is the one with the largest $L(w,b)$.<br>$$w^{\star},b^{\star} = \mathop{argmax}_{w,b}L(w,b)$$</p>
<p>我们将上式做一下数学式上的转换,这样可以让计算变得容易点，如下：<br>$$w^{\star},b^{\star} = \mathop{argmax}_{w,b}L(w,b)$$ </p>
<p>$$\Downarrow$$</p>
<p>$$w^{\star},b^{\star} = \mathop{argmin}_{w,b}-InL(w,b)$$</p>
<p>其中<br>$$-InL(w,b) = -Inf_{w,b}(x^1)-Inf_{w,b}(x^2)-Inf_{w,b}(1-f_{w,b}(x^3))\cdots $$</p>
<p>为了能够用求和符号表示上面式子，我们做一个符号上的转换。用1表示Class 1,用0表示Class 2<br><img src="/2019/09/01/Logistic Regression/4.png" alt="4"><br>这样我们就可以把上面的式子表示成：<br>$$<br>\begin{equation}<br>\begin{split}<br>-InL(w,b) = &amp;-\left[\hat{y}^1Inf(x^1)+(1-\hat{y}^1In(1-f(x^1)))\right] \\<br>&amp;-\left[\hat{y}^2Inf(x^2)+(1-\hat{y}^2In(1-f(x^2)))\right] \\<br>&amp;-\left[\hat{y}^3Inf(x^3)+(1-\hat{y}^3In(1-f(x^3)))\right]\cdots<br>\end{split}<br>\end{equation}<br>$$</p>
<p>所以就有<br>$$-InL(w,b) = \sum_n-\left[\hat{y}^nInf_{w,b}(x^n)+(1-\hat{y}^nIn(1-f_{w,b}(x^n)))\right]$$</p>
<p>上面求和式子中的每一项称为<strong>Cross entropy between two Bernoulli distribution</strong>,Cross entropy代表的含义是这两个distribution有多接近,如果两个distribution一模一样，那么算出来的cross entropy就是0.<br><img src="/2019/09/01/Logistic Regression/5.png" alt="5"><br>$$H(p,q) = -\sum_xp(x)In(q(x))$$</p>
<p>所以在Logistic Regression里面，我们就可以通过如下方式来定义（左边是Logistic Regression,右边是Linear Regression）<br><img src="/2019/09/01/Logistic Regression/6.png" alt="6"><br>其中Cross Entropy:<br>$$C(f(x^n),\hat{y}^n) = -\left[\hat{y}^nInf(x^n)+(1-\hat{y}^n)In(1-f(x^n))\right]$$</p>
<p>这里先留一个问题，为什么我们不选择square error而要用cross entropy呢？</p>
<h2 id="Step3-Find-the-best-function"><a href="#Step3-Find-the-best-function" class="headerlink" title="Step3 Find the best function"></a>Step3 Find the best function</h2><p>这一步很简单，用Gradient Descent找一个最好的函数即可。我们先来算$-InL(w,b)$对$w_i$的偏微分<br>$$<br>\frac{-InL(w,b)}{\partial w_i} = \sum_n-\left[\hat{y}^n\frac{Inf_{w,b}(x^n)}{\partial w_i}+(1-\hat{y}^n)\frac{In(1-f_{w,b}(x^n))}{\partial w_i}\right]<br>$$</p>
<p>$$\frac{Inf_{w,b}(x^n)}{\partial w_i} = \frac{\partial Inf_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i}$$</p>
<p>注意：<br>$$f_{w,b}(x)=\sigma(z)=1/(1+exp(-z))$$<br>$$z = w\cdot x + b =\sum_iw_ix_i+b$$</p>
<p>所以有</p>
<p>$$<br>\begin{equation}<br>\begin{split}<br>\frac{\partial Inf_{w,b}(x)}{\partial z} &amp;= \frac{\partial In\sigma (z)}{\sigma z} \\<br>&amp;= \frac{1}{\sigma (z)}\frac{\partial \sigma(z)}{\partial z} \\<br>&amp;= \frac{1}{\partial (z)}\sigma (z)(1-\sigma (z)) \\<br>&amp;= (1-\sigma(z))<br>\end{split}<br>\end{equation}<br>$$</p>
<p>同时<br>$$\frac{\partial z}{\partial w_i} = x_i$$</p>
<p>同理可得</p>
<p>$$<br>\begin{equation}<br>\begin{split}<br>\frac{In(1-f_{w,b}(x^n))}{\partial w_i} &amp;= \frac{In(1-f_{w,b}(x))}{\partial z}\frac{\partial z}{\partial w_i} \\<br>&amp;= \frac{In(1-\sigma (z))}{\sigma z}x_i \\<br>&amp;= -\frac{1}{1-\sigma(z)}\frac{\partial \sigma(z)}{\partial z}x_i \\<br>&amp;= -\frac{1}{1-\sigma(z)}\sigma(z)(1-\sigma(z))x_i \\<br>&amp;= -\sigma(z)<br>\end{split}<br>\end{equation}<br>$$</p>
<p>最终整理可得到<br>$$<br>\begin{equation}<br>\begin{split}<br>-\frac{InL(w,b)}{\partial w_i} &amp;=\sum_n-\left[\hat{y}^n(1-f_{w,b}(x^n))x_i^n - (1-\hat{y}^n)f_{w,b}(x^n)x_i^n\right] \\<br>&amp;=\sum_n-\left[\hat{y}^n-\hat{y}^nf_{w,b}(x^n) -f_{w,b}(x^n)+\hat{y}^nf_{w,b}(x^n) \right]x_i^n \\<br>&amp;=\sum_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n<br>\end{split}<br>\end{equation}<br>$$</p>
<p>我们发现最后得到的结果是非常neat的。<br>用Gradient Descent来update<br>$$w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$</p>
<p>分析上式的含义，其中$\hat{y}^n-f_{w,b}(x^n)$代表目标与实际输出的差距，<strong>Larger difference,larger update</strong>,这个结果还是比较合理的。<br>到这里我们可以发现，Linear regression与Logistic regression它们做Gradient Descent的式子是一模一样的<br>$${\tt Logistic\ regression}: w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$  $${\tt Linear\ regression}: w_i \leftarrow w_i - \eta \sum_n-\left(\hat{y}^n-f_{w,b}(x^n)\right)x_i^n$$</p>
<p>唯一不同之处在于Logistic regression中$\hat{y}^n$一定是0或1，而Linear regression中$\hat{y}^n$可以是任意的real number.</p>
<h1 id="Why-not-Square-Error"><a href="#Why-not-Square-Error" class="headerlink" title="Why not Square Error"></a>Why not Square Error</h1><p>回到前面提到的问题，Logistic Regression为什么不能用Square Error?<br>我们用Square Error看看会怎样，还是一样的3个步骤</p>
<ol>
<li><strong>step1</strong><br>$$f_{w,b}(x) = \sigma(\sum_iw_ix_i+b)$$</li>
<li><strong>step2</strong><br>Training data:$(x^n,\hat{y}^n)$,$\hat{y}^n$:1 for class 1,0 for class 2<br>$$L(f)=\frac{1}{2}\sum_n\left(f_{w,b}(x^n)-\hat{y}^n\right)^2$$</li>
<li><strong>step3</strong><br>$$<br>\begin{equation}<br>\begin{split}<br>\frac{\partial(f_{w,b}(x)-\hat{y})^2}{\partial w_i} &amp;=2(f_{w,b}(x)-\hat{y})\frac{\partial f_{w,b}(x)}{\partial z}\frac{\partial z}{\partial w_i} \\<br>&amp;= 2(f_{w,b}(x)-\hat{y})f_{w,b}(x)(1-f_{w,b}(x))x_i \\<br>\end{split}<br>\end{equation}<br>$$</li>
</ol>
<p>这里我们可以发现一个问题：<br>当$\hat{y}^n=1$,如果$f_{w,b}(x^n)=1$,这时得到的结果是perfect的(close to target),这个时候$\frac{\partial L}{\partial w_i}=0$,这是我们所期望的。<br>而当$\hat{y}^n=1$,如果$f_{w,b}(x^n)=0$,这时得到的结果是不够好的(far from target),然而这个时候却依然有$\frac{\partial L}{\partial w_i}=0$<br>对于$\hat{y}^n=0$依然存在上面问题。<br>如果我们把参数的变化对Total Loss做图的话，选择Cross Entropy跟Square Error的图像如下，其中黑色的是Cross Entropy,红色的是Square Error<br><img src="/2019/09/01/Logistic Regression/7.png" alt="7"><br>可以发现，对于Cross Entropy，距离目标越远，微分值越大，参数update时变化越大，这个是符合我们期望的。<br>对于Square Error，无论是距离目标远或近，微分值都比较小，这样参数update的速度是非常慢的。</p>
<h1 id="Discriminative-v-s-Generative"><a href="#Discriminative-v-s-Generative" class="headerlink" title="Discriminative v.s. Generative"></a>Discriminative v.s. Generative</h1><p>上面Logistic Regression的方法我们称之为<strong>Discriminative</strong>方法，而上一章所讲的几率模型称为<strong>Generative</strong>方法。<br>在几率模型中，只要我们将covariance matrix设成是共享的，它们两者的Model其实是一模一样的：<br>$$P(C_1|x)=\sigma(wx\cdot +b)$$<br>Discriminative可以直接将w和b找出来，而用Generative方法要先找到$\mu^1$,$\mu^2$$\sigma^{-1}$,然后再计算下面式子<br>$$w^T = (\mu^1-\mu^2)^T\Sigma^{-1}$$</p>
<p>$$b=-\frac{1}{2}(\mu^1)^T(\Sigma)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T(\Sigma)^{-1}\mu^2+In\frac{N_1}{N_2}$$</p>
<p>思考一下，用Discriminative和Generative两种方法求得的w和b是相同的吗？答案是不同！<br><strong>The same model(function set),but different function is selected by the same training data.</strong><br>在Discriminative里面，我们没有做对probability distribution做任何的假设，而在Generative里面，我们有对probability distribution做出某些假设（例如Gaussain、Navie Bayes）<br>那么用哪一种方法找到的w和b是比较好的呢？Discriminative的表现常常要比Generative要好。我们用下面的例子来说明<br><img src="/2019/09/01/Logistic Regression/8.png" alt="8"><br>假设我们有一笔training data如上，每一个data有2个feature,总共有$1+3\times 4=13$个data.其中Class1的data两个feature都是1，Class2的training data有3种不同类型。<br>现在如果给我们一个testing data,它的两个feature值都是1，作为人来判断，显然它是属于Class 1的。<br>但是Naive Bayes会怎么认为吗？所谓Naive Bayes即假设所有的feature产生的几率是independent:<br>$$P(x|C_i)=P(x_1|C_i)P(x_2|C_i)$$</p>
<p>接着我们算Prior:<br>$$P(C_1)=\frac{1}{13}$$ $$P(C_2)=\frac{12}{13}$$</p>
<p>另外<br>$$P(x_1=1|C_1)=1$$ $$P(x_2=1|C_1)=1$$<br>$$P(x_1=1|C_2)=\frac{1}{3}$$ $$P(x_2=1|C_2)=\frac{1}{3}$$ </p>
<p>还是给一个两个feature值都是1的Testing data,可以算出它属于Class 1的几率是：<br>$$P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}=\frac{1\times 1\times \frac{1}{13}}{1\times 1\times \frac{1}{13}+\frac{1}{3}\times \frac{1}{3}\times \frac{12}{13}}&lt;0.5$$ </p>
<p>Navie Bayes认为该data居然是属于Class 2!!这和我们的直觉是相反的！<br>对于Navie Bayes来说，data的两个feature是independent产生的，在Class 2之所以没有观测到所给的test data,是因为我们sample的不够多。</p>
<p>这再一次说明，Generative跟Discriminative的差别就在于，<strong>Generative Model它有自己做了某些假设</strong>，即它有自己“脑补”某些事情。通常情况下，“脑补”可能不是一件好的事情，但是如果是在data很少的情况下，“脑补”有时候也是有用的。有些时候Generative Model也是有优势的。比如我们的train data很少，因为Discriminative Model完全没有做任何假设，它是看着data说话的，所以它的performance受data量的影响很大。Generative Model受Data量影响相对较少。</p>
<p>Benefit of generative model</p>
<ul>
<li>With the assumption of probability distribution,less training data is needed</li>
<li>With the assumption of prabability distribution,more robust to the noise</li>
<li>Priors and class-dependent probabilities can be estimated from different sources.</li>
</ul>
<h1 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h1><p>之前我们以二分类为例子，如果要分的类别大于2呢？假设有3个Class，每个Class都有自己的${\bf w}$和b，如下<br>$$C_1:w^1,b_1\quad z_1=w^1\cdot x+b_1$$ $$C_2:w^2,b_2\quad z_2=w^2\cdot x+b_2$$ $$C_3:w^3,b_3\quad z_3=w^3\cdot x+b_3$$</p>
<p>接下来我们把$z_1$、$z_2$、$z_3$丢进一个<strong>softmax</strong> function,这个function做的事情如下<br><img src="/2019/09/01/Logistic Regression/9.png" alt="9"><br><em>Probability</em></p>
<ul>
<li>$1&gt;y_i&gt;0$</li>
<li>$\sum_iy_i=1$</li>
</ul>
<p>原来的$z_1$、$z_2$、$z_3$可以是任何值，做完softmax以后，output的值一定是介于1和0之间。softmax会对最大的值做<strong>强化</strong>，使大的值和小的值之间的差距会被拉的更开。$y_i=P(C_i|x)$,即得到input x是属于$C_i$的几率。<br>为什么要用softmax function是可以推导出来的，这里不做探讨(google maximum entropy)。</p>
<p>总结上面所做的事情，如下<br><img src="/2019/09/01/Logistic Regression/10.png" alt="10"><br>$y$与$\hat{y}$都是一个probability distribution,$\hat{y}$我们可以设成如下<br>$$if\ x\in class1 \quad \quad \hat{y}=\begin{bmatrix}1&amp;0&amp;0\end{bmatrix}^T$$ $$if\ x\in class2 \quad \quad \hat{y}=\begin{bmatrix}0&amp;1&amp;0\end{bmatrix}^T$$ $$if\ x\in class3 \quad \quad \hat{y}=\begin{bmatrix}0&amp;0&amp;1\end{bmatrix}^T$$</p>
<h1 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h1><p>Logistic Regression其实是有非常强的限制，假设有下面这个case,<br><img src="/2019/09/01/Logistic Regression/11.png" alt="11"><br>我们用Logistic Regression对上面的class1和class2做分类，发现这件事情是办不到的（即让两个红色点的输出几率大于0.5，而两个蓝色点的几率小于0.5），其原因是Logistic Regression的Boundary就是一条直线,而无论我们怎么去画一条直线，都不能将红色的点划在一边，而蓝色的点划在另一边。<br><img src="/2019/09/01/Logistic Regression/12.png" alt="12"> </p>
<h2 id="Feature-Transformation"><a href="#Feature-Transformation" class="headerlink" title="Feature Transformation"></a>Feature Transformation</h2><p>为解决上面问题，还是用Logistic Regression,我们可以用<strong>Feature Transformation</strong>。我们可以对不好的feature进行一些转换，使得Logistic Regression变得好处理。<br>我们将$\begin{bmatrix}x_1&amp;x_2\end{bmatrix}^T$转化成$\begin{bmatrix}x_1^{\prime}&amp;x_2^{\prime}\end{bmatrix}^T$</p>
<p>我们定义$x_1^{\prime}$是点到$\begin{bmatrix}0&amp;0\end{bmatrix}^T$的距离，$x_2^{\prime}$是点到$\begin{bmatrix}1&amp;1\end{bmatrix}^T$的距离，经过这样的转换，我们可以对转换后的点找到一条Boundary<br><img src="/2019/09/01/Logistic Regression/13.png" alt="13"><br>然后，要找到一个Transform是比较麻烦的（Not always easy to find a good transformation）。<strong>我们期望这个Transformation是由机器自己产生的</strong>。</p>
<h2 id="Cascading-logistic-regression-models"><a href="#Cascading-logistic-regression-models" class="headerlink" title="Cascading logistic regression models"></a>Cascading logistic regression models</h2><p>为了让机器自己产生Transformation,我们将多个logistic regression串联起来，如下图<br><img src="/2019/09/01/Logistic Regression/14.png" alt="14"><br>前面两个logistic regression负责坐标转换，使得不同分类的点可以被一条直线分开，最后的logistic regression负责分类。</p>
<p>所以将某些logistic regression的输出作为其他logistic regression的输入，这样叠在一起，它就变得powerful。<br><img src="/2019/09/01/Logistic Regression/15.png" alt="15"><br>其中每一个logistic regression称为<strong>Neuron</strong>,把这些Neuron串起来，就叫做<strong>Neural Network</strong></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8" target="_blank" rel="noopener">Demystifying Cross-Entropy</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/27/Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liang Qi">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="学习笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/27/Classification/" itemprop="url">Classification</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-27T18:27:43+08:00">
                2019-08-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Classification的应用可以有银行信用评估、医疗诊断、手写辨识等。他们的输入输出如下</p>
<ul>
<li><strong>Credit Scoring</strong><ul>
<li>Input:income,savings,profession,age,past financial history…</li>
<li>Output:accept or refuse</li>
</ul>
</li>
<li><strong>Medical Diagnosis</strong><ul>
<li>Input:current symptoms,age,gender,past medical history…</li>
<li>Output:which kind of diseases</li>
</ul>
</li>
<li><strong>Handwriten charater recognition</strong></li>
<li><strong>Face recognition</strong></li>
</ul>
<h1 id="Classification-as-Regression"><a href="#Classification-as-Regression" class="headerlink" title="Classification as Regression"></a>Classification as Regression</h1><p>我们可以将Classification的问题当作Regression来解吗？</p>
<p>以Binary classification为例，在训练的时候，Class 1对应值1，Class 2对应值-1。在测试阶段，如果结果更靠近1，则分类为<br>class 1,如果结果更靠近-1，则分类为class 2.</p>
<p>这么做会遇到的问题如下，如下图，<br><img src="/2019/08/27/Classification/1.png" alt="1"></p>
<p>右边的图中绿色的线为classification的结果，紫色的线为Regression的结果，显然紫色的线并不是一个好的分类结果。太大于1的那些点反而还不好，它会去“惩罚”那些“太正确”的点。（<strong>Penalize to the examples that are “too correct”</strong>）</p>
<h1 id="Ideal-Alternatives"><a href="#Ideal-Alternatives" class="headerlink" title="Ideal Alternatives"></a>Ideal Alternatives</h1><ul>
<li>Function(Model)</li>
</ul>
<p>$$f(n) =<br>\begin{cases}<br>g(x)&gt;0,  &amp; \text{Output = class 1} \\<br>else, &amp; \text{Output = class 2}<br>\end{cases}<br>$$</p>
<ul>
<li><p>Loss function<br>$$L(f) = \sum_n\delta(f(x^n)\neq \hat{y}^n)$$<br>f在训练集上得到不正确结果(分类次数)的次数。</p>
</li>
<li><p>Find the best function</p>
</li>
</ul>
<p>注意$L(f)$没办法微分，我们不能用Gradient Descent来解。以后会讲它其实可用<strong>Perceptron</strong>和<strong>SVM</strong>来解。<br>我们先来讲另一个解法，该方法是通过几率的观点来看待。<br><img src="/2019/08/27/Classification/2.png" alt="2"><br>如上图有两个盒子，它们都有篮球和绿球，假设我们随机从某个盒子中抽取一个球出来，它是蓝色的，那么这个蓝色的球从Box1和Box2中抽取出来的几率分别是多少？<br>假设从Box1中抽取的几率为$P(B_1) = 2/3$,从Box2中抽取的几率为$P(B_2) = 1/3$,并且<br>$$P(Blue|B_1) = 4/5$$ $$P(Green|B_1) = 1/5$$ $$P(Blue|B_2) = 2/5$$ $$P(Green|B_2) = 3/5$$<br>有了上面的信息，那我们可以轻易计算<strong>得到一个篮球，它是从Box1中抽取出来的几率</strong>：<br>$$P(B_1|Blue) = \frac{P(Blue|B_1)P(B_1)}{P(Blue|B_1)P(B_1)+P(Blue|B_2)P(B_2)}$$</p>
<p>以上这些和分类有什么关系呢？<br>我们把Box1和Box2换成Class 1和Class 2，给我们一个x,比如说某一只宝可梦，它是从某一个class里面Sample的几率是多少呢？我们需要知道以下值</p>
<ul>
<li>$P(C_1)$ ：从Class1中抽一个样本出来的几率</li>
<li>$P(C_2)$ ：从Class2中抽一个样本出来的几率</li>
<li>$P(x|C_1)$ :从Class1中抽出x的几率</li>
<li>$P(x|C_2)$ :从Class2中抽出x的几率</li>
</ul>
<p>有了上面的值，我们就可以计算出x属于Class1的几率$P(C_1|x)$ :<br>$$P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}$$<br>有了$P(C_1|x)$，我们就可以比较x属于不同class的几率，几率最大的就是输出的结果。</p>
<p>所以现在的问题就是：我们要从train data里面将<strong>$P(C_1)$</strong>、<strong>$P(C_2)$</strong>、<strong>$P(x|C_1)$</strong>、<strong>$P(x|C_2)$</strong> 的值估算出来。这一整套的想法就叫做<strong>Generative Model</strong>。之所以叫这个名字，是因为我们有了这个model的话，就可以用它来Generate一个x,即可以计算某一个x出现的几率，如果我们知道每一个x出现的几率，知道x出现的distribution,那么我们就可以用这个distribution来sample出x。x的几率计算如下<br>$$P(x) = P(x|C_1)P(C_1) + P(x|C_2)P(C_2)$$</p>
<h2 id="Prior"><a href="#Prior" class="headerlink" title="Prior"></a>Prior</h2><p>我们先来算$P(C_1)$和$P(C_2)$，它们被称为Prior。<br>以宝可梦为例，假设Class1为水系宝可梦，Class2为一般系。先把宝可梦中ID&lt;400的作为train data,其余的作为testing data。其中有79只水系，61只一般系，于是：<br>$$P(C_1) = 79/(79+61) = 0.56$$ $$P(C_2) = 61/(79+61) = 0.44$$</p>
<h2 id="Probability-from-Class"><a href="#Probability-from-Class" class="headerlink" title="Probability from Class"></a>Probability from Class</h2><p>接下来如何计算$P(x|C_1)$呢？我们可以用一个向量来表示每一只宝可梦，简单起见，取<strong>Defense</strong>和<strong>SP Defense</strong>两个属性，即用一个二维向量来表示。每一个宝可梦用一个二维平面上的点表示，如下图<br><img src="/2019/08/27/Classification/3.png" alt="3"><br>现在的问题是如果给我们一个新的点，这个点没有出现在training data里面，如一个新的宝可梦海龟，那么从水系的宝可梦的抽出一个宝可梦，它是海龟的几率是多少。<br>我们可以这样考虑，水系的宝可梦都是从一个Gaussian Distribution里面Sample出来的，我们得到的79个training data也是从这个Gaussian Distribution里面Sample出来的，现在的问题是如何通过这79个点来最大限度的估测出这个Gaussian Distribution。</p>
<h3 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h3><p>Gaussian Distribution的向量形式如下，其中输入x是向量，输出是sample出x的概率，$\mu$是mean,$\Sigma$是covariance matrix.(<strong>The shape of the function determines by mean $\mu$ and covariance matrix $\Sigma$</strong>)<br>$$f_{\mu,\Sigma}(x) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}}exp\left\lbrace -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) \right\rbrace$$</p>
<p>在当前例子中x就是宝可梦对应的向量，越接近中心点$\mu$的点被sample的几率越大，现在的问题就是如何寻找$\mu$和$\Sigma$.</p>
<h3 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h3><p>上述的79个点可以从任意的Gaussian里被Sample出来，但是不同的Gaussian,它Sample出这79个点的可能性（<strong>Likelihood</strong>）是不一样的。换句话说，给我们一个Gaussian的$\mu$和$\Sigma$，我们可以算出它Sample出这79个点的几率。计算公式如下<br>$$L(\mu,\Sigma) = f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)f_{\mu,\Sigma}(x^3)\cdots f_{\mu,\Sigma}(x^{79})$$<br>我们要找出$(\mu^{\star},\Sigma^{\star})$使得$L(\mu,\Sigma)$的值最大,即</p>
<p>$$ \mu^{\star},\Sigma^{\star} = \mathop{argmax}_{\mu,\Sigma}L(\mu,\Sigma)$$</p>
<p>解得：<br>$$\mu^{\star} = \frac{1}{79}\sum_{n=1}^{79}x^n$$ $$\Sigma^{\star} = \frac{1}{79}\sum_{n=1}^{79}(x^n - \mu^{\star})(x^n-\mu^{\star})^T$$</p>
<p>接着，只要算出$P(C_1|x)&gt;0.5$,我们便可以说x是属于Class 1。在Training data上得到的结果如下<br><img src="/2019/08/27/Classification/4.png" alt="4"><br>将得到的分界线apply到test data上，结果如下<br><img src="/2019/08/27/Classification/5.png" alt="5"><br>然而我们得到的正确率只有47%。可能是因为代表宝可梦向量的维数太低。（在2维空间无法分开，在7维空间中可能会被分开）。<br>在7维空间中进行分类，$\mu$是一个7维向量，$\Sigma$是一个$7\times7$的矩阵。然而，我们依旧只得到了54%的正确率。</p>
<h2 id="Modifying-Model"><a href="#Modifying-Model" class="headerlink" title="Modifying Model"></a>Modifying Model</h2><p>我们要怎么改进呢？<br>其实上面介绍的这种模型是比较少见的，我们不常看到给每一个Gaussian都有自己的mean和covariance matrix,更常见的作法是<strong>不同的Gaussian可以共享同一给covariance matrix</strong>,如下图。<br><img src="/2019/08/27/Classification/6.png" alt="6"><br>因为随着维度的增大，covariance matrix的参数个数增长是非常大的（$n^2$），如果分别给不同的Gaussian不同的$\Sigma$,那参数就更多了，参数多就会导致overfitting。现在我们要做的是找到$\mu^1$,$\mu^2$,$\Sigma$,使得$L(\mu^1,\mu^2,\Sigma)$的值最大<br>$$ L(\mu^1,\mu^2,\Sigma) = f_{\mu^1,\Sigma}(x^1)f_{\mu^1,\Sigma}(x^2)\cdots f_{\mu^1,\Sigma}(x^{79})f_{\mu^2,\Sigma}(x^{80})f_{\mu^2,\Sigma}(x^{81})\cdots f_{\mu^2,\Sigma}(x^{140}) $$ </p>
<p>$\mu^1$与$\mu^2$的算法与上面相同，将训练数据加起来取平均即可。$\Sigma$的计算略有区别:<br>$$ \Sigma = \frac{79}{140}\Sigma^1 + \frac{61}{140}\Sigma^2 $$</p>
<p>仍用2维向量表示宝可梦，这次得到的结果如下图<br><img src="/2019/08/27/Classification/7.png" alt="7"><br>此时我们发现分界线boundary从原来的曲线变为了直线，所以这样的模型我们也称为<strong>Linear Model</strong>，正确率从原来的54%变为了73%</p>
<h3 id="Probability-Distribution"><a href="#Probability-Distribution" class="headerlink" title="Probability Distribution"></a>Probability Distribution</h3><p>为什么要现在Gaussian Distribution，其实也可以选择别的分布。<br>假设样本${\bf x} = \begin{bmatrix}{x_{1}}&amp;{x_{2}}&amp;{\cdots}&amp;{x_{n}}\end{bmatrix}^T$的每一个维从几率模型产生的几率是independent的，于是$P(x|C_1)$可以写成:<br>$$ P(x|C_1) = P(x_1|C_1)P(x_2|C_1)\cdots P(x_n|C_1) $$</p>
<p>每一维的几率分布是independent,从$x_1$到$x_n$产生的几率都是一维的Gaussian,这样相当于原来高维度的Gaussian，它的covariance matrix变成Diagnoal matrix(不是对角线的地方值都是0),这个就可以减少参数，得到一个更简单的模型。尝试了这个模型，它的结果不太好，还是太简单了。不同维度之间的covariance看来还是必要的。</p>
<p>假设某个feature是binary的，那么它就不太可能是用Gaussain产生的（<strong>For binary features,you may assume they are from Bernoulli distributions</strong>）<br>如果假设所有的feature之间都是independent的，这种分类方法叫做<strong>Naive Bayes Classifier</strong></p>
<h2 id="Posterior-Probability"><a href="#Posterior-Probability" class="headerlink" title="Posterior Probability"></a>Posterior Probability</h2><p>接下来我们对Posterior Probability进行分析，<br>$$<br>\begin{equation}<br>\begin{split}<br>P(C_1|x) &amp;= \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}\\<br>&amp;= \frac{1}{1 + \frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}\\<br>&amp;= \frac{1}{1+exp(-z)}<br>&amp;= \sigma(z)<br>\end{split}<br>\nonumber<br>\end{equation}<br>$$</p>
<p>其中<br>$$z = In\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}$$</p>
<p>$\sigma(z)$称为<strong>Sigmoid function</strong>，函数图像如下<br><img src="/2019/08/27/Classification/8.png" alt="8"><br>我们来看一下z长什么样子<br>$$z = In\frac{P(x|C_1)}{P(x|C_2)}+In\frac{P(C_1)}{P(C_2)} $$</p>
<p>其中<br>$$In\frac{P(C_1)}{P(C_2)} = \frac{\frac{N_1}{N_1+N_2}}{\frac{N_2}{N_1+N_2}} = \frac{N_1}{N_2}$$<br>$$P(x|C_1) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma^1|^{1/2}}exp\left\lbrace -\frac{1}{2}(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1) \right\rbrace$$<br>$$P(x|C_2) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma^2|^{1/2}}exp\left\lbrace -\frac{1}{2}(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2) \right\rbrace$$</p>
<p>整理可得<br>$$In\frac{P(x|C_1)}{P(x|C_2)} = In\frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} - \frac{1}{2}[(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)-(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)]$$</p>
<p>又有<br>$$(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1) = x^T(\Sigma^1)^{-1}x - 2(\mu^1)^T(\Sigma^1)^{-1}x + (\mu^1)^T(\Sigma^1)^{-1}\mu^1$$<br>$$(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2) = x^T(\Sigma^2)^{-1}x - 2(\mu^2)^T(\Sigma^2)^{-1}x + (\mu^2)^T(\Sigma^2)^{-1}\mu^2$$</p>
<p>所以<br>$$z = In\frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}} - \frac{1}{2}x^T(\Sigma^1)^{-1}x + (\mu^1)^T(\Sigma^1)^{-1}x - \frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1 + \frac{1}{2}x^T(\Sigma^2)^{-1}x - (\mu^2)^T(\Sigma^2)^{-1}x + \frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2 + In\frac{N_1}{N_2}$$</p>
<p>考虑到$\Sigma_1 = \Sigma_2 = \Sigma$,上式可以简化成<br>$$z = (\mu^1-\mu^2)^T\Sigma^{-1}x - \frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1 + \frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2 + In\frac{N_1}{N_2}$$</p>
<p>我们令${\bf w^T} = (\mu^1-\mu^2)^T\Sigma^{-1}$,$b = \frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1 + \frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2 + In\frac{N_1}{N_2}$.w是向量，b是一个scalar</p>
<p>所以就有<br>$$P(C_1|x) = \sigma({\bf w}\cdot {\bf x}+b)$$</p>
<p>从上面的式子我们可以看出来为什么我们将$\Sigma^1$与$\Sigma^2$合并，得到的boundary会是一条直线。<br>在generative model里面，我们做的事情是用某些方法来找到$N_1$,$N_2$,$\mu^1$,$\mu^2$,$\Sigma$的值，找出这些后我们就能算出${\bf w}$和$b$,从而再算出几率。</p>
<p>但是为什么要这么麻烦呢？既然我们只需要找到${\bf w}$和$b$,何必要舍近求远来求$N_1$,$N_2$,$\mu^1$,$\mu^2$,$\Sigma$的值，可不可以直接来求${\bf w}$和$b$？这个就是下一节<strong>Logistic Regression</strong>要涉及的内容</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="https://oysz2016.github.io/post/8611e6fb.html" target="_blank" rel="noopener">数学公式语法——MathJax教程</a></li>
<li><a href="https://www.zybuluo.com/knight/note/96093" target="_blank" rel="noopener">MathJax使用LaTeX语法编写数学公式教程</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/33857596" target="_blank" rel="noopener">Hexo博客使用MathJax公式并解决Markdown渲染冲突问题</a></li>
<li><a href="http://masikkk.com/article/hexo-13-MathJax/" target="_blank" rel="noopener">公式对齐</a></li>
<li><a href="https://www.cnblogs.com/xuejianbest/p/10285243.html" target="_blank" rel="noopener">微积分常用符号</a></li>
<li><a href="https://www.githang.com/2018/12/22/hexo-new-post-path/" target="_blank" rel="noopener">如何在Hexo中对文章md文件分类</a></li>
<li><a href="https://blog.csdn.net/csdnSR/article/details/78300820" target="_blank" rel="noopener">修改hexo的主题nexT中的Pisces主题宽度</a></li>
<li><a href="https://nipgeihou.com/Hexo/hexo_max_width/" target="_blank" rel="noopener">设置图片大小</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">GitHub+Hexo搭建个人网站详细教程</a></li>
<li><a href="https://blog.csdn.net/dajian790626/article/details/78595684" target="_blank" rel="noopener">hexo搭建Github博客上传后，网页显示404问题解决方案</a></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Liang Qi">
            
              <p class="site-author-name" itemprop="name">Liang Qi</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liang Qi</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>